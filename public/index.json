[{"authors":["admin"],"categories":null,"content":"I am a scientist working within the domains of machine learning and mental health. My diverse background in behavioural neuroscience, applied statistics, and epidemiology leads me to apply a multidisciplinary and creative approach to doing research. I deeply enjoy solving complex problems and exploring new ideas.\nI’ve worked both as an independent contributor and as a leader of teams of up to 10. In my current role as a Data Scientist with the Province of British Columbia I primarily work in R and SQL.\nIn my free time I enjoy coding, gaming, reading, biking, hiking, playing guitar, listening to live music, and visiting with friends and family.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a scientist working within the domains of machine learning and mental health. My diverse background in behavioural neuroscience, applied statistics, and epidemiology leads me to apply a multidisciplinary and creative approach to doing research. I deeply enjoy solving complex problems and exploring new ideas.\nI’ve worked both as an independent contributor and as a leader of teams of up to 10. In my current role as a Data Scientist with the Province of British Columbia I primarily work in R and SQL.","tags":null,"title":"Dr. Craig P. Hutton","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]]\rname = \u0026#34;Courses\u0026#34;\rurl = \u0026#34;courses/\u0026#34;\rweight = 50\rOr, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]]\rname = \u0026#34;Docs\u0026#34;\rurl = \u0026#34;docs/\u0026#34;\rweight = 50\rUpdate the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I'll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I'll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using Academic's Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":["R","Reproducible Research","Data Cleaning and Transformation"],"content":"\r\r1 TL;DR\r2 Introduction\r3 Regular expressions\r4 Detecting pattern matches with str_detect(), str_which(), str_count(), and str_locate().\r5 Subsetting strings \u0026amp; data frames with str_subset(), str_sub(), str_match(), \u0026amp; str_extract().\r6 Combining and splitting strings using str_c(), str_flatten(), str_split(), \u0026amp; str_glue().\r7 Manage the lengths of strings using str_length(), str_pad(), str_trunc(), \u0026amp; str_trim()\r8 Mutating strings with str_sub(), str_replace(), str_replace_all(), str_remove(), \u0026amp; str_remove_all()\r9 You can modify the case of a string using str_to_lower(), str_to_upper(), str_to_title(), \u0026amp; str_to_sentence()\r10 Example application: Using str_detect() or str_which() to subset with data frames\r11 Navigation\r12 Notes\r\r\r1 TL;DR\rBeing able to work with character strings is an essential skill in data analysis and science. In this post we’ll learn a few of the ways in which the stringr package and regular expressions (AKA “regex” or “regexps”) makes working with strings in R considerably easier.\n\r2 Introduction\rThe 7th post of the Scientist’s Guide to R series is all about showing you how to work with strings in R, using the intuitive stringr package from the tidyverse. You’ll also learn about regular expressions, which allow you to use concisely specified patterns to search, subset, and modify strings. This is quite a large topic, so for this post I’ll focus on some of the more common operations that I’ve had to use in my work as an academic researcher and data scientist. Usually, this means working with strings vectors that are columns in data frames or the names of the columns in a data frame (which you may recall we can get with the base R names() function). Specifically, we’ll consider:\n\rWhat regular expressions are and how to use them.\n\rDetecting, locating, and counting pattern matches with str_detect(), str_which(), str_locate(), \u0026amp; str_count().\n\rSubset strings with str_subset(), str_sub(), \u0026amp; str_extract().\n\rCombining and splitting strings using str_c(), str_flatten(), str_split(), \u0026amp; str_glue().\n\rManage lengths of strings with str_length(), str_pad(), str_trunc(), \u0026amp; str_trim().\n\rMutate strings using str_sub(), str_replace(), \u0026amp; str_remove().\n\rAlter the case of strings with str_to_lower(), str_to_upper(), \u0026amp; str_to_title().\n\r\rNote: as usual, at this point in the blog series I’ll assume you’re familiar with the pipe operator (%\u0026gt;%). To refresh your memory or if you’re reading about the pipe operator for the 1st time, see this section of an earlier post.\n\r3 Regular expressions\rRegular expressions, or “regexps” for short, are a powerful way to work with patterns in strings. Becoming familiar with regexps is well worth the effort in the time they will save you. Regex allows you to match patterns in strings using a set of special characters that tell regexps-supported functions in R how to concisely describe the pattern in question. You can learn more about regular expressions here, here, and here. I won’t be using all of these in the subsequent demonstrations of the stringr functions (to keep things simple), but listing the majority of the available options will be useful as future reference for us to use in constructing regexps.\nIn this post we’ll focus on some of the most common special chararcters you’ll need, specifically (special character = definition):\nBasics:\n\r^ = start of a string\n\r$ = end of a string\n\r. = any character\n\r[:digit:] = any digit (use an extra set of square brackets for base R functions that accept regexps)\n\r[:alpha:] = any letter\n\r[:alnum:] = letters and/or numbers\n\r[:punct:] = punctuation characters\n\r[:graph:] = letters, numbers, and punctuation characters\n\r[:lower:] = lowercase letters only\n\r[:upper:] = uppercase letters only\n\r[:space:] = empty spaces\n\r[:blank:] = empty spaces or tabs\n\r\rQuantifiers These are useful when you want to match a pattern a specific number of times (based on the preceding character in the regexp):\n\r* = matches the preceding character any number of times\n\r+ = matches the preceding character once\n\r? = matches the predecing character at most once (i.e. optionally)\n\r{n} = matches the preceding character exactly n times\n\r{n, } = n or more times\n\r{n, m} = between n \u0026amp; m times\n\r\rAlternatives \u0026amp; look-arounds are useful for matching patterns more flexibly:\n\r| = or (just like the base R logical operator), e.g. the regexp “apples|oranges” would look for apples or oranges\n\r[abc] = one of a, b, or c (or whatever else you put within the [ ])\n\r[t-z] = a letter from t to z\n\r[^abc] = anything other than a, b, or c\n\r(?=) = look ahead, e.g. i(?=e) = i when it comes before e\n\r(?!) = negative look ahead, e.g. i(?!e) i when it comes before something that isn’t e\n\r(?\u0026lt;) = look behind, e.g. (?\u0026lt;=e)i = i when it follows e\n\r(?\u0026lt;!) = negative look behind, e.g. (?\u0026lt;!e)i = i when it does not follow e\n\r\rCapturing pattern groups\nParentheses can be used to specify the order of evaluation (as for mathematical expressions) and to capture groups or components of regexps. After defining pattern groups this way, you can refer to the groups using “\\\\n”, where n is the group number, assigned to groups in the regexp in order from left to right. The most common reason you would want to do this is to replace patterns using str_replace() or str_replace_all(), as will be demonstrated below in the section on mutating strings.\nEscaping characters\nWhat if you want to search for a literal “.” or “$” rather than use the special regexp characters for “any character” or “the end of a string”. This can be accomplished by escaping the character, which simply means putting one or more backslashes “\\” in front of it1. So to search for a literal period, “.”, you could use “\\\\.”\nAside from the characters above, most of the rest of a regexp will consist of the literal text you want to match. For example, the regexp “^[Ss]un.*” when applied to a string vector of the days of the week, would match entries for “Sunday” or “sunday” since these entries start with (^) an “S” or an “s”, followed by the literal characters “un”, then any character (“.”) repeated any number of times (asterisk).\n\r4 Detecting pattern matches with str_detect(), str_which(), str_count(), and str_locate().\rTo use any of the stringr functions, we 1st need to load the stringr package via the library() function\nlibrary(tidyverse) #note: stringr is installed and loaded with the tidyverse\rWe’ll start exploring the uses of some of these special characters with some basic regexps by submitting them to the pattern arguments (2nd argument) of str_detect(), str_which(), str_count(), and str_locate()\nstr_detect() finds matches for a regexp and returns a logical vector that is TRUE for matching entries, and FALSE for non-matching entries.\n#1st we\u0026#39;ll construct a vector of weekdays, repeated 10 times\rdays \u0026lt;- rep(c(\u0026quot;Monday\u0026quot;, \u0026quot;Tuesday\u0026quot;, \u0026quot;Wednesday\u0026quot;, \u0026quot;Thursday\u0026quot;, \u0026quot;Friday\u0026quot;, \u0026quot;Saturday\u0026quot;, \u0026quot;Sunday\u0026quot;), 3) #note that stringr also has a function called str_dup() which can be used to\r#replicate/duplicate string values.\rstr_detect(days, \u0026quot;^[Ss]un.*\u0026quot;) #every 7th entry is a match\r## [1] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE\r## [13] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE\r#in this case, since Su appears uniquely for the Sunday entries, we could also\r#just use \u0026quot;Su\u0026quot; for the regexp\ridentical(str_detect(days, \u0026quot;Su\u0026quot;), str_detect(days, \u0026quot;^[Ss]un.*\u0026quot;))\r## [1] TRUE\r#str_detect() is the tidyverse equivalent to the grepl() function in base R\ridentical(grepl(\u0026quot;Su\u0026quot;, days), str_detect(days, \u0026quot;Su\u0026quot;))\r## [1] TRUE\r#the main advantages of str_detect() are that the string is the 1st argument\r#(i.e. it is pipe friendly), and there many of the stringr functions also have a\r#\u0026quot;negate\u0026quot; option which allows you to look for non-matching entries instead of\r#matching entries, e.g.\rdays %\u0026gt;% str_detect(\u0026quot;Su\u0026quot;, negate = T) #find non-matches instead of matches\r## [1] TRUE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE\r## [13] TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE FALSE\rstr_which() returns the indices of matches for a regexp and returns a numeric vector, i.e. it tells you which entries are matches.\ndays %\u0026gt;% str_which(\u0026quot;^M\u0026quot;) #indices of matching entries, in this case starting with a capital \u0026quot;M\u0026quot; (for \u0026quot;Monday\u0026quot;)\r## [1] 1 8 15\rdays %\u0026gt;% str_which(\u0026quot;^M\u0026quot;, negate = T) #indices of non-matching entries\r## [1] 2 3 4 5 6 7 9 10 11 12 13 14 16 17 18 19 20 21\r#if there were non-day values in our weekday vector, we could use either of\r#these functions to identify the unusual values, e.g.\r#replace 10 randomly selected values with a non-weekday entry, like a \u0026quot;day off\u0026quot;\r#of work... which can happen irregularly if you\u0026#39;re a grad student ;) lol\rdays[sample(1:70, 10)] \u0026lt;- \u0026quot;day off\u0026quot; #find the indices of the entries which do not end in \u0026quot;day\u0026quot;, i.e. the indices of the days off\rdays %\u0026gt;% str_which(\u0026quot;day$\u0026quot;, negate = T)\r## [1] 13 15 18 23 26 38 46 52 57 60\r#as you can see, this is useful for checking for unusual values which may be\r#data entry/coding errors.\r#example of the \u0026quot;or\u0026quot; operator in a regular expression\rstr_which(days, \u0026quot;(Monday|Tuesday)\u0026quot;) #indices for entries containing either \u0026quot;Monday\u0026quot; or \u0026quot;Tuesday\u0026quot;\r## [1] 1 2 8 9 16\r#which entries contain an empty space (the \u0026quot;day off\u0026quot; ones)?\rstr_which(days, \u0026quot;[:space:]{1}\u0026quot;) #match entries containing a single {1} space [:space:]\r## [1] 13 15 18 23 26 38 46 52 57 60\rstr_locate() or str_locate_all() tell you the character positions the pattern characters are found in. I haven’t needed to use them so far.\ndays %\u0026gt;% str_locate(\u0026quot;day\u0026quot;) %\u0026gt;% #this tells us the starting and ending positions of the pattern \u0026quot;day\u0026quot;\rhead() #only print the 1st 6 rows of the output\r## start end\r## [1,] 4 6\r## [2,] 1 3\r## [3,] 7 9\r## [4,] 6 8\r## [5,] 4 6\r## [6,] 6 8\r#if someone else provided us with the days vector and expected it to only\r#contain entries for days of the week, all of these values should have start\r#values greater than 1.\r#str_locate() will return the starting and stopping positions of the FIRST match\r#only, if you want the positions of all matches, use str_locate_all() instead.\rstr_count() tells you how many times a pattern appears in each entry of a string/character vector. This might be most useful for checking for unexpected duplications of values when cleaning data or for extracting data from unstructured text (e.g. quantifying the number of times a specific keyword appears in an interview transcript).\ndays %\u0026gt;% str_count(\u0026quot;e\u0026quot;) #number of times the letter \u0026quot;e\u0026quot; appears in each entry\r## [1] 0 0 2 0 0 0 0 0 1 2 0 0 0 0 0 1 2 0 0 0 0 0 1 2 0 0 0 0 0 1 2 0 0 0 0 0 1 2\r## [39] 0 0 0 0 0 1 2 0 0 0 0 0 1 2 0 0 0 0 0 0 2 0 0 0 0 0 1 2 0 0 0 0\r\r5 Subsetting strings \u0026amp; data frames with str_subset(), str_sub(), str_match(), \u0026amp; str_extract().\rFor the remainer of this post we’ll work with the gapminder data, which you may recall from earlier posts contains data on life expectancy, population, and GDP per capita for 142 countries.\nlibrary(gapminder) #load the gapminder package containing the gapminder data\rgapminder %\u0026gt;% glimpse() #show the structure of a data frame using dplyr::glimpse()\r## Rows: 1,704\r## Columns: 6\r## $ country \u0026lt;fct\u0026gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afgha...\r## $ continent \u0026lt;fct\u0026gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asi...\r## $ year \u0026lt;int\u0026gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 199...\r## $ lifeExp \u0026lt;dbl\u0026gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 4...\r## $ pop \u0026lt;int\u0026gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372,...\r## $ gdpPercap \u0026lt;dbl\u0026gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.113...\rOne of the first things we learned about was how to use “patterns” with numeric vectors via base R logical comparison operators like \u0026gt;, \u0026lt;, \u0026lt;=, or \u0026gt;=. The str_subset(), str_sub(), str_match(), \u0026amp; str_extract() from stringr help us subset strings2\nstr_subset() returns all values in a vector which match a pattern. We could use it to figure out which countries in the gapminder data begin with an A or C and also end with an A or C.\nsubsetted_country_names \u0026lt;- gapminder$country %\u0026gt;% unique() %\u0026gt;% #just get the unique() values to drop duplicates for a vector\rstr_subset(\u0026quot;^[AC].*a$\u0026quot;) #above pattern = starts with (^) an A or C ([AC]), then anything (.) repeated\r#any number of times (*), then a lower case \u0026quot;a\u0026quot; at the end ($).\rsubsetted_country_names %\u0026gt;% head()\r## [1] \u0026quot;Albania\u0026quot; \u0026quot;Algeria\u0026quot; \u0026quot;Angola\u0026quot; \u0026quot;Argentina\u0026quot; \u0026quot;Australia\u0026quot; \u0026quot;Austria\u0026quot;\r#A tidyverse-only alternative to the $ symbol for \u0026quot;pulling\u0026quot; a variable from a\r#data frame is the pull() function\rgapminder %\u0026gt;% pull(country) %\u0026gt;% unique() %\u0026gt;% str_subset(\u0026quot;^[AC].*a$\u0026quot;) %\u0026gt;% identical(subsetted_country_names)\r## [1] TRUE\rYou may recall that this behaviour is similar to the matches() “tidy-select” helper function that you can use to select() columns with dplyr (which was loaded as one of the tidyverse packages). For example,\ngap_c_cols \u0026lt;- gapminder %\u0026gt;% select(matches(\u0026quot;^c\u0026quot;))\rgap_c_cols %\u0026gt;% head() #only print the 1st 6 rows\r## # A tibble: 6 x 2\r## country continent\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Afghanistan Asia ## 2 Afghanistan Asia ## 3 Afghanistan Asia ## 4 Afghanistan Asia ## 5 Afghanistan Asia ## 6 Afghanistan Asia\ris equivalent to\nc_names \u0026lt;- gapminder %\u0026gt;% names() %\u0026gt;% str_subset(\u0026quot;^c\u0026quot;)\rall.equal(\r#recall that we can subset columns with a character vector of names\rgapminder[, c_names], gap_c_cols #compare to the select(matches()) version\r)\r## [1] TRUE\r#more on stringr for subsetting data frames later...\rWhile, I don’t expect anyone to use str_subset() and [] instead of dplyr::select() \u0026amp; tidyselect::matches(), the take home message here is that the fact that the column names of a data frame are also a string vector means that you can use stringr functions to work with them.\nstr_sub() extracts parts of strings based on their position with the start and end arguments\ngap_names \u0026lt;- gapminder %\u0026gt;% names\rgap_names %\u0026gt;% str_sub(start = 1, end = 6) #return the 1st 6 characters of each column name\r## [1] \u0026quot;countr\u0026quot; \u0026quot;contin\u0026quot; \u0026quot;year\u0026quot; \u0026quot;lifeEx\u0026quot; \u0026quot;pop\u0026quot; \u0026quot;gdpPer\u0026quot;\rgap_names %\u0026gt;% str_sub(start = -3, end = -1) #return the last 3 characters of each column name\r## [1] \u0026quot;try\u0026quot; \u0026quot;ent\u0026quot; \u0026quot;ear\u0026quot; \u0026quot;Exp\u0026quot; \u0026quot;pop\u0026quot; \u0026quot;cap\u0026quot;\rstr_extract() is useful if you want to extract just the part of the string matching the specified regex instead of the entire entry as would be returned by str_subset(). For example, if you have some data on names and phone numbers in the same column, you might want to extract just the phone number or name portions.\nx \u0026lt;- c(\u0026quot;Bob: 250-999-8888\u0026quot;, \u0026quot;Emily: 416-908-2004\u0026quot;, \u0026quot;Roger: 204-192-9879\u0026quot;, \u0026quot;Lindsay: 250-209-3047\u0026quot;)\r#extract just the phone numbers using a regex that detects 3 numbers followed by\r#a dash, then 3 numbers, another dash, then 4 numbers\rstr_extract(x, \u0026quot;[:digit:]{3}-[:digit:]{3}-[:digit:]{4}\u0026quot;) \r## [1] \u0026quot;250-999-8888\u0026quot; \u0026quot;416-908-2004\u0026quot; \u0026quot;204-192-9879\u0026quot; \u0026quot;250-209-3047\u0026quot;\r#extract just the names with a regex that matches a sequence of letters \u0026quot;[:alpha:]\u0026quot;\r#of arbitrary length \u0026quot;*\u0026quot;\rstr_extract(x, \u0026quot;[:alpha:]*\u0026quot;) \r## [1] \u0026quot;Bob\u0026quot; \u0026quot;Emily\u0026quot; \u0026quot;Roger\u0026quot; \u0026quot;Lindsay\u0026quot;\rIt is worth mentioning that within each entry of the string vector str_extract() will pull the first set of values matching the specified pattern, so in the example above, if we had first and last names separated by a space, we would only get the 1st one with the [:alpha:]* pattern I used. To extract all matches, we could use str_extract_all() instead.\nx \u0026lt;- c(\u0026quot;Bob Jones: 250-999-8888\u0026quot;, \u0026quot;Emily Robins: 416-908-2004\u0026quot;, \u0026quot;Roger Smith: 204-192-9879\u0026quot;, \u0026quot;Lindsay Richards: 250-209-3047\u0026quot;)\r#only returns the 1st name (the 1st match)\rstr_extract(x, \u0026quot;[:alpha:]*\u0026quot;) \r## [1] \u0026quot;Bob\u0026quot; \u0026quot;Emily\u0026quot; \u0026quot;Roger\u0026quot; \u0026quot;Lindsay\u0026quot;\r#str_extract_all() returns both names (all matches), but gives you a list\r#(simplify = FALSE, the default) or matrix (simplify = TRUE)\rstr_extract_all(x, \u0026quot;[:alpha:]{2,}\u0026quot;, simplify = TRUE) \r## [,1] [,2] ## [1,] \u0026quot;Bob\u0026quot; \u0026quot;Jones\u0026quot; ## [2,] \u0026quot;Emily\u0026quot; \u0026quot;Robins\u0026quot; ## [3,] \u0026quot;Roger\u0026quot; \u0026quot;Smith\u0026quot; ## [4,] \u0026quot;Lindsay\u0026quot; \u0026quot;Richards\u0026quot;\r#here I used the {2, } quantifier for \u0026quot;2 or more\u0026quot; because the * quantifier\r#returns a bunch of empty strings as well.\r#alternatively, we could just pass a more complex regex to the pattern argument\r#of str_extract() to look for the 1st set of letters and 2nd set of letters with\r#a space between them\rstr_extract(x, \u0026quot;[:alpha:]* [:alpha:]*\u0026quot;)\r## [1] \u0026quot;Bob Jones\u0026quot; \u0026quot;Emily Robins\u0026quot; \u0026quot;Roger Smith\u0026quot; \u0026quot;Lindsay Richards\u0026quot;\r#of course that will only work if the names are formatted consitently, e.g. no\r#commas between names\r\r6 Combining and splitting strings using str_c(), str_flatten(), str_split(), \u0026amp; str_glue().\rstr_c() is very useful if you want to combine multiple strings or other vectors into a single character vector on an element-wise basis. I often use this to add an indicator string to the names of a data frame before joining it to another one to make it easy to keep track of which columns came from which data frame.\nnms \u0026lt;- names(gapminder) #store the original names\rnms \u0026lt;- str_c(names(gapminder), \u0026quot;_gap\u0026quot;) #add _gap to the end of each column name\rnms\r## [1] \u0026quot;country_gap\u0026quot; \u0026quot;continent_gap\u0026quot; \u0026quot;year_gap\u0026quot; \u0026quot;lifeExp_gap\u0026quot; ## [5] \u0026quot;pop_gap\u0026quot; \u0026quot;gdpPercap_gap\u0026quot;\r#in contrast, the c() operator from base R, would not do the combination on an\r#elementwise basis and just adds \u0026quot;_gap\u0026quot; as a separate entry at the end of the\r#names vector\rnames(gapminder) %\u0026gt;% c(\u0026quot;_gap\u0026quot;)\r## [1] \u0026quot;country\u0026quot; \u0026quot;continent\u0026quot; \u0026quot;year\u0026quot; \u0026quot;lifeExp\u0026quot; \u0026quot;pop\u0026quot; \u0026quot;gdpPercap\u0026quot;\r## [7] \u0026quot;_gap\u0026quot;\rstr_flatten() collapses a string vector into a single string (i.e. a character vector of length 1)\nstr_flatten(c(\u0026quot;a_3\u0026quot;, \u0026quot;d_54\u0026quot;), #vector to collapse\rcollapse = \u0026quot; \u0026quot;) #character(s) to insert between each piece\r## [1] \u0026quot;a_3 d_54\u0026quot;\rstr_split() splits a string into a list or matrix of pieces based on a supplied pattern\nstr_split(c(\u0026quot;a_3\u0026quot;, \u0026quot;d_54\u0026quot;), pattern = \u0026quot;_\u0026quot;) #pattern to use for splitting. returns a list.\r## [[1]]\r## [1] \u0026quot;a\u0026quot; \u0026quot;3\u0026quot;\r## ## [[2]]\r## [1] \u0026quot;d\u0026quot; \u0026quot;54\u0026quot;\rstr_split(c(\u0026quot;a_3\u0026quot;, \u0026quot;d_54\u0026quot;), pattern = \u0026quot;_\u0026quot;, simplify = TRUE) #returns a matrix\r## [,1] [,2]\r## [1,] \u0026quot;a\u0026quot; \u0026quot;3\u0026quot; ## [2,] \u0026quot;d\u0026quot; \u0026quot;54\u0026quot;\rstr_glue() is a convenience wrapper for the glue::glue() function, which allows you to interpolate strings and values that have been assigned to names in R. To insert a value, you simply wrap it in {}:\ny \u0026lt;- Sys.Date() #store the current date\rstr_glue(\u0026quot;today is {y}\u0026quot;)\r## today is 2020-06-28\rnm \u0026lt;- \u0026quot;Craig\u0026quot;\rstr_glue(\u0026quot;Hi, my name is {nm}\u0026quot;)\r## Hi, my name is Craig\ra \u0026lt;- 5\rstr_glue(\u0026quot;a = {a}\u0026quot;)\r## a = 5\r#the base R equivalent is the paste0 function, which requires separating the\r#text and values with commas. This still accomplishes the same thing, but the\r#code doesn\u0026#39;t look quite as nice.\rpaste0(\u0026quot;today is \u0026quot;, y)\r## [1] \u0026quot;today is 2020-06-28\u0026quot;\r\r7 Manage the lengths of strings using str_length(), str_pad(), str_trunc(), \u0026amp; str_trim()\rstr_length() tells you how many characters are in each entry of a character vector\nnames(gapminder)\r## [1] \u0026quot;country\u0026quot; \u0026quot;continent\u0026quot; \u0026quot;year\u0026quot; \u0026quot;lifeExp\u0026quot; \u0026quot;pop\u0026quot; \u0026quot;gdpPercap\u0026quot;\rstr_length(names(gapminder))\r## [1] 7 9 4 7 3 9\rstr_pad() standardizes the length of strings in a character vector by padding it on the left or right ends with a specified character (by default, a space)\nstr_pad(string = names(gapminder), width = 9, #the minimum width of the string to fill/pad positions up to\rside = \u0026quot;both\u0026quot;, #the side to insert the extra characters on (left, right, or both)\rpad = \u0026quot;_\u0026quot;) #single character to use for padding \r## [1] \u0026quot;_country_\u0026quot; \u0026quot;continent\u0026quot; \u0026quot;__year___\u0026quot; \u0026quot;_lifeExp_\u0026quot; \u0026quot;___pop___\u0026quot; \u0026quot;gdpPercap\u0026quot;\rstr_trunc() standardizes string lengths in the opposite direction, by controlling the maximum width and truncating strings longer which are longer than this\nstr_trunc(string = names(gapminder), width = 7, #the maximum width to allow for strings\rside= \u0026quot;right\u0026quot;,\r#entries which have been truncated will show this to indicate that\r#something has been removed\rellipsis = \u0026quot;...\u0026quot;) \r## [1] \u0026quot;country\u0026quot; \u0026quot;cont...\u0026quot; \u0026quot;year\u0026quot; \u0026quot;lifeExp\u0026quot; \u0026quot;pop\u0026quot; \u0026quot;gdpP...\u0026quot;\rstr_trim() removes empty spaces on the ends of a string\n#add some whitespace\rpadded_names \u0026lt;- names(gapminder) %\u0026gt;% str_pad(12, \u0026quot;both\u0026quot;)\rpadded_names\r## [1] \u0026quot; country \u0026quot; \u0026quot; continent \u0026quot; \u0026quot; year \u0026quot; \u0026quot; lifeExp \u0026quot; \u0026quot; pop \u0026quot;\r## [6] \u0026quot; gdpPercap \u0026quot;\r#remove it\rstr_trim(padded_names)\r## [1] \u0026quot;country\u0026quot; \u0026quot;continent\u0026quot; \u0026quot;year\u0026quot; \u0026quot;lifeExp\u0026quot; \u0026quot;pop\u0026quot; \u0026quot;gdpPercap\u0026quot;\r\r8 Mutating strings with str_sub(), str_replace(), str_replace_all(), str_remove(), \u0026amp; str_remove_all()\rstr_sub() can also be used to replace values based on position when combined with the assignment operator (\u0026lt;-)\ngap_names \u0026lt;- gapminder %\u0026gt;% names\rstr_sub(gap_names, end = 1) \u0026lt;- \u0026quot;X_\u0026quot; #replace the 1st character with \u0026quot;X_\u0026quot;\rgap_names \r## [1] \u0026quot;X_ountry\u0026quot; \u0026quot;X_ontinent\u0026quot; \u0026quot;X_ear\u0026quot; \u0026quot;X_ifeExp\u0026quot; \u0026quot;X_op\u0026quot; ## [6] \u0026quot;X_dpPercap\u0026quot;\r#the main downside to this is that it modifies the original string vector, so\r#you would need to recreate it if you make a mistake\rgap_names \u0026lt;- gapminder %\u0026gt;% names #recreate the original gap_names vector\rTo modify a copy instead, you can use str_replace() or str_replace_all(). str_replace() replaces the 1st match in each entry of the string vector, while str_repalce_all() replaces all matches. I almost always use str_replace_all() instead of str_replace().\ngap_names \u0026lt;- gapminder %\u0026gt;% names\rstr_replace(gap_names, pattern = \u0026quot;^.{3}\u0026quot;, #match the 1st 3 characters of each string in the vector\rreplacement = \u0026quot;X_\u0026quot;) #replace them with with \u0026quot;X_\u0026quot;\r## [1] \u0026quot;X_ntry\u0026quot; \u0026quot;X_tinent\u0026quot; \u0026quot;X_r\u0026quot; \u0026quot;X_eExp\u0026quot; \u0026quot;X_\u0026quot; \u0026quot;X_Percap\u0026quot;\rgap_names #original names vector is unaffected\r## [1] \u0026quot;country\u0026quot; \u0026quot;continent\u0026quot; \u0026quot;year\u0026quot; \u0026quot;lifeExp\u0026quot; \u0026quot;pop\u0026quot; \u0026quot;gdpPercap\u0026quot;\rcountries \u0026lt;- unique(gapminder$country)\r#replace all lower case a\u0026#39;s with A\u0026#39;s flanked by \u0026quot;_\u0026quot; to make it obvious what has changed\rstr_replace(countries, pattern = \u0026quot;a\u0026quot;, replacement = \u0026quot;_A_\u0026quot;) %\u0026gt;% #replace them with with \u0026quot;_A_\u0026quot;\rhead()\r## [1] \u0026quot;Afgh_A_nistan\u0026quot; \u0026quot;Alb_A_nia\u0026quot; \u0026quot;Algeri_A_\u0026quot; \u0026quot;Angol_A_\u0026quot; ## [5] \u0026quot;Argentin_A_\u0026quot; \u0026quot;Austr_A_lia\u0026quot;\r#replace all lower case e\u0026#39;s with E\u0026#39;s\rcountries \u0026lt;- str_replace_all(countries, pattern = \u0026quot;e\u0026quot;, replacement = \u0026quot;E\u0026quot;) #replace them with with \u0026quot;E\u0026quot;\rcountries %\u0026gt;% head()\r## [1] \u0026quot;Afghanistan\u0026quot; \u0026quot;Albania\u0026quot; \u0026quot;AlgEria\u0026quot; \u0026quot;Angola\u0026quot; \u0026quot;ArgEntina\u0026quot; ## [6] \u0026quot;Australia\u0026quot;\r#to delete part of a string, you can just set the replacement value to \u0026quot;\u0026quot;\runderscores_removed \u0026lt;- str_replace_all(countries, pattern = \u0026quot;_\u0026quot;, replacement = \u0026quot;\u0026quot;) %\u0026gt;% head() #replace them with \u0026quot;\u0026quot;, effectively deleting them\runderscores_removed \r## [1] \u0026quot;Afghanistan\u0026quot; \u0026quot;Albania\u0026quot; \u0026quot;AlgEria\u0026quot; \u0026quot;Angola\u0026quot; \u0026quot;ArgEntina\u0026quot; ## [6] \u0026quot;Australia\u0026quot;\r#a shortcut for this use of str_replace() or str_replace_all() to delete regexp\r#matches are the str_remove() \u0026amp; str_remove_all()\rstr_remove_all(countries, \u0026quot;_\u0026quot;) %\u0026gt;% head() %\u0026gt;% identical(underscores_removed)\r## [1] TRUE\r\r9 You can modify the case of a string using str_to_lower(), str_to_upper(), str_to_title(), \u0026amp; str_to_sentence()\rstr_to_lower(\u0026quot;AFTER such a long HIKE in the sun, the BEER was very refreshing\u0026quot;)\r## [1] \u0026quot;after such a long hike in the sun, the beer was very refreshing\u0026quot;\rstr_to_upper(\u0026quot;after such a long hike in the sun, the beer was very refreshing\u0026quot;)\r## [1] \u0026quot;AFTER SUCH A LONG HIKE IN THE SUN, THE BEER WAS VERY REFRESHING\u0026quot;\rstr_to_title(\u0026quot;after such a long hike in the sun, the beer was very refreshing\u0026quot;)\r## [1] \u0026quot;After Such A Long Hike In The Sun, The Beer Was Very Refreshing\u0026quot;\rstr_to_sentence(\u0026quot;after such a long hike in the sun, the beer was very refreshing\u0026quot;)\r## [1] \u0026quot;After such a long hike in the sun, the beer was very refreshing\u0026quot;\r\r10 Example application: Using str_detect() or str_which() to subset with data frames\rWe saw one example of this earlier, but here are a few more. You can use these functions for subsetting because they return logical vectors or vectors of indices.\nRecall from a prior post that we can subset columns or rows using vectors of indices or logical (TRUE/FALSE) values.\nnames(gapminder)\r## [1] \u0026quot;country\u0026quot; \u0026quot;continent\u0026quot; \u0026quot;year\u0026quot; \u0026quot;lifeExp\u0026quot; \u0026quot;pop\u0026quot; \u0026quot;gdpPercap\u0026quot;\rgapminder[, c(1, 4, 6)] %\u0026gt;% names() #columns 1, 4, 6\r## [1] \u0026quot;country\u0026quot; \u0026quot;lifeExp\u0026quot; \u0026quot;gdpPercap\u0026quot;\rgapminder[, c(TRUE, FALSE, TRUE, FALSE, FALSE, FALSE)] %\u0026gt;% names() #columns 1 \u0026amp; 3\r## [1] \u0026quot;country\u0026quot; \u0026quot;year\u0026quot;\rWe can take advantage of this to select columns using a regexp via the str_detect() or str_which() functions\ngap_names \u0026lt;- names(gapminder)\rgapminder[, str_detect(gap_names, \u0026quot;^c\u0026quot;)] %\u0026gt;% names() #columns with names starting with (^) a c\r## [1] \u0026quot;country\u0026quot; \u0026quot;continent\u0026quot;\rgapminder[, str_which(gap_names, \u0026quot;p$\u0026quot;)] %\u0026gt;% names() #columns with names that end ($) in p\r## [1] \u0026quot;lifeExp\u0026quot; \u0026quot;pop\u0026quot; \u0026quot;gdpPercap\u0026quot;\rWhile it’s unlikley that you would ever do this in practice given that dplyr::select() makes it a bit easier, the same logic can be used for subsetting rows using regexps, which is something that dplyr::filter() doesn’t do on its own.\n#get the indices of the rows with data for Canada or (|) Italy\rCI_rows \u0026lt;- str_which(gapminder$country, \u0026quot;Canada|Italy\u0026quot;) CI_rows %\u0026gt;% head() #view the 1st 6\r## [1] 241 242 243 244 245 246\rgapminder[CI_rows, ] %\u0026gt;% head\r## # A tibble: 6 x 6\r## country continent year lifeExp pop gdpPercap\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Canada Americas 1952 68.8 14785584 11367.\r## 2 Canada Americas 1957 70.0 17010154 12490.\r## 3 Canada Americas 1962 71.3 18985849 13462.\r## 4 Canada Americas 1967 72.1 20819767 16077.\r## 5 Canada Americas 1972 72.9 22284500 18971.\r## 6 Canada Americas 1977 74.2 23796400 22091.\r#str_detect() yields the same results\ridentical(gapminder[str_detect(gapminder$country, \u0026quot;Canada|Italy\u0026quot;), ],\rgapminder[str_which(gapminder$country, \u0026quot;Canada|Italy\u0026quot;), ]\r)\r## [1] TRUE\r#or if you prefer dplyr::filter() to the square bracket notation, use\r#str_detect(), an advantage of this is that when working within filter you don\u0026#39;t\r#need to use the dataframe$variable syntax since it knows to look for the\r#variable in the data frame you\u0026#39;ve piped in (gapminder)\ridentical(\r#str_detect() combined with dplyr::filter()\rgapminder %\u0026gt;% filter(str_detect(country, \u0026quot;Canada|Italy\u0026quot;)),\r#str_detect() with [] from base R\rgapminder[str_detect(gapminder$country, \u0026quot;Canada|Italy\u0026quot;), ]\r)\r## [1] TRUE\rYou can see from these examples that stringr functions alone can do some of the same things that dplyr functions can, as well as building upon the functionality of some dplyr functions like filter().\nCongrats! If you’ve made it here working with strings just got a whole lot easier for you…\n\r11 Navigation\rClick here to go back to the previous post on joining data with dplyr. A link to the next one will be added as soon as I have time to write it.\n\r12 Notes\r\rIf you want to up your string manipulation game even more, you can learn more from the strings chapter of R 4 data science here or the stringr package documentation on CRAN, and some base R string processing functions here.\n\rData scientist \u0026amp; skilled R developer Garrick Aden-Buie has also built an Addin for R studio, via the regexplain package that can make it easier to work with regexps, which you may find helpful.\n\rWhile it is a good idea to develop some direct knowledge of regexps for simple cases like the ones we’ve explored here, the rebus and/or Regularity packages can make building complex regexps quite a bit easier and I recommend checking them out if you’ll be working with a lot of unstructured text in your research.\n\rThis isn’t something you’ll need to use often, but you can remove special characters from a string using the iconv() function from base R.\n\r\rThank you for visiting my blog. I welcome any suggestions for future posts, comments or other feedback you might have. Feedback from beginners and science students/trainees (or with them in mind) is especially helpful in the interest of making this guide even better for them.\n\r\rThis applies to escaping characters in R. Escaping characters and building regex statements in other languages may require slightly different syntax↩\n\ror any other type of vector if we convert it to a character class temporarily↩\n\r\r\r","date":1593302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593302400,"objectID":"d521537900d24dbca6b830b61110fd48","permalink":"/post/2020-06-28-asgr-2-3-string-manipulation/","publishdate":"2020-06-28T00:00:00Z","relpermalink":"/post/2020-06-28-asgr-2-3-string-manipulation/","section":"post","summary":"1 TL;DR\r2 Introduction\r3 Regular expressions\r4 Detecting pattern matches with str_detect(), str_which(), str_count(), and str_locate().\r5 Subsetting strings \u0026amp; data frames with str_subset(), str_sub(), str_match(), \u0026amp; str_extract().\r6 Combining and splitting strings using str_c(), str_flatten(), str_split(), \u0026amp; str_glue().\r7 Manage the lengths of strings using str_length(), str_pad(), str_trunc(), \u0026amp; str_trim()\r8 Mutating strings with str_sub(), str_replace(), str_replace_all(), str_remove(), \u0026amp; str_remove_all()\r9 You can modify the case of a string using str_to_lower(), str_to_upper(), str_to_title(), \u0026amp; str_to_sentence()\r10 Example application: Using str_detect() or str_which() to subset with data frames\r11 Navigation\r12 Notes\r\r\r1 TL;DR\rBeing able to work with character strings is an essential skill in data analysis and science.","tags":["R","R basics"],"title":"A Scientist's Guide to R: Step 2.3 - string manipulation and regex","type":"post"},{"authors":null,"categories":["R","Reproducible Research","Data Cleaning and Transformation"],"content":"\r\r1 TL;DR\r2 Introduction\r2.1 setup\r\r3 left_join()\r4 right_join()\r5 full_join()\r6 inner_join()\r7 semi_join()\r8 anti_join()\r9 building data frames using bind_rows() or bind_cols()\r9.1 add_row()\r\r10 joining 3 or more data frames\r11 merge()\r12 Navigation\r13 Notes\r\r\r1 TL;DR\rOut in the real world you may often find yourself working with data from multiple sources. It will probably be stored in separate files and you’ll need to combine them before you can attempt to answer any of your research questions. This post will show you how you can combine data frames using another set of dplyr functions called joins.\n\r2 Introduction\rThe 6th post of the Scientist’s Guide to R series is all about using joins to combine data. While tidy data organized nicely into a single .csv or .xlsx spreadsheet may be provided to you in courses, in the real world you’ll often collect data from multiple sources often only containing one or two similar “key” columns (like subject ID #) and have to combine pieces of them to do anything interesting. This type of data is called relational data, since the datasets are related through common key column(s). Relational databases are how most data are stored in modern non-academic organizations.\nFortunately, a package we’re already familiar with from a couple of posts ago, dplyr, also has a set of functions for combining data with functions called “joins”. For this post we will cover the 6 most common joins:\nleft_join(x, y) which combines all columns in data frame x with those in data frame y but only retains rows from x.\n\rright_join(x, y) also keeps all columns but operates in the opposite direction, returning only rows from y.\n\rfull_join(x, y) combines all columns of x with all columns of y and retains all rows from both data frames.\n\rinner_join(x, y) combines all columns present in either x or y but only retains rows that are present in both data frames.\n\rsemi_join(x, y) returns the columns from x only and retains rows of x that are also present in y\n\ranti_join(x, y) returns the columns from x only and retains rows of x that are not present in y.\n\r\rA nice design feature of these functions is that their names and behaviour were inspired by analogous functions for joining data in the ubiquitous database management programming language “Stuctured Query Language” (SQL). Learning dplyr therefore also makes SQL easier to learn in the future, which will be helpful if you ever want to work with data for a living.\nIn case you find yourself working in an environment where only base R is available, we’ll also cover how to join data using the base R function merge().\nAside from specifying the data frames to be joined, one other thing we need to do is specify the key column(s) to be used for aligning the rows prior to joining the data.\nKey columns are specified with the by argument, e.g. inner_join(x, y, by = \"subject_id\") adds columns of y to x for all rows where the values of the “subject_id” column (present in each data frame) match. If the name of the key column differs between the data frames, e.g. “subject_id” in x and “subj_id” in y, then you have to specify both names using by = c(\"subject_id\" = \"subj_id\") so that the functions know which columns to compare.\nA nice feature of the *_join() functions is that if you don’t specify the by argument they will assume that columns with the same names across x and y are key columns. This is very convenient when the columns with the same names in fact contain the same type of values.\n2.1 setup\rTo demonstrate the use of the join functions, I’ll prepare an example of relational data using the gapminder dataset for the year 2007 aggregated to the continent level. In this representation of the data, the life expectancy, population, and gdpPercap are stored in separate data frames (called life_df, pop_df, and gdp_df respectively). This sort of arrangement is closer to what you might encouter if the gapminder data were stored in a relational database.\nlibrary(gapminder) #contains the gapminder data\rlibrary(dplyr) #functions for manipulating and joining data\rlife_df \u0026lt;- gapminder %\u0026gt;%\rfilter(year \u0026gt;= 1997 \u0026amp; year \u0026lt;= 2007) %\u0026gt;%\rgroup_by(continent, year) %\u0026gt;% summarise(mean_life_expectancy = mean(lifeExp, na.rm = T)) %\u0026gt;% filter(continent != \u0026quot;Asia\u0026quot;) %\u0026gt;% ungroup()\rpop_df \u0026lt;- gapminder %\u0026gt;% filter(year \u0026gt;= 1997 \u0026amp; year \u0026lt;= 2007) %\u0026gt;%\rgroup_by(continent, year) %\u0026gt;% summarise(total_population = sum(pop, na.rm = T)) %\u0026gt;% filter(continent != \u0026quot;Europe\u0026quot;) %\u0026gt;%\rungroup()\rgdp_df \u0026lt;- gapminder %\u0026gt;% filter(year == 1997 | year == 2007) %\u0026gt;%\rgroup_by(continent, year) %\u0026gt;% summarise(total_gdpPercap = sum(gdpPercap, na.rm = T)) %\u0026gt;% ungroup()\rRecall that we can print a view of the structure of each data frame using the glimpse function from dplyr\nlife_df %\u0026gt;% glimpse()\r## Observations: 12\r## Variables: 3\r## $ continent \u0026lt;fct\u0026gt; Africa, Africa, Africa, Americas, Americas, Am...\r## $ year \u0026lt;int\u0026gt; 1997, 2002, 2007, 1997, 2002, 2007, 1997, 2002...\r## $ mean_life_expectancy \u0026lt;dbl\u0026gt; 53.59827, 53.32523, 54.80604, 71.15048, 72.422...\rpop_df %\u0026gt;% glimpse()\r## Observations: 12\r## Variables: 3\r## $ continent \u0026lt;fct\u0026gt; Africa, Africa, Africa, Americas, Americas, Americ...\r## $ year \u0026lt;int\u0026gt; 1997, 2002, 2007, 1997, 2002, 2007, 1997, 2002, 20...\r## $ total_population \u0026lt;dbl\u0026gt; 743832984, 833723916, 929539692, 796900410, 849772...\rgdp_df %\u0026gt;% glimpse()\r## Observations: 10\r## Variables: 3\r## $ continent \u0026lt;fct\u0026gt; Africa, Africa, Americas, Americas, Asia, Asia, Eur...\r## $ year \u0026lt;int\u0026gt; 1997, 2007, 1997, 2007, 1997, 2007, 1997, 2007, 199...\r## $ total_gdpPercap \u0026lt;dbl\u0026gt; 123695.50, 160629.70, 222232.52, 275075.79, 324525....\ror print them to the console using the names\nlife_df\r## # A tibble: 12 x 3\r## continent year mean_life_expectancy\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Africa 1997 53.6\r## 2 Africa 2002 53.3\r## 3 Africa 2007 54.8\r## 4 Americas 1997 71.2\r## 5 Americas 2002 72.4\r## 6 Americas 2007 73.6\r## 7 Europe 1997 75.5\r## 8 Europe 2002 76.7\r## 9 Europe 2007 77.6\r## 10 Oceania 1997 78.2\r## 11 Oceania 2002 79.7\r## 12 Oceania 2007 80.7\rpop_df \r## # A tibble: 12 x 3\r## continent year total_population\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Africa 1997 743832984\r## 2 Africa 2002 833723916\r## 3 Africa 2007 929539692\r## 4 Americas 1997 796900410\r## 5 Americas 2002 849772762\r## 6 Americas 2007 898871184\r## 7 Asia 1997 3383285500\r## 8 Asia 2002 3601802203\r## 9 Asia 2007 3811953827\r## 10 Oceania 1997 22241430\r## 11 Oceania 2002 23454829\r## 12 Oceania 2007 24549947\rgdp_df \r## # A tibble: 10 x 3\r## continent year total_gdpPercap\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Africa 1997 123695.\r## 2 Africa 2007 160630.\r## 3 Americas 1997 222233.\r## 4 Americas 2007 275076.\r## 5 Asia 1997 324525.\r## 6 Asia 2007 411610.\r## 7 Europe 1997 572303.\r## 8 Europe 2007 751634.\r## 9 Oceania 1997 48048.\r## 10 Oceania 2007 59620.\rFrom these printouts we can tell that each data frame only has values for some of the continents and/or some of the years that are present in the full gapminder data. I’ve structured them this way so that it is easier to see how they are joined, as you’ll soon find out.\n\r\r3 left_join()\rIf we wanted to add the population data for each continent that appears in the life expectancy data frame, we could use left_join() and the key columns continent and year.\n#all columns in x will be returned and #all columns of y will be returned #for rows in the key column that have values in y that match those in x\rleft_join(x = life_df, y = pop_df, by = c(\u0026quot;continent\u0026quot;, \u0026quot;year\u0026quot;))\r## # A tibble: 12 x 4\r## continent year mean_life_expectancy total_population\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Africa 1997 53.6 743832984\r## 2 Africa 2002 53.3 833723916\r## 3 Africa 2007 54.8 929539692\r## 4 Americas 1997 71.2 796900410\r## 5 Americas 2002 72.4 849772762\r## 6 Americas 2007 73.6 898871184\r## 7 Europe 1997 75.5 NA\r## 8 Europe 2002 76.7 NA\r## 9 Europe 2007 77.6 NA\r## 10 Oceania 1997 78.2 22241430\r## 11 Oceania 2002 79.7 23454829\r## 12 Oceania 2007 80.7 24549947\r# if the key columns have different names, you can tell the join function which\r# columns to use with the equality operator\r#1st I\u0026#39;ll just rename the continent column for pedagogical purposes\rlife_df_renamed \u0026lt;- rename(life_df, cont = continent)\rnames(life_df_renamed)\r## [1] \u0026quot;cont\u0026quot; \u0026quot;year\u0026quot; \u0026quot;mean_life_expectancy\u0026quot;\rleft_join(x = life_df_renamed, y = pop_df, #since the continent column is now called \u0026quot;cont\u0026quot; in life_df, #we have to tell left_join which columns to match on.\r#You\u0026#39;ll get an error if you try by = c(\u0026quot;continent\u0026quot;, \u0026quot;year\u0026quot;) this time\rby = c(\u0026quot;cont\u0026quot; = \u0026quot;continent\u0026quot;,\r\u0026quot;year\u0026quot;))\r## # A tibble: 12 x 4\r## cont year mean_life_expectancy total_population\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Africa 1997 53.6 743832984\r## 2 Africa 2002 53.3 833723916\r## 3 Africa 2007 54.8 929539692\r## 4 Americas 1997 71.2 796900410\r## 5 Americas 2002 72.4 849772762\r## 6 Americas 2007 73.6 898871184\r## 7 Europe 1997 75.5 NA\r## 8 Europe 2002 76.7 NA\r## 9 Europe 2007 77.6 NA\r## 10 Oceania 1997 78.2 22241430\r## 11 Oceania 2002 79.7 23454829\r## 12 Oceania 2007 80.7 24549947\rNote that the total_population column from pop_df has been joined to life_df based on matching values in the key columns that appear in both data frames. Since we used a left join and Europe is listed as a continent in life_df, the row for it is returned in the joined data frame. However, because there are no population values for Europe in pop_df, these rows are filled with NAs under the total_population column.\n\r4 right_join()\rA right join is basically the same thing as a left_join but in the other direction, where the 1st data frame (x) is joined to the 2nd one (y), so if we wanted to add life expectancy and GDP per capita data we could either use:\na right_join() with life_df on the left side and gdp_df on the right side, or\n\ra left_join() with gdp_df on the left side and life_df on the right side\n\r\r… and get the same result with only the columns arranged differently…\n#Also, since the key columns have the same names in each data frame we don\u0026#39;t have to specify them\r#we can also pipe in the 1st dataframe using the pipe operator (`%\u0026gt;%`)\rrj \u0026lt;- life_df %\u0026gt;% right_join(gdp_df) rj\r## # A tibble: 10 x 4\r## continent year mean_life_expectancy total_gdpPercap\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Africa 1997 53.6 123695.\r## 2 Africa 2007 54.8 160630.\r## 3 Americas 1997 71.2 222233.\r## 4 Americas 2007 73.6 275076.\r## 5 Asia 1997 NA 324525.\r## 6 Asia 2007 NA 411610.\r## 7 Europe 1997 75.5 572303.\r## 8 Europe 2007 77.6 751634.\r## 9 Oceania 1997 78.2 48048.\r## 10 Oceania 2007 80.7 59620.\rlj \u0026lt;- gdp_df %\u0026gt;% left_join(life_df)\ridentical(rj, lj)\r## [1] FALSE\ridentical(rj, select(lj, 1, 2, 4, 3))\r## [1] TRUE\rThis time there are missing values for Asia’s mean life expectancy because Asia does not appear in the continent column of life_df (but it does appear in gdp_df), and no rows for the year 2002 because 2002 does not appear in the “year” key column of gdp_df.\n\r5 full_join()\rAfter aligning rows by matches in the key column(s), a full join retains all rows that appear in x or y\nlife_df %\u0026gt;% full_join(gdp_df) %\u0026gt;% arrange(year, continent) #sort by year then by continent\r## # A tibble: 14 x 4\r## continent year mean_life_expectancy total_gdpPercap\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Africa 1997 53.6 123695.\r## 2 Americas 1997 71.2 222233.\r## 3 Asia 1997 NA 324525.\r## 4 Europe 1997 75.5 572303.\r## 5 Oceania 1997 78.2 48048.\r## 6 Africa 2002 53.3 NA ## 7 Americas 2002 72.4 NA ## 8 Europe 2002 76.7 NA ## 9 Oceania 2002 79.7 NA ## 10 Africa 2007 54.8 160630.\r## 11 Americas 2007 73.6 275076.\r## 12 Asia 2007 NA 411610.\r## 13 Europe 2007 77.6 751634.\r## 14 Oceania 2007 80.7 59620.\rThe output now includes rows for the year 2002, which were present in life_df but not in gdp_df. It also includes rows for Asia, which are present in gdp_df but are missing from life_df. As you can see, a full join retains all of the data, filling in missing values where necessary.\nLeft joins, right joins, and full joins are also collectively referred to as outer joins because they retain the observations from at least one of the joined tables. This excellent set of diagrams from R for Data Science (R4DS) can help you build an intuitive sense of how these outer joins work:\nsource: https://r4ds.had.co.nz/relational-data.html\n\r\r6 inner_join()\rOften you may only want to work with rows which have matching entries in both data sources. Since only some rows are retained, we’re no longer dealing with an outer join. In this case you could use inner_join(), which returns all rows in both x and y but only rows with that appear in the key columns of both data frames.\nlife_df %\u0026gt;% inner_join(gdp_df)\r## # A tibble: 8 x 4\r## continent year mean_life_expectancy total_gdpPercap\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Africa 1997 53.6 123695.\r## 2 Africa 2007 54.8 160630.\r## 3 Americas 1997 71.2 222233.\r## 4 Americas 2007 73.6 275076.\r## 5 Europe 1997 75.5 572303.\r## 6 Europe 2007 77.6 751634.\r## 7 Oceania 1997 78.2 48048.\r## 8 Oceania 2007 80.7 59620.\rThis time we only get data for 1997 and 2007 even though life_df has values for 2002 because gdp_df did not contain any data for 2002. We also don’t get any data for Asia, which was present in gdp_df, because there was no data for Asia in life_df.\nThis diagram from R4DS shows you another example of how an inner join works:\nsource: https://r4ds.had.co.nz/relational-data.html\n\r\r7 semi_join()\rSo far we’ve been filterting rows based on matches in the key columns but extacting all columns from both data frames. The other two main dplyr join functions are available for situations where you only want to keep the columns of one data frame (x).\nsemi_join(x, y) filters the rows of x to retain only those that also appear in y\nlife_df %\u0026gt;% semi_join(pop_df)\r## # A tibble: 9 x 3\r## continent year mean_life_expectancy\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Africa 1997 53.6\r## 2 Africa 2002 53.3\r## 3 Africa 2007 54.8\r## 4 Americas 1997 71.2\r## 5 Americas 2002 72.4\r## 6 Americas 2007 73.6\r## 7 Oceania 1997 78.2\r## 8 Oceania 2002 79.7\r## 9 Oceania 2007 80.7\rThis time we only get the columns from life_df and we’ve dropped rows for Europe because Europe only appears under the continent key column for life_df and not pop_df.\nHere is the R4DS semi-join diagram showing that only columns and rows from table 1 are retained as a second example:\nsource: https://r4ds.had.co.nz/relational-data.html\n\r\r8 anti_join()\rIn contrast to semi joins, anti joins return the rows x that do not appear in y.\nlife_df %\u0026gt;% anti_join(pop_df)\r## # A tibble: 3 x 3\r## continent year mean_life_expectancy\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Europe 1997 75.5\r## 2 Europe 2002 76.7\r## 3 Europe 2007 77.6\rAs you can see, anti joins can be very useful if you want to know which rows are excluded due to mismatches in the key columns. Checking for consistencies and inconsistencies between data sources is an important part of the data cleaning process and can often help to uncover data entry or coding errors that should be fixed prior to conducting any analyses.\nR4DS diagram showing how the anti-join works:\nsource: https://r4ds.had.co.nz/relational-data.html\n\r\r9 building data frames using bind_rows() or bind_cols()\rIf you have two data frames with the same columns, you can combine their rows using dplyr::bind_rows() or rbind(). rbind() is best suited for rowwise combinations of vectors or matrices, while bind_rows() is better for combining data frames. In my experience, the most common reason to use either would be to add data for new cases to a data frame, so I will only demonstrate bind_rows() here. For example, say we decided to add the gapminder life expectancy data for Asia to life_df:\n#first get the data for asia from the original gapminder dataset\rasia_life_exp \u0026lt;- gapminder %\u0026gt;%\rfilter(continent == \u0026quot;Asia\u0026quot;,\rbetween(year, 1997, 2007)) %\u0026gt;% #between is a shortcut for (column \u0026gt;= value \u0026amp; column \u0026lt;= value) group_by(continent, year) %\u0026gt;% summarise(mean_life_expectancy = mean(lifeExp, na.rm = T))\r#then add it to the top of life_df\rbind_rows(asia_life_exp, life_df) \r## # A tibble: 15 x 3\r## # Groups: continent [5]\r## continent year mean_life_expectancy\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Asia 1997 68.0\r## 2 Asia 2002 69.2\r## 3 Asia 2007 70.7\r## 4 Africa 1997 53.6\r## 5 Africa 2002 53.3\r## 6 Africa 2007 54.8\r## 7 Americas 1997 71.2\r## 8 Americas 2002 72.4\r## 9 Americas 2007 73.6\r## 10 Europe 1997 75.5\r## 11 Europe 2002 76.7\r## 12 Europe 2007 77.6\r## 13 Oceania 1997 78.2\r## 14 Oceania 2002 79.7\r## 15 Oceania 2007 80.7\rIf we instead had two data frames with the same cases/rows but different columns (and no common key columns to enable the use of joins), we could combine them using dplyr::bind_cols() or cbind(). Again, bind_cols() is preferred over cbind() for combining data frames by column. However, if you’re only adding a few columns, dplyr::mutate() or a \"df$newcol \u0026lt;- newcol\" statement per column to add would also work.\nFor example, if we were provided with each column of the gapminder dataframe as separate vectors (each with values in identical order), without any common key columns among any of the fragments, we could reconstruct the orginal gapminder data frame using bind_cols(), e.g.:\nctry \u0026lt;- gapminder$country\rctin \u0026lt;- gapminder$continent\ryr \u0026lt;- gapminder$year\rle \u0026lt;- gapminder$lifeExp\rpop \u0026lt;- gapminder$pop\rgdp \u0026lt;- gapminder$gdpPercap\rbound_gap \u0026lt;- bind_cols(\u0026quot;country\u0026quot; = ctry, #add names using \u0026quot;name\u0026quot; = vector syntax\r\u0026quot;continent\u0026quot; = ctin, \u0026quot;year\u0026quot; = yr, \u0026quot;lifeExp\u0026quot; = le, \u0026quot;pop\u0026quot; = pop, \u0026quot;gdpPercap\u0026quot; = gdp)\ridentical(gapminder, bound_gap)\r## [1] TRUE\r#the main difference between cbind() and bind_rows() is that bind_rows returns a\r#tibble\rWhen considering the use of rbind()/bind_rows() or cbind()/bind_cols() you must keep in mind that because no key columns are checked for matching values you need to be sure that the columns (when binding rows) or rows (when binding columns) are arranged in exactly the same order for each portion of the dataframe before binding the pieces together. This approach can be very error prone, particularly in cases where data cleaning or analysis is being done collaboratively.\nFor this reason I strongly recommend that you make use of key columns and combine data using joins whenever possible.\n9.1 add_row()\rIf you only wanted to add a single row to a data frame, you can use tibble::add_row() and (recall that the tibble package is also part of the tidyverse). Let’s say (hypothetically of course) we found out that the mean life expectancy for countries in Africa had gone up to 56 for the year 2012. We could add this row as follows:\nupdated_life_df \u0026lt;- life_df %\u0026gt;% tibble::add_row(continent = \u0026quot;Africa\u0026quot;, year = 2012, mean_life_expectancy = 56) %\u0026gt;% #specify values to be added using column = value syntax\rarrange(continent, year)\rupdated_life_df #now the value we added appears in the printout of the data frame\r## # A tibble: 13 x 3\r## continent year mean_life_expectancy\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Africa 1997 53.6\r## 2 Africa 2002 53.3\r## 3 Africa 2007 54.8\r## 4 Africa 2012 56 ## 5 Americas 1997 71.2\r## 6 Americas 2002 72.4\r## 7 Americas 2007 73.6\r## 8 Europe 1997 75.5\r## 9 Europe 2002 76.7\r## 10 Europe 2007 77.6\r## 11 Oceania 1997 78.2\r## 12 Oceania 2002 79.7\r## 13 Oceania 2007 80.7\r\r\r10 joining 3 or more data frames\rJoining 3 or more data frames is also pretty easy using dplyr, just pipe the output of a join into another join. This is incredibly simple if they all have key columns with the same names. For example, if I wanted to combine life_df, pop_df and gdp_df and keep rows present in any of them, all I have to do is:\ncombined_data \u0026lt;- life_df %\u0026gt;% full_join(pop_df) %\u0026gt;% #life_df is inserted as the 1st argument i.e. data frame \u0026quot;x\u0026quot; of the full_join\rfull_join(gdp_df) #the output of the previous join is passed to the first argument of the second full_join\rcombined_data #now it\u0026#39;s all together, using what could be condensed into a single line of code\r## # A tibble: 15 x 5\r## continent year mean_life_expectancy total_population total_gdpPercap\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Africa 1997 53.6 743832984 123695.\r## 2 Africa 2002 53.3 833723916 NA ## 3 Africa 2007 54.8 929539692 160630.\r## 4 Americas 1997 71.2 796900410 222233.\r## 5 Americas 2002 72.4 849772762 NA ## 6 Americas 2007 73.6 898871184 275076.\r## 7 Europe 1997 75.5 NA 572303.\r## 8 Europe 2002 76.7 NA NA ## 9 Europe 2007 77.6 NA 751634.\r## 10 Oceania 1997 78.2 22241430 48048.\r## 11 Oceania 2002 79.7 23454829 NA ## 12 Oceania 2007 80.7 24549947 59620.\r## 13 Asia 1997 NA 3383285500 324525.\r## 14 Asia 2002 NA 3601802203 NA ## 15 Asia 2007 NA 3811953827 411610.\r# Alternatively, I could use full_join(life_df, pop_df) %\u0026gt;% full_join(gdp_df)\r# or full_join(full_join(life_df, pop_df), gdp_df)\rThis is another example of how nested function calls can be easier to read when written in series with the pipe operator (%\u0026gt;%), which I covered in more detail here.\n\r11 merge()\rIn the very unlikely event that you find yourself having to combine relational data but are working on a computer that only has base R and no admin priviledges to enable you to install dplyr, have no fear! You can use the merge() function from base R to perform left joins, right joins, inner joins, and full joins.\nIn addition to the x and y arguments that need to be used to specify the data frames to be joined and the by argument that indicates which key columns to use, the type of join is determined via the all.x and all.y arguments\n# inner join/merge\rij_merge \u0026lt;- merge(life_df, pop_df,\rall.x = F, all.y = F) #the default merge/join is an inner join, in which all.x and all.y are both FALSE\rij_dplyr \u0026lt;- inner_join(life_df, pop_df) %\u0026gt;% as.data.frame() #removes the tibble class which the merge result doesn\u0026#39;t have\rall.equal(ij_merge, ij_dplyr)\r## [1] TRUE\r#full join/merge\rfj_merge \u0026lt;- merge(life_df, pop_df,\rall.x = T, all.y = T) #set (all.x = T and all.y = T) or (all = T), to perform a full join\rfj_dplyr \u0026lt;- full_join(life_df, pop_df) %\u0026gt;% as.data.frame() %\u0026gt;%\rarrange(continent)\rall.equal(fj_merge, fj_dplyr)\r## [1] TRUE\r#for a left join, all.x should be set to TRUE and all.y to FALSE\rlj_merge \u0026lt;- merge(life_df, pop_df,\rall.x = T, all.y = F) lj_dplyr \u0026lt;- left_join(life_df, pop_df) %\u0026gt;% as.data.frame()\rall.equal(lj_merge, lj_dplyr)\r## [1] TRUE\r#for a right join, all.x should be set to FALSE and all.y to TRUE\rrj_merge \u0026lt;- merge(life_df, pop_df,\rall.x = F, all.y = T) rj_dplyr \u0026lt;- right_join(life_df, pop_df) %\u0026gt;% as.data.frame()\rall.equal(rj_merge, rj_dplyr)\r## [1] TRUE\rWhy bother with dplyr joins if merge() can do so much? Simply because the dplyr code is easier to read and the dplyr functions are faster. Unfortunately, merge also can’t handle semi joins or anti joins, so you’d have to do a bit more work to achieve the same results without dplyr.\nIf you’ve followed along, congratulations! You now know how the basics of combining data frames in with dplyr joins and base R. Just practice these operations a few times on your own and joins will seem trivial!\n\r12 Navigation\rClick here to go back to the previous post on reshaping data with tidyr or here) to go to the next post on string manipulation with stringr.\n\r13 Notes\r\rYou can learn more about joins, working with relational data, and the set operation functions in R here.\n\rYou can learn more about SQL here.\n\rThe author of dplyr, Hadley Wickham, also wrote the dbplyr package, which translates dplyr to SQL for you so you can actually query databases directly using dplyr code or even view the SQL code translations. You can learn more about dbplyr here. The code translation is really helpful if you’re trying to learn SQL.\n\r\rThank you for visiting my blog. I welcome any suggestions for future posts, comments or other feedback you might have. Feedback from beginners and science students/trainees (or with them in mind) is especially helpful in the interest of making this guide even better for them.\n\r","date":1585440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585440000,"objectID":"d6149875cdc3efe8c260bb85bdd8f5eb","permalink":"/post/2020-03-29-asgr-2-2-joining-data/","publishdate":"2020-03-29T00:00:00Z","relpermalink":"/post/2020-03-29-asgr-2-2-joining-data/","section":"post","summary":"1 TL;DR\r2 Introduction\r2.1 setup\r\r3 left_join()\r4 right_join()\r5 full_join()\r6 inner_join()\r7 semi_join()\r8 anti_join()\r9 building data frames using bind_rows() or bind_cols()\r9.1 add_row()\r\r10 joining 3 or more data frames\r11 merge()\r12 Navigation\r13 Notes\r\r\r1 TL;DR\rOut in the real world you may often find yourself working with data from multiple sources. It will probably be stored in separate files and you’ll need to combine them before you can attempt to answer any of your research questions.","tags":["R","R basics"],"title":"A Scientist's Guide to R: Step 2.2 - Joining Data with dplyr","type":"post"},{"authors":null,"categories":["R","Reproducible Research","Data Cleaning and Transformation"],"content":"\r\r1 TL;DR\r2 Introduction\r2.1 “long” data, “wide” data, and “tidy” data\r\r3 pivot_longer()\r4 pivot_wider()\r5 unite()\r6 separate()\r7 Navigation\r8 Notes\r\r\r1 TL;DR\rIn the 5th post of the Scientist’s Guide to R series we explore using the tidyr package to reshape data. You’ll learn all about splitting and combining columns and how to do wide to long or long to wide transformations.\n\r2 Introduction\rIn the 5th post of the Scientist’s Guide to R series we explore using the tidyr package to reshape data. As you’ll see, this is really helpful if you are working with repeated-measures data, which is pretty common these days.\nSpecifically, we will be learning how to use 4 core functions of the tidyr package to:\nPerform “wide-to-long” transformations using pivot_longer(), converting multiple columns of the same measure into a single value column and associated key column.\n\rSpread a pair of value and key columns across multiple columns, AKA “long-to-wide” transformations, using pivot_wider().\n\rCombine multiple columns into a single column using unite().\n\rPerform the reverse operation to separate() a complex column into multiple simple columns.\n\r\rAdditional relevant functions that build on this foundation will also be covered where appropriate. If some of these terms don’t mean very much to you now don’t worry. Hopefully they will by the time you’re done reading this post!\nIn this post I will assume you know how to use the pipe operator (%\u0026gt;%), which I covered here.\nGetting repeated-measures data ready for analysis will be no trouble at all once you’ve mastered these functions.\nThe examples below will make use of 2 datasets provided by R packages: the gapminder life expectancy data from the gapminder package and the OBrienKaiser data from the carData package. Since carData is automatically also loaded with the car package which we will be using later in the series for regression diagnostics (car = companion to applied regression), we’ll import the OBrienKaiser data indirectly via the car package.\nThese 2 datasets were chosen because they both contain repeated measurements. One, gapminder, is by default in the so-called “long” form. While the other, OBrienKaiser, is by default in the so-called “wide” form.\nRecall that we load a package using the library() function, which imports any data structures and functions from the package and makes them available for use in the current R session.\nlibrary(gapminder) #load the gapminder package which mostly just contains the gapminder dataset\rlibrary(car) #load the car package with the associated OBrienKaiser data\rNaturally the 1st thing you should do after importing any data is to examine its structure using either str() or dplyr::glimpse(), so that’s what we’ll do next. Data from packages also usually comes with helpful documentation you can view using ?data_object_name.\n#since I prefer to use glimpse() over str() we\u0026#39;ll also load the tidyverse\r#package collection (glimpse is included with both tibble and dplyr) which also\r#loads the tidyr package we will need later anyways. #For now this gives us access to both the pipe operator and the glimpse function.\rlibrary(tidyverse)\r## -- Attaching packages ---------------------------------------------------------------------------------------------------------- tidyverse 1.3.0 --\r## v ggplot2 3.2.1 v purrr 0.3.3\r## v tibble 2.1.3 v dplyr 0.8.4\r## v tidyr 1.0.2 v stringr 1.4.0\r## v readr 1.3.1 v forcats 0.5.0\r## Warning: package \u0026#39;forcats\u0026#39; was built under R version 3.6.3\r## -- Conflicts ------------------------------------------------------------------------------------------------------------- tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\r## x dplyr::recode() masks car::recode()\r## x purrr::some() masks car::some()\rgapminder %\u0026gt;% glimpse\r## Observations: 1,704\r## Variables: 6\r## $ country \u0026lt;fct\u0026gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afgha...\r## $ continent \u0026lt;fct\u0026gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asi...\r## $ year \u0026lt;int\u0026gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 199...\r## $ lifeExp \u0026lt;dbl\u0026gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 4...\r## $ pop \u0026lt;int\u0026gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372,...\r## $ gdpPercap \u0026lt;dbl\u0026gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.113...\rOBrienKaiser %\u0026gt;% glimpse\r## Observations: 16\r## Variables: 17\r## $ treatment \u0026lt;fct\u0026gt; control, control, control, control, control, A, A, A, A, ...\r## $ gender \u0026lt;fct\u0026gt; M, M, M, F, F, M, M, F, F, M, M, M, F, F, F, F\r## $ pre.1 \u0026lt;dbl\u0026gt; 1, 4, 5, 5, 3, 7, 5, 2, 3, 4, 3, 6, 5, 2, 2, 4\r## $ pre.2 \u0026lt;dbl\u0026gt; 2, 4, 6, 4, 4, 8, 5, 3, 3, 4, 3, 7, 5, 2, 2, 5\r## $ pre.3 \u0026lt;dbl\u0026gt; 4, 5, 5, 7, 6, 7, 6, 5, 4, 5, 4, 8, 6, 3, 3, 7\r## $ pre.4 \u0026lt;dbl\u0026gt; 2, 3, 7, 5, 4, 9, 4, 3, 6, 3, 2, 6, 8, 1, 4, 5\r## $ pre.5 \u0026lt;dbl\u0026gt; 1, 4, 7, 4, 3, 9, 5, 2, 4, 4, 3, 3, 6, 2, 4, 4\r## $ post.1 \u0026lt;dbl\u0026gt; 3, 2, 4, 2, 6, 9, 7, 2, 4, 6, 5, 9, 4, 5, 6, 7\r## $ post.2 \u0026lt;dbl\u0026gt; 2, 2, 5, 2, 7, 9, 7, 4, 5, 7, 4, 10, 6, 6, 6, 7\r## $ post.3 \u0026lt;dbl\u0026gt; 5, 3, 7, 3, 8, 10, 8, 8, 6, 6, 7, 11, 6, 7, 7, 8\r## $ post.4 \u0026lt;dbl\u0026gt; 3, 5, 5, 5, 6, 8, 10, 6, 4, 8, 5, 9, 8, 5, 9, 6\r## $ post.5 \u0026lt;dbl\u0026gt; 2, 3, 4, 3, 3, 9, 8, 5, 1, 8, 4, 6, 6, 2, 7, 7\r## $ fup.1 \u0026lt;dbl\u0026gt; 2, 4, 7, 4, 4, 9, 8, 6, 5, 8, 5, 8, 7, 6, 7, 7\r## $ fup.2 \u0026lt;dbl\u0026gt; 3, 5, 6, 4, 3, 10, 9, 6, 4, 8, 6, 7, 7, 7, 7, 8\r## $ fup.3 \u0026lt;dbl\u0026gt; 2, 6, 9, 5, 6, 11, 11, 7, 7, 9, 8, 10, 8, 8, 8, 10\r## $ fup.4 \u0026lt;dbl\u0026gt; 4, 4, 7, 3, 4, 9, 9, 5, 5, 7, 6, 8, 10, 6, 6, 8\r## $ fup.5 \u0026lt;dbl\u0026gt; 4, 1, 6, 4, 3, 6, 8, 6, 4, 8, 5, 7, 8, 3, 7, 7\rWhen working with data from an add-on package it is often also a good idea to work on a copy of it rather than overwriting/modifying it under the original name. Using a copy for experimental tranformations is recommended in general in case the transformation doesn’t go quite as planned and you want to change it or recover the original data. To recover data provided in a package if you have accidentally modified it you’d have to remove the object from the global environment (deleting your custom copy of it but not the original version in the package) using the rm() function… which can be a bit confusing. Best to just work on a copy with a clearly different name.\nThe same recommendation applies to naming things in general. Try to use names that aren’t already assigned to other objects or functions. If you’re not sure if a name is in use or not, try to print it (just the name, enclosed in print() if you want) or look-up documentation for it with ?object or help(object).\n#we\u0026#39;ll save a copy of the gapminder data to gap_df (short for gapminder_dataframe)\rgap_df \u0026lt;- gapminder\rob_df \u0026lt;- OBrienKaiser\r#now we can do whatever we want to gap_df or ob_df without affecting gapminder or OBrienKaiser\r#For the next section it helps to focus on just the names of the variables in each of these data frames\rnames(gap_df)\r## [1] \u0026quot;country\u0026quot; \u0026quot;continent\u0026quot; \u0026quot;year\u0026quot; \u0026quot;lifeExp\u0026quot; \u0026quot;pop\u0026quot; \u0026quot;gdpPercap\u0026quot;\rnames(ob_df)\r## [1] \u0026quot;treatment\u0026quot; \u0026quot;gender\u0026quot; \u0026quot;pre.1\u0026quot; \u0026quot;pre.2\u0026quot; \u0026quot;pre.3\u0026quot; \u0026quot;pre.4\u0026quot; ## [7] \u0026quot;pre.5\u0026quot; \u0026quot;post.1\u0026quot; \u0026quot;post.2\u0026quot; \u0026quot;post.3\u0026quot; \u0026quot;post.4\u0026quot; \u0026quot;post.5\u0026quot; ## [13] \u0026quot;fup.1\u0026quot; \u0026quot;fup.2\u0026quot; \u0026quot;fup.3\u0026quot; \u0026quot;fup.4\u0026quot; \u0026quot;fup.5\u0026quot;\rIf you haven’t see either of these datasets before I recommend reviewing the documentation for them, using:\n?gapminder #and\r?OBrienKaiser\rBefore reading on, try to answer the question: What is the main difference between the variable naming styles used by the authors of these two datasets?\n2.1 “long” data, “wide” data, and “tidy” data\rFrom the glimpses we requested, we can see that the gapminder data contains 1,704 observations (rows) and 6 variables (columns), while the OBrienKaiser data has only 16 rows but 17 columns. Related to the question I posed above, note that gapminder has unique names for each variable and that most of the columns in OBrienKaiser have generic names with a string.number format. Why is this?\ngapminder is an example of “long” data because each row represents a unique observation and each column represents a unique variable. This is the form that data needs to be in to do most statistical analyses and generate visualizations in R. Because each unique variable is a single column, each column has a unique name.\nOBrienKaiser is an example of “wide” data because each row represents a unique case or individual and all measures for these individuals are spread across multiple columns. Because several of the variables are spread across multiple columns, these have more generic names.\nThis is the form that data typically needs to be in to do statstical analyses in other programs like SPSS, so you’ll encounter it often if you’re importing data directly from SPSS or working on Excel files produced by people who primarily work in SPSS or just Excel itself.\nThe term “tidy” data refers to the long form arrangement of data with the additional criterion that each type of observational unit is in a single table rather than being split across multiple tables. Data cleaning and “tidy-ness” are so germane to real world data analysis and data science that the tidyverse was named after them. If you want to conduct an analysis using data that are spread across multiple data frames or tables, my next blog post will show you how to combine them using a set of functions called joins.\nIt won’t always be so obvious, but now you know that even the names of a data frame’s columns can tell you quite a bit about its structure.\n\r\r3 pivot_longer()\rTo make data longer so that we have all values of the same measure in a single column we can use the aptly named pivot_longer(), which is a newer version of the tidyr gather() function. Arguments we need to specify are:\ndata (or we can pipe the data in using the pipe operator)\n\rcols: the distributed columns we want to combine\n\rnames_to: string name to use for the new character vector that keeps track\rof which columns the values originally came from, sometimes this is called the\r“key” column. Values of the “names_to” are by default the names of the\rcombined columns\n\rvalues_to: string name to use for the new column that will contain all of\rthe combined values\n\r\rFor example, this is how you could combine all of the distributed measurement columns from the ob_df data frame into a single tidy key-value column pair.\n#first we\u0026#39;ll add a column to keep track of the subject ID since the OBrienKaiser\r#data doesn\u0026#39;t already have one. you could do this either using dplyr\u0026#39;s mutate() function,\r#or using the shortcut function rowid_to_column, from the tidyverse package \u0026quot;tibble\u0026quot;\rob_df_v2 \u0026lt;- ob_df %\u0026gt;% rowid_to_column(\u0026quot;subject_id\u0026quot;) #subject ID column values take on the row ID values\r#by default this adds the column to the left side of the data frame (in the 1st position)\r#alternative using mutate\rob_df_v2b \u0026lt;- ob_df %\u0026gt;% mutate(subject_id = c(1:nrow(ob_df))) %\u0026gt;% #same end result\r# subject id becomes the numbers: [1 to the number of total rows]. # By default, this adds the column to the end, so we will rearrange it using\r# select() with tidyselect helper everything()\rselect(subject_id, everything()) #this selects the new subject_id column, then everything else\ridentical(ob_df_v2, ob_df_v2b) #check if the objects are identical\r## [1] TRUE\r#in this case both methods are equivalent so rowid_to_column() is preferred, but\r#if you want to assign something else as subject ids, then mutate() would be\r#better. Rearranging with select() is optional of course. I typically prefer to\r#work with subject ID in the 1st position though\r# next we make the data longer using pivot_longer()\rob_long \u0026lt;- pivot_longer(data = ob_df_v2, cols = pre.1:fup.5, names_to = \u0026quot;session\u0026quot;,\rvalues_to = \u0026quot;score\u0026quot;\r)\rob_long %\u0026gt;% glimpse \r## Observations: 240\r## Variables: 5\r## $ subject_id \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,...\r## $ treatment \u0026lt;fct\u0026gt; control, control, control, control, control, control, co...\r## $ gender \u0026lt;fct\u0026gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M,...\r## $ session \u0026lt;chr\u0026gt; \u0026quot;pre.1\u0026quot;, \u0026quot;pre.2\u0026quot;, \u0026quot;pre.3\u0026quot;, \u0026quot;pre.4\u0026quot;, \u0026quot;pre.5\u0026quot;, \u0026quot;post.1\u0026quot;, \u0026quot;...\r## $ score \u0026lt;dbl\u0026gt; 1, 2, 4, 2, 1, 3, 2, 5, 3, 2, 2, 3, 2, 4, 4, 4, 4, 5, 3,...\r#now we have a total of 256 observations for our 16 subjects/cases, score\r#measurements are all in a single column that we\u0026#39;ve called \u0026quot;score\u0026quot;, and the data\r#collection session is coded using the session variable (the values of which\r#used to be the names of the original columns).\rAs you can see, pooling a single measure is pretty easy, but what if we want to assign multiple sets of measures to different columns, like the pre, post, and fup components of the ob_df_v2 data frame (e.g. if these represented different types of measurements on distinct scales like height, weight, and temperature). This requires an extra step and adjusting our arguments a bit, but pivot_longer() makes this pretty easy as well.\nThe main requirements are:\nA consistent and unique string pattern that can be used to determine\rwhich columns of values belong together and what to use as the values of the key colunn\n\rusing the special “.value” string as part of the names_to argument and\rthe names_sep argument instead of using values_to\n\r\rIn the lucky event that your column names already have a consistent pattern in them, then you can just make use of that pattern and skip to step 2. Otherwise it may be a good idea to create one using either the dplyr::rename() function (if you only have to modify a few names), or using some string manipulation functions from the stringr tidyverse package. This section will serve as a preview of the sort of monotony that stringr and regular expressions can free you from, which will be expanded upon in a future post covering stringr.\nApplied to our ob_df_v2 dataset:\n# step 1. consistent pattern across the set of value columns ----\rnames(ob_df_v2)\r## [1] \u0026quot;subject_id\u0026quot; \u0026quot;treatment\u0026quot; \u0026quot;gender\u0026quot; \u0026quot;pre.1\u0026quot; \u0026quot;pre.2\u0026quot; ## [6] \u0026quot;pre.3\u0026quot; \u0026quot;pre.4\u0026quot; \u0026quot;pre.5\u0026quot; \u0026quot;post.1\u0026quot; \u0026quot;post.2\u0026quot; ## [11] \u0026quot;post.3\u0026quot; \u0026quot;post.4\u0026quot; \u0026quot;post.5\u0026quot; \u0026quot;fup.1\u0026quot; \u0026quot;fup.2\u0026quot; ## [16] \u0026quot;fup.3\u0026quot; \u0026quot;fup.4\u0026quot; \u0026quot;fup.5\u0026quot;\r# Looking at the names again, we have a mere period \u0026quot;.\u0026quot; as a consistent pattern.\r# while you could use this to flag the columns you want to pivot, something to\r# consider is that \u0026quot;.\u0026quot; is also a special character used in regular expressions\r# (a string pattern matching system) to represent \u0026quot;any character\u0026quot;, so we would\r# have to preface it with one or more of another special character combination,\r# in this case \u0026quot;\\\\\u0026quot;, to \u0026quot;escape it\u0026quot; for string pattern matching functions to\r# interpret it as a literal \u0026quot;.\u0026quot; instead of \u0026quot;any character\u0026quot;. This is usually done\r# as part of a regular expression made up of combinations of special characters.\r# Don\u0026#39;t worry if this strange business of escaping characters and regular\r# expressions seems confusing at this point, we\u0026#39;ll learn about it in much more\r# detail in a later post.\r# For now, you could subset the data and rename the columns, e.g.\rob_df_v2a \u0026lt;- ob_df_v2 %\u0026gt;%\rrename(pretest1 = pre.1, #rename(\u0026quot;new_name\u0026quot; = \u0026quot;old_name\u0026quot;)\rpretest2 = pre.2,\rpretest3 = pre.3,\rpretest4 = pre.4,\rpretest5 = pre.5,\rposttest1 = post.1, posttest2 = post.2,\rposttest3 = post.3,\rposttest4 = post.4,\rposttest5 = post.5,\rfuptest1 = fup.1,\rfuptest2 = fup.2,\rfuptest3 = fup.3,\rfuptest4 = fup.4,\rfuptest5 = fup.5)\rob_df_v2a\r## subject_id treatment gender pretest1 pretest2 pretest3 pretest4 pretest5\r## 1 1 control M 1 2 4 2 1\r## 2 2 control M 4 4 5 3 4\r## 3 3 control M 5 6 5 7 7\r## 4 4 control F 5 4 7 5 4\r## 5 5 control F 3 4 6 4 3\r## 6 6 A M 7 8 7 9 9\r## 7 7 A M 5 5 6 4 5\r## 8 8 A F 2 3 5 3 2\r## 9 9 A F 3 3 4 6 4\r## 10 10 B M 4 4 5 3 4\r## 11 11 B M 3 3 4 2 3\r## 12 12 B M 6 7 8 6 3\r## 13 13 B F 5 5 6 8 6\r## 14 14 B F 2 2 3 1 2\r## 15 15 B F 2 2 3 4 4\r## 16 16 B F 4 5 7 5 4\r## posttest1 posttest2 posttest3 posttest4 posttest5 fuptest1 fuptest2 fuptest3\r## 1 3 2 5 3 2 2 3 2\r## 2 2 2 3 5 3 4 5 6\r## 3 4 5 7 5 4 7 6 9\r## 4 2 2 3 5 3 4 4 5\r## 5 6 7 8 6 3 4 3 6\r## 6 9 9 10 8 9 9 10 11\r## 7 7 7 8 10 8 8 9 11\r## 8 2 4 8 6 5 6 6 7\r## 9 4 5 6 4 1 5 4 7\r## 10 6 7 6 8 8 8 8 9\r## 11 5 4 7 5 4 5 6 8\r## 12 9 10 11 9 6 8 7 10\r## 13 4 6 6 8 6 7 7 8\r## 14 5 6 7 5 2 6 7 8\r## 15 6 6 7 9 7 7 7 8\r## 16 7 7 8 6 7 7 8 10\r## fuptest4 fuptest5\r## 1 4 4\r## 2 4 1\r## 3 7 6\r## 4 3 4\r## 5 4 3\r## 6 9 6\r## 7 9 8\r## 8 5 6\r## 9 5 4\r## 10 7 8\r## 11 6 5\r## 12 8 7\r## 13 10 8\r## 14 6 3\r## 15 6 7\r## 16 8 7\r# However, I\u0026#39;m sure you\u0026#39;ll appreciate how tedious this could get for more than a\r# handful of columns. A better solution is to use a combination of the names()\r# and stringr::str_replace_all() functions\rob_df_v2b \u0026lt;- ob_df_v2 #create a copy (optional)\rnames(ob_df_v2b) \u0026lt;- str_replace_all( #use stringr::str_replace_all() to replace the names\rnames(ob_df_v2b), #character vector to replace matching values in\r\u0026quot;pre.\u0026quot;, #string component pattern to be matched\r#here the \u0026quot;.\u0026quot; doesn\u0026#39;t matter because I\u0026#39;ve also\r#specified the rest of the start of the\r#string up to the \u0026quot;.\u0026quot;\r\u0026quot;pretest\u0026quot; #string to replace it with\r) names(ob_df_v2b) \u0026lt;- str_replace_all(names(ob_df_v2b), \u0026quot;post.\u0026quot;, \u0026quot;posttest\u0026quot;)\rnames(ob_df_v2b) \u0026lt;- str_replace_all(names(ob_df_v2b), \u0026quot;fup.\u0026quot;, \u0026quot;fuptest\u0026quot;) identical(ob_df_v2a, ob_df_v2b)\r## [1] TRUE\r#a bit better... but still inefficient. The obvious third option is to just\r#escape the \u0026quot;.\u0026quot; using \u0026quot;\\\\.\u0026quot;:\rob_df_v2c \u0026lt;- ob_df_v2 names(ob_df_v2c) \u0026lt;- str_replace_all(names(ob_df_v2), \u0026quot;\\\\.\u0026quot;, #anything with a name that contains a period\r\u0026quot;test\u0026quot;)\ridentical(ob_df_v2a, ob_df_v2c)\r## [1] TRUE\r# Fortunately pivot_longer() incorporates the same string matching/detection\r# behaviour of str_replace_all(), so you can actually just pass \u0026quot;\\\\.\u0026quot; directly\r# to the cols and names_sep arguments, as in the 2nd and 3rd examples for step 2\r# below...\r# step 2. use a string patten to pivot columns ----\r#with one of the renamed versions of ob_df_v2\rob_long2 \u0026lt;- ob_df_v2c %\u0026gt;%\rpivot_longer(cols = contains(\u0026quot;test\u0026quot;), #select columns with names containing a pattern\rnames_to = c(\u0026quot;.value\u0026quot;, #special string that tells pivot_longer to use names\r#on each side of the separator pattern\r\u0026quot;session\u0026quot;), #name of the key column to use for the names\rnames_sep = \u0026quot;test\u0026quot; #separator to use\r)\rob_long2\r## # A tibble: 80 x 7\r## subject_id treatment gender session pre post fup\r## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 control M 1 1 3 2\r## 2 1 control M 2 2 2 3\r## 3 1 control M 3 4 5 2\r## 4 1 control M 4 2 3 4\r## 5 1 control M 5 1 2 4\r## 6 2 control M 1 4 2 4\r## 7 2 control M 2 4 2 5\r## 8 2 control M 3 5 3 6\r## 9 2 control M 4 3 5 4\r## 10 2 control M 5 4 3 1\r## # ... with 70 more rows\r#directly with a regular expression using the matches() tidyselect helper function\rob_long3 \u0026lt;- ob_df_v2 %\u0026gt;%\rpivot_longer(cols = matches(\u0026quot;\\\\.\u0026quot;), #select columns with names matching a regular expression pattern\rnames_to = c(\u0026quot;.value\u0026quot;, \u0026quot;session\u0026quot;), #special \u0026quot;.value\u0026quot; indictor and name of the key column to use for the names\rnames_sep = \u0026quot;\\\\.\u0026quot;) #separator to use\rob_long3\r## # A tibble: 80 x 7\r## subject_id treatment gender session pre post fup\r## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 control M 1 1 3 2\r## 2 1 control M 2 2 2 3\r## 3 1 control M 3 4 5 2\r## 4 1 control M 4 2 3 4\r## 5 1 control M 5 1 2 4\r## 6 2 control M 1 4 2 4\r## 7 2 control M 2 4 2 5\r## 8 2 control M 3 5 3 6\r## 9 2 control M 4 3 5 4\r## 10 2 control M 5 4 3 1\r## # ... with 70 more rows\ridentical(ob_long2, ob_long3)\r## [1] TRUE\r#directly using the even easier contains() tidyselect helper function that\r#absolves you of the need to worry about special characters/regular expressions\rob_long4 \u0026lt;- ob_df_v2 %\u0026gt;%\rpivot_longer(cols = contains(\u0026quot;.\u0026quot;), #select columns with names containing a literal string pattern\rnames_to = c(\u0026quot;.value\u0026quot;, \u0026quot;session\u0026quot;), #special \u0026quot;.value\u0026quot; indictor and name of the key column to use for the names\rnames_sep = \u0026quot;\\\\.\u0026quot; #separator to use. #names_sep unfortunately requires the escaping of special\r#characters if you want them matched literally. Unfortunately\r#contains() doesnt work in the names_sep argument.\r)\ridentical(ob_long3, ob_long4)\r## [1] TRUE\rNow you know something about making data longer as well as few helpful bits about using string patterns to change variable names.\nThe preceeding use case represented a situation when you have multiple value columns that you want to aggregate separately, yielding one value column for each separate set. This could occur if data on multiple measures (e.g. height \u0026amp; weight) were collected from the same subjects on multiple occassions. An alternative situation you might encounter (albeit probably less often) occurs when the values of a single measure of interest are spread across numerous columns with complex names consisting of components that specify the measurement group in combination. The simplest example of this might be something like a column naming format for daily temperature values of “d2019-01-23”, “d2019-02-01”, etc., being used to represent dates and you are interested in splitting the key column into its year, month, and day components (yyyy-mm-dd) and putting the values all in a single “temperature” column.\nDoing this with pivot_longer() requires making use of the names_pattern argument (instead of names_sep) with a more complex regular expression that determines where to break up all of the name component. This is accomplished by enclosing each component with parentheses, e.g. names_pattern = \"(component_1)(component_2)(component_3)\"\nTo demonstrate this one we’ll use the WHO tuberculosis report data from the tidyr package to replicate the example of this use case from the pivot_longer() documentation, with minor modifications to simplify it a bit. This dataset is automatically imported when the tidyr package is loaded so you can use it by calling the name “who”\n#?who #uncomment and run this to check the documentation for the who dataset\rnames(who) #check the names of the columns\r## [1] \u0026quot;country\u0026quot; \u0026quot;iso2\u0026quot; \u0026quot;iso3\u0026quot; \u0026quot;year\u0026quot; \u0026quot;new_sp_m014\u0026quot; ## [6] \u0026quot;new_sp_m1524\u0026quot; \u0026quot;new_sp_m2534\u0026quot; \u0026quot;new_sp_m3544\u0026quot; \u0026quot;new_sp_m4554\u0026quot; \u0026quot;new_sp_m5564\u0026quot;\r## [11] \u0026quot;new_sp_m65\u0026quot; \u0026quot;new_sp_f014\u0026quot; \u0026quot;new_sp_f1524\u0026quot; \u0026quot;new_sp_f2534\u0026quot; \u0026quot;new_sp_f3544\u0026quot;\r## [16] \u0026quot;new_sp_f4554\u0026quot; \u0026quot;new_sp_f5564\u0026quot; \u0026quot;new_sp_f65\u0026quot; \u0026quot;new_sn_m014\u0026quot; \u0026quot;new_sn_m1524\u0026quot;\r## [21] \u0026quot;new_sn_m2534\u0026quot; \u0026quot;new_sn_m3544\u0026quot; \u0026quot;new_sn_m4554\u0026quot; \u0026quot;new_sn_m5564\u0026quot; \u0026quot;new_sn_m65\u0026quot; ## [26] \u0026quot;new_sn_f014\u0026quot; \u0026quot;new_sn_f1524\u0026quot; \u0026quot;new_sn_f2534\u0026quot; \u0026quot;new_sn_f3544\u0026quot; \u0026quot;new_sn_f4554\u0026quot;\r## [31] \u0026quot;new_sn_f5564\u0026quot; \u0026quot;new_sn_f65\u0026quot; \u0026quot;new_ep_m014\u0026quot; \u0026quot;new_ep_m1524\u0026quot; \u0026quot;new_ep_m2534\u0026quot;\r## [36] \u0026quot;new_ep_m3544\u0026quot; \u0026quot;new_ep_m4554\u0026quot; \u0026quot;new_ep_m5564\u0026quot; \u0026quot;new_ep_m65\u0026quot; \u0026quot;new_ep_f014\u0026quot; ## [41] \u0026quot;new_ep_f1524\u0026quot; \u0026quot;new_ep_f2534\u0026quot; \u0026quot;new_ep_f3544\u0026quot; \u0026quot;new_ep_f4554\u0026quot; \u0026quot;new_ep_f5564\u0026quot;\r## [46] \u0026quot;new_ep_f65\u0026quot; \u0026quot;newrel_m014\u0026quot; \u0026quot;newrel_m1524\u0026quot; \u0026quot;newrel_m2534\u0026quot; \u0026quot;newrel_m3544\u0026quot;\r## [51] \u0026quot;newrel_m4554\u0026quot; \u0026quot;newrel_m5564\u0026quot; \u0026quot;newrel_m65\u0026quot; \u0026quot;newrel_f014\u0026quot; \u0026quot;newrel_f1524\u0026quot;\r## [56] \u0026quot;newrel_f2534\u0026quot; \u0026quot;newrel_f3544\u0026quot; \u0026quot;newrel_f4554\u0026quot; \u0026quot;newrel_f5564\u0026quot; \u0026quot;newrel_f65\u0026quot;\rNote from the documentation that many of the columns have a name with 3 components: 2 separated by an underscore and an “m” or “f” male/female indicator. These components represent three different variables that collectively specify the type of case and the values of these columns are all counts of tuberculosis cases. According to the documentation for the who data, the newrel* columns are part of this scheme as well, but they only have a single underscore.\n# we can standardize the newrel* column names to match the structure of the\r# other new* columns using str_replace_all() to simplify subsequent pattern\r# matching in pivot_longer()\rwho2 \u0026lt;- who\rnames(who2) \u0026lt;- str_replace_all(names(who2), \u0026quot;newrel\u0026quot;, \u0026quot;new_rel\u0026quot;)\rwho_long \u0026lt;- who2 %\u0026gt;% pivot_longer(\rcols = new_sp_m014:new_rel_f65,\rnames_to = c(\u0026quot;diagnosis\u0026quot;, \u0026quot;gender\u0026quot;, \u0026quot;age\u0026quot;),\rnames_pattern = \u0026quot;new_(.*)_(.)(.*)\u0026quot;, # pattern to be matched in this case is # \u0026quot;new_(component 1)_(component2[m/f])_(component3)\u0026quot;, # where component 1 is everything between the 1st and 2nd underscore,\r# component 2 is the lowercase letter \u0026quot;m\u0026quot; or \u0026quot;f\u0026quot;, and component 3 is\r# everything after component 2.\r# # note: \u0026quot;*\u0026quot; is used to represent the repetition of the preceeding chraracter\r# (in this case \u0026quot;.\u0026quot; for \u0026quot;any character\u0026quot;) any number of times in a regular\r# expresion\rvalues_to = \u0026quot;count\u0026quot;)\rwho_long %\u0026gt;% arrange(desc(count)) # arrange by count in descending order\r## # A tibble: 405,440 x 8\r## country iso2 iso3 year diagnosis gender age count\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 India IN IND 2007 sn m 3544 250051\r## 2 India IN IND 2007 sn f 3544 148811\r## 3 China CN CHN 2013 rel m 65 124476\r## 4 China CN CHN 2013 rel m 5564 112558\r## 5 India IN IND 2007 ep m 3544 105825\r## 6 India IN IND 2007 ep f 3544 101015\r## 7 China CN CHN 2013 rel m 4554 100297\r## 8 India IN IND 2009 sp m 3544 90830\r## 9 India IN IND 2008 sp m 3544 90498\r## 10 India IN IND 2010 sp m 3544 90440\r## # ... with 405,430 more rows\r# now all of the values (i.e. counts) are in a single column and our distinct\r# grouping variables are specified using a tidy set of 3 simple key columns\r# instead of one complicated one\r# see the documentation for the who data if you\u0026#39;re wondering why some values of\r# age are so high\r\r4 pivot_wider()\rpivot_wider() makes a data frame wider by spliting one or more value columns into a larger number of value columns, with names assigned according to the levels of key columns specified using the names_from argument. Here we’ll use it to make the gapminder data wider by putting the annual life expectancy column, “lifeExp” into separate columns for each year (named using the values of the key column “year”).\nArguments to specify for the simple case of spreading a single key-value column pair are:\ndata\n\rid_cols: the columns that together uniquely identify each observation. by\rdefault this is all columns\n\rnames_from: the key column(s) to pull the names from for the new set of\rreplacement columns\n\rvalues_from: the value column(s) to pull values from for the new set of\rreplacement columns\n\r\rYou might want to do this as a temporary transformation to facilitate the construction of new variables derived from a combination of the more widely distributed value columns that result from the application of pivot_wider() with mutate(), or if you plan to export the data in wide from to share with someone working in other statistical software programs which require wide data.\ngap_df %\u0026gt;% pivot_wider(id_cols = c(country, continent), names_from = year, values_from = lifeExp)\r## # A tibble: 142 x 14\r## country continent `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987`\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Afghan~ Asia 28.8 30.3 32.0 34.0 36.1 38.4 39.9 40.8\r## 2 Albania Europe 55.2 59.3 64.8 66.2 67.7 68.9 70.4 72 ## 3 Algeria Africa 43.1 45.7 48.3 51.4 54.5 58.0 61.4 65.8\r## 4 Angola Africa 30.0 32.0 34 36.0 37.9 39.5 39.9 39.9\r## 5 Argent~ Americas 62.5 64.4 65.1 65.6 67.1 68.5 69.9 70.8\r## 6 Austra~ Oceania 69.1 70.3 70.9 71.1 71.9 73.5 74.7 76.3\r## 7 Austria Europe 66.8 67.5 69.5 70.1 70.6 72.2 73.2 74.9\r## 8 Bahrain Asia 50.9 53.8 56.9 59.9 63.3 65.6 69.1 70.8\r## 9 Bangla~ Asia 37.5 39.3 41.2 43.5 45.3 46.9 50.0 52.8\r## 10 Belgium Europe 68 69.2 70.2 70.9 71.4 72.8 73.9 75.4\r## # ... with 132 more rows, and 4 more variables: `1992` \u0026lt;dbl\u0026gt;, `1997` \u0026lt;dbl\u0026gt;,\r## # `2002` \u0026lt;dbl\u0026gt;, `2007` \u0026lt;dbl\u0026gt;\r#now all the lifeExp values for each year are in separate columns\rWe can also reverse the first ob_df_v2 lengthening transformation we did by using the names_sep argument again. However, you should be aware that names_sep behaves differently for pivot_wider() than it did for pivot_longer(). Specifically, since we’re adding “.” to the names in pivot_wider() instead of searching for it (as we were in pivot_longer()), the names_sep argument text is read literally for pivot_wider() rather than read like a regular expression for pattern matching like it was with pivor_longer(). In most cases you can use the general rules:\nadding/inserting text = “literal text” where\ryou don’t need to worry about special characters\npattern searching/matching = “regular\rexpresison” where you do need to worry about special characters\nSome functions behave differently, but we’ll reserve that discussion for another post.\nob_long %\u0026gt;% glimpse\r## Observations: 240\r## Variables: 5\r## $ subject_id \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,...\r## $ treatment \u0026lt;fct\u0026gt; control, control, control, control, control, control, co...\r## $ gender \u0026lt;fct\u0026gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M,...\r## $ session \u0026lt;chr\u0026gt; \u0026quot;pre.1\u0026quot;, \u0026quot;pre.2\u0026quot;, \u0026quot;pre.3\u0026quot;, \u0026quot;pre.4\u0026quot;, \u0026quot;pre.5\u0026quot;, \u0026quot;post.1\u0026quot;, \u0026quot;...\r## $ score \u0026lt;dbl\u0026gt; 1, 2, 4, 2, 1, 3, 2, 5, 3, 2, 2, 3, 2, 4, 4, 4, 4, 5, 3,...\rob_wide_again \u0026lt;- pivot_wider(data = ob_long, names_from = \u0026quot;session\u0026quot;,\rvalues_from = \u0026quot;score\u0026quot;,\rnames_sep = \u0026quot;.\u0026quot;)\rob_wide_again %\u0026gt;% glimpse\r## Observations: 16\r## Variables: 18\r## $ subject_id \u0026lt;int\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16\r## $ treatment \u0026lt;fct\u0026gt; control, control, control, control, control, A, A, A, A,...\r## $ gender \u0026lt;fct\u0026gt; M, M, M, F, F, M, M, F, F, M, M, M, F, F, F, F\r## $ pre.1 \u0026lt;dbl\u0026gt; 1, 4, 5, 5, 3, 7, 5, 2, 3, 4, 3, 6, 5, 2, 2, 4\r## $ pre.2 \u0026lt;dbl\u0026gt; 2, 4, 6, 4, 4, 8, 5, 3, 3, 4, 3, 7, 5, 2, 2, 5\r## $ pre.3 \u0026lt;dbl\u0026gt; 4, 5, 5, 7, 6, 7, 6, 5, 4, 5, 4, 8, 6, 3, 3, 7\r## $ pre.4 \u0026lt;dbl\u0026gt; 2, 3, 7, 5, 4, 9, 4, 3, 6, 3, 2, 6, 8, 1, 4, 5\r## $ pre.5 \u0026lt;dbl\u0026gt; 1, 4, 7, 4, 3, 9, 5, 2, 4, 4, 3, 3, 6, 2, 4, 4\r## $ post.1 \u0026lt;dbl\u0026gt; 3, 2, 4, 2, 6, 9, 7, 2, 4, 6, 5, 9, 4, 5, 6, 7\r## $ post.2 \u0026lt;dbl\u0026gt; 2, 2, 5, 2, 7, 9, 7, 4, 5, 7, 4, 10, 6, 6, 6, 7\r## $ post.3 \u0026lt;dbl\u0026gt; 5, 3, 7, 3, 8, 10, 8, 8, 6, 6, 7, 11, 6, 7, 7, 8\r## $ post.4 \u0026lt;dbl\u0026gt; 3, 5, 5, 5, 6, 8, 10, 6, 4, 8, 5, 9, 8, 5, 9, 6\r## $ post.5 \u0026lt;dbl\u0026gt; 2, 3, 4, 3, 3, 9, 8, 5, 1, 8, 4, 6, 6, 2, 7, 7\r## $ fup.1 \u0026lt;dbl\u0026gt; 2, 4, 7, 4, 4, 9, 8, 6, 5, 8, 5, 8, 7, 6, 7, 7\r## $ fup.2 \u0026lt;dbl\u0026gt; 3, 5, 6, 4, 3, 10, 9, 6, 4, 8, 6, 7, 7, 7, 7, 8\r## $ fup.3 \u0026lt;dbl\u0026gt; 2, 6, 9, 5, 6, 11, 11, 7, 7, 9, 8, 10, 8, 8, 8, 10\r## $ fup.4 \u0026lt;dbl\u0026gt; 4, 4, 7, 3, 4, 9, 9, 5, 5, 7, 6, 8, 10, 6, 6, 8\r## $ fup.5 \u0026lt;dbl\u0026gt; 4, 1, 6, 4, 3, 6, 8, 6, 4, 8, 5, 7, 8, 3, 7, 7\rall.equal(ob_df_v2, ob_wide_again, check.attributes = F) \r## [1] TRUE\r# In case you\u0026#39;re wondering why I didn\u0026#39;t use identical()... all.equal with\r# check.atributes = F checks equality of the contents but not attributes of the\r# objects. In this case ob_df_v2 is not a tibble (it\u0026#39;s just a data frame) but\r# ob_wide_again is a tibble and a data frame, so the \u0026quot;class\u0026quot; attributes of the\r# objects are different, but the contents are the same.\r…or the 2nd transformation with multiple distinct value columns… just specify them all as a character vector in the values_from argument\nob_long2 %\u0026gt;% glimpse\r## Observations: 80\r## Variables: 7\r## $ subject_id \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4,...\r## $ treatment \u0026lt;fct\u0026gt; control, control, control, control, control, control, co...\r## $ gender \u0026lt;fct\u0026gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, F, F, F, F,...\r## $ session \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;...\r## $ pre \u0026lt;dbl\u0026gt; 1, 2, 4, 2, 1, 4, 4, 5, 3, 4, 5, 6, 5, 7, 7, 5, 4, 7, 5,...\r## $ post \u0026lt;dbl\u0026gt; 3, 2, 5, 3, 2, 2, 2, 3, 5, 3, 4, 5, 7, 5, 4, 2, 2, 3, 5,...\r## $ fup \u0026lt;dbl\u0026gt; 2, 3, 2, 4, 4, 4, 5, 6, 4, 1, 7, 6, 9, 7, 6, 4, 4, 5, 3,...\rob_wide_again2 \u0026lt;- ob_long2 %\u0026gt;% pivot_wider(names_from = \u0026quot;session\u0026quot;,\rvalues_from = c(\u0026quot;pre\u0026quot;, \u0026quot;post\u0026quot;, \u0026quot;fup\u0026quot;),\rnames_sep = \u0026quot;.\u0026quot;)\rall.equal(ob_df_v2, ob_wide_again2, check.attributes = F)\r## [1] TRUE\r…or the who2 dataset lengthening transformation by passing a vector with the key column names to the names_from argument and the original starting “new_” portion to the names_prefix argument\nwho_long %\u0026gt;% glimpse\r## Observations: 405,440\r## Variables: 8\r## $ country \u0026lt;chr\u0026gt; \u0026quot;Afghanistan\u0026quot;, \u0026quot;Afghanistan\u0026quot;, \u0026quot;Afghanistan\u0026quot;, \u0026quot;Afghanistan...\r## $ iso2 \u0026lt;chr\u0026gt; \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF...\r## $ iso3 \u0026lt;chr\u0026gt; \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;...\r## $ year \u0026lt;int\u0026gt; 1980, 1980, 1980, 1980, 1980, 1980, 1980, 1980, 1980, 198...\r## $ diagnosis \u0026lt;chr\u0026gt; \u0026quot;sp\u0026quot;, \u0026quot;sp\u0026quot;, \u0026quot;sp\u0026quot;, \u0026quot;sp\u0026quot;, \u0026quot;sp\u0026quot;, \u0026quot;sp\u0026quot;, \u0026quot;sp\u0026quot;, \u0026quot;sp\u0026quot;, \u0026quot;sp\u0026quot;, \u0026quot;sp...\r## $ gender \u0026lt;chr\u0026gt; \u0026quot;m\u0026quot;, \u0026quot;m\u0026quot;, \u0026quot;m\u0026quot;, \u0026quot;m\u0026quot;, \u0026quot;m\u0026quot;, \u0026quot;m\u0026quot;, \u0026quot;m\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;f...\r## $ age \u0026lt;chr\u0026gt; \u0026quot;014\u0026quot;, \u0026quot;1524\u0026quot;, \u0026quot;2534\u0026quot;, \u0026quot;3544\u0026quot;, \u0026quot;4554\u0026quot;, \u0026quot;5564\u0026quot;, \u0026quot;65\u0026quot;, \u0026quot;014...\r## $ count \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...\rwho2_wide_again \u0026lt;- who_long %\u0026gt;% pivot_wider(names_from = c(\u0026quot;diagnosis\u0026quot;, \u0026quot;gender\u0026quot;, \u0026quot;age\u0026quot;),\rvalues_from = c(\u0026quot;count\u0026quot;),\rnames_prefix = \u0026quot;new_\u0026quot;,\rnames_sep = c(\u0026quot;_\u0026quot;)\r)\r#remove the extra underscore that was not present between the 2nd and 3rd\r#components of the original names\rnames(who2_wide_again) \u0026lt;- str_replace_all(names(who2_wide_again), \u0026quot;m_\u0026quot;, \u0026quot;m\u0026quot;)\rnames(who2_wide_again) \u0026lt;- str_replace_all(names(who2_wide_again), \u0026quot;f_\u0026quot;, \u0026quot;f\u0026quot;)\rall.equal(who2, who2_wide_again, check.attributes = F)\r## [1] TRUE\rNext we move on to unite() and separate() which are specialized tidyr functions for selectively reshaping sets of columns without altering the rest of the manipulated data frame.\n\r5 unite()\runite() allows you to merge 2 or more columns into a single column without changing the length of the data frame. You typically specify the following arguments:\ndata\n\rcol: the name of the new column containing the values of the merged columns\n\r… : a selection of columns to unite, or if unspecified, all columns will\rbe united. Follows similar rules to the … argument of the select() function.\n\rsep: string to use to demarcate the component values of the united column\n\rremove: set this to FALSE if you want to retain the original columns, leave\rit as the default value of TRUE if you want to drop them.\n\r\rFor example, we could combine the country and continent columns of the gapminder data frame copy gap_df:\n#drop the original columns\runited_gap \u0026lt;- gap_df %\u0026gt;% unite(col = \u0026quot;country_continent\u0026quot;,\rcountry, continent,\rsep = \u0026quot;;\u0026quot;) #the default separator is \u0026quot;_\u0026quot;\runited_gap\r## # A tibble: 1,704 x 5\r## country_continent year lifeExp pop gdpPercap\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Afghanistan;Asia 1952 28.8 8425333 779.\r## 2 Afghanistan;Asia 1957 30.3 9240934 821.\r## 3 Afghanistan;Asia 1962 32.0 10267083 853.\r## 4 Afghanistan;Asia 1967 34.0 11537966 836.\r## 5 Afghanistan;Asia 1972 36.1 13079460 740.\r## 6 Afghanistan;Asia 1977 38.4 14880372 786.\r## 7 Afghanistan;Asia 1982 39.9 12881816 978.\r## 8 Afghanistan;Asia 1987 40.8 13867957 852.\r## 9 Afghanistan;Asia 1992 41.7 16317921 649.\r## 10 Afghanistan;Asia 1997 41.8 22227415 635.\r## # ... with 1,694 more rows\r#keep them\rgap_df %\u0026gt;% unite(col = \u0026quot;country_continent\u0026quot;,\rcountry, continent,\rsep = \u0026quot;;\u0026quot;,\rremove = FALSE) \r## # A tibble: 1,704 x 7\r## country_continent country continent year lifeExp pop gdpPercap\r## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Afghanistan;Asia Afghanistan Asia 1952 28.8 8425333 779.\r## 2 Afghanistan;Asia Afghanistan Asia 1957 30.3 9240934 821.\r## 3 Afghanistan;Asia Afghanistan Asia 1962 32.0 10267083 853.\r## 4 Afghanistan;Asia Afghanistan Asia 1967 34.0 11537966 836.\r## 5 Afghanistan;Asia Afghanistan Asia 1972 36.1 13079460 740.\r## 6 Afghanistan;Asia Afghanistan Asia 1977 38.4 14880372 786.\r## 7 Afghanistan;Asia Afghanistan Asia 1982 39.9 12881816 978.\r## 8 Afghanistan;Asia Afghanistan Asia 1987 40.8 13867957 852.\r## 9 Afghanistan;Asia Afghanistan Asia 1992 41.7 16317921 649.\r## 10 Afghanistan;Asia Afghanistan Asia 1997 41.8 22227415 635.\r## # ... with 1,694 more rows\r\r6 separate()\rseparate() does the opposite of unite(); it splits the values of a column and distributes them across multiple columns based on a separator string pattern. It also does not affect the length of the data frame. Arguments you’ll use most often are:\ndata\n\rcol: the name of the column you want to separate into pieces\n\rinto: a vector of names for the new columns. NA can be used to omit component columns\n\rsep: the string pattern to use to determine where to split the values.\rsep also accepts an integer for position-based splitting\n\rremove: drops the original complex column\n\rconvert: if TRUE, re-evaluates and assigns appropriate classes to the new\rcolumns based on the type of their contents after splitting has occurred.\n\r\rHere we’ll use separate() to reverse what we just did with unite()\ngap_df_separated \u0026lt;- united_gap %\u0026gt;% separate(col = \u0026quot;country_continent\u0026quot;,\rinto = c(\u0026quot;country\u0026quot;, \u0026quot;continent\u0026quot;),\rsep = \u0026quot;;\u0026quot;) %\u0026gt;% #one thing separate() won\u0026#39;t do is convert strings to factors for you (the\r#original variables were factors), so we\u0026#39;ll take of that using dplyr::mutate()\rmutate(country = as.factor(country), continent = as.factor(continent))\rgap_df_separated\r## # A tibble: 1,704 x 6\r## country continent year lifeExp pop gdpPercap\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Afghanistan Asia 1952 28.8 8425333 779.\r## 2 Afghanistan Asia 1957 30.3 9240934 821.\r## 3 Afghanistan Asia 1962 32.0 10267083 853.\r## 4 Afghanistan Asia 1967 34.0 11537966 836.\r## 5 Afghanistan Asia 1972 36.1 13079460 740.\r## 6 Afghanistan Asia 1977 38.4 14880372 786.\r## 7 Afghanistan Asia 1982 39.9 12881816 978.\r## 8 Afghanistan Asia 1987 40.8 13867957 852.\r## 9 Afghanistan Asia 1992 41.7 16317921 649.\r## 10 Afghanistan Asia 1997 41.8 22227415 635.\r## # ... with 1,694 more rows\ridentical(gap_df, gap_df_separated)\r## [1] TRUE\r#keep the complex column\runited_gap %\u0026gt;% separate(col = \u0026quot;country_continent\u0026quot;,\rinto = c(\u0026quot;country\u0026quot;, \u0026quot;continent\u0026quot;),\rsep = \u0026quot;;\u0026quot;,\rremove = FALSE) \r## # A tibble: 1,704 x 7\r## country_continent country continent year lifeExp pop gdpPercap\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Afghanistan;Asia Afghanistan Asia 1952 28.8 8425333 779.\r## 2 Afghanistan;Asia Afghanistan Asia 1957 30.3 9240934 821.\r## 3 Afghanistan;Asia Afghanistan Asia 1962 32.0 10267083 853.\r## 4 Afghanistan;Asia Afghanistan Asia 1967 34.0 11537966 836.\r## 5 Afghanistan;Asia Afghanistan Asia 1972 36.1 13079460 740.\r## 6 Afghanistan;Asia Afghanistan Asia 1977 38.4 14880372 786.\r## 7 Afghanistan;Asia Afghanistan Asia 1982 39.9 12881816 978.\r## 8 Afghanistan;Asia Afghanistan Asia 1987 40.8 13867957 852.\r## 9 Afghanistan;Asia Afghanistan Asia 1992 41.7 16317921 649.\r## 10 Afghanistan;Asia Afghanistan Asia 1997 41.8 22227415 635.\r## # ... with 1,694 more rows\r#drop one of the new columns using NA\runited_gap %\u0026gt;% separate(col = \u0026quot;country_continent\u0026quot;,\rinto = c(\u0026quot;country\u0026quot;, NA),\rsep = \u0026quot;;\u0026quot;) \r## # A tibble: 1,704 x 5\r## country year lifeExp pop gdpPercap\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Afghanistan 1952 28.8 8425333 779.\r## 2 Afghanistan 1957 30.3 9240934 821.\r## 3 Afghanistan 1962 32.0 10267083 853.\r## 4 Afghanistan 1967 34.0 11537966 836.\r## 5 Afghanistan 1972 36.1 13079460 740.\r## 6 Afghanistan 1977 38.4 14880372 786.\r## 7 Afghanistan 1982 39.9 12881816 978.\r## 8 Afghanistan 1987 40.8 13867957 852.\r## 9 Afghanistan 1992 41.7 16317921 649.\r## 10 Afghanistan 1997 41.8 22227415 635.\r## # ... with 1,694 more rows\rNow just practice these operations a few times with your own data and reshaping data to make it tidyr (pun intended) should be a breeze!\n\r7 Navigation\rClick here to go back to the previous post. You can read the next one on joining data with dplyr here.\n\r8 Notes\r\rYou can learn more about tidy data here and here.\n\rIf you want to get head start on learning about regular expressions and string manipulation, check this out.\n\r\rThank you for visiting my blog. I welcome any suggestions for future posts, comments or other feedback you might have. Feedback from beginners and science students/trainees (or with them in mind) is especially helpful in the interest of making this guide even better for them.\n\r","date":1579910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579910400,"objectID":"2a34df7adeeb18b4f44f64849ba1e938","permalink":"/post/2020-01-25-asgr-2-1-data-transformation-part-2/","publishdate":"2020-01-25T00:00:00Z","relpermalink":"/post/2020-01-25-asgr-2-1-data-transformation-part-2/","section":"post","summary":"1 TL;DR\r2 Introduction\r2.1 “long” data, “wide” data, and “tidy” data\r\r3 pivot_longer()\r4 pivot_wider()\r5 unite()\r6 separate()\r7 Navigation\r8 Notes\r\r\r1 TL;DR\rIn the 5th post of the Scientist’s Guide to R series we explore using the tidyr package to reshape data. You’ll learn all about splitting and combining columns and how to do wide to long or long to wide transformations.","tags":["R","R basics"],"title":"A Scientist's Guide to R: Step 2.1 Data Transformation - part 2","type":"post"},{"authors":["Lysenko-Martin, M.R.","Hutton, C.P.","Sparks, T.","Snowden, T.","Christie, B.R."],"categories":null,"content":"","date":1579219200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579219200,"objectID":"98946cfbfcb9a70c1745f1cea80c74cd","permalink":"/publication/lysenk-martin-hutton-et-al-2020/","publishdate":"2020-01-17T00:00:00Z","relpermalink":"/publication/lysenk-martin-hutton-et-al-2020/","section":"publication","summary":"The diagnosis of concussion remains challenging, particularly in cases where several months have passed between a head injury and clinical assessment. Tracking multiple moving objects in three-dimensional (3D) space engages many of the same cognitive processes that are affected by concussion, a form of mild traumatic brain injury (mTBI), suggesting that tests of 3D multiple object tracking (3D-MOT) may be sensitive to post-concussion syndrome after a brain injury has occurred. To test this, we evaluated 3D-MOT performance (using NeuroTrackerTM) against Sports Concussion Assessment Tool results for cognition, balance, and symptom severity in a large sample (N = 457) of male and female participants between the ages of 6 to 73. 3D-MOT performance in subjects under age 13 was not impaired by a history of concussion, but was positively associated with cognition and balance. 3D-MOT performance in those 13 and older was negatively associated with concussion symptom severity, and positively associated with cognition and balance. 3D-MOT was selectively impaired in subjects with probable post-concussion syndrome (pPCS), defined using the 95th percentile of symptom severity for subjects with no history of concussion. A decision tree predicted concussion status with 95.2% overall test accuracy (91.1% sensitivity, 97.8% specificity) using concussion history, age, and 3D-MOT score. Individuals with a history of concussion in the past 37 days were predicted to have pPCS if they were age 35 or older, or if they were under age 35 but achieved scores below 1.2 on the 3D-MOT. These results demonstrate the potential of 3D-MOT for pPCS diagnosis, and highlight the increased vulnerability to concussion symptoms that comes with age.","tags":["cognitive function","human studies","neuropsychology","traumatic brain injury","concussion","machine learning"],"title":"Multiple object tracking scores predict post-concussion status years after mild traumatic brain injury","type":"publication"},{"authors":null,"categories":["R","Reproducible Research","Data Cleaning and Transformation"],"content":"\r\r1 TL;DR\r2 Introduction\r3 select()\r3.1 Renaming Columns with select() or rename()\r\r4 filter()\r4.1 Subset Rows using Indices with slice()\r\r5 mutate()\r5.1 Recoding or Creating Indicator Variables using if_else(), case_when(), or recode()\r\r6 summarise()\r7 group_by()\r8 Chaining Functions with the pipe operator (%\u0026gt;%)\r9 Navigation\r10 Notes\r\r\r1 TL;DR\rThe 4th post in the Scientist’s Guide to R series introduces data transformation techniques useful for wrangling/tidying/cleaning data. Specifically, we will be learning how to use the 6 core functions (and a few others) from the popular dplyr package, perform similar operations in base R, and chain operations with the pipe operator (%\u0026gt;%) to streamline the process.\n\r2 Introduction\rThe 4th post in the Scientist’s Guide to R series introduces data transformation techniques useful for wrangling/tidying/cleaning data, which often takes longer than any other step in the data analysis process. Specifically, we will be learning how to use the 6 core functions or “verbs” of the popular dplyr package to:\nEasily select a subset of columns.\n\rfilter rows using logical tests with values of specified columns.\n\rModify columns or add new ones using mutate.\n\rObtain descriptive summaries of data using summarise (or the equivalent summarize()).\n\rAssign a grouping structure to a data frame to enable subsequent dplyr package function calls to be executed within groups using group_by.\n\rarrange a data frame for improved readability.\n\r\rSome additional relevant functions that build on this foundation will also be covered where appropriate.\nWhile other great packages (and other functions in base R) exist to help you manipulate data, using the dplyr package is perhaps the easiest way to learn these essential data processing techniques. However, if you find yourself working with very large datasets (e.g. \u0026gt; 1,000,000 rows) you might want to check out the data.table package, which emphasizes efficiency and performance over readability and ease of use.\nMost dplyr functions accept a data frame or tibble as their first argument and return a data frame as their output. This is a key design feature that enables you to apply a series of operations to a data frame in a chain using the pipe operator (%\u0026gt;%), as demonstrated in section 8.\n\r3 select()\rselect() is one of the dplyr functions you can use for subsetting. select() lets you extract a subset of columns from a data frame.\nColumns may be specified using indices, names, or using string patterns with the assistance of a few tidyselect helper functions such as contains(), starts_with(), and ends_with().\nlibrary(dplyr) #load the dplyr package with the library function.\r#the dplyr package will be used throughout this tutorial so make sure it is\r#loaded before trying to run the examples yourself\rdf \u0026lt;- starwars #assign the star wars data frame (imported when dplyr is loaded) to the label/name \u0026quot;df\u0026quot;\r# inspect the structure\rglimpse(df)\r## Observations: 87\r## Variables: 13\r## $ name \u0026lt;chr\u0026gt; \u0026quot;Luke Skywalker\u0026quot;, \u0026quot;C-3PO\u0026quot;, \u0026quot;R2-D2\u0026quot;, \u0026quot;Darth Vader\u0026quot;, \u0026quot;Leia...\r## $ height \u0026lt;int\u0026gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180...\r## $ mass \u0026lt;dbl\u0026gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, ...\r## $ hair_color \u0026lt;chr\u0026gt; \u0026quot;blond\u0026quot;, NA, NA, \u0026quot;none\u0026quot;, \u0026quot;brown\u0026quot;, \u0026quot;brown, grey\u0026quot;, \u0026quot;brown\u0026quot;...\r## $ skin_color \u0026lt;chr\u0026gt; \u0026quot;fair\u0026quot;, \u0026quot;gold\u0026quot;, \u0026quot;white, blue\u0026quot;, \u0026quot;white\u0026quot;, \u0026quot;light\u0026quot;, \u0026quot;light\u0026quot;...\r## $ eye_color \u0026lt;chr\u0026gt; \u0026quot;blue\u0026quot;, \u0026quot;yellow\u0026quot;, \u0026quot;red\u0026quot;, \u0026quot;yellow\u0026quot;, \u0026quot;brown\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;blu...\r## $ birth_year \u0026lt;dbl\u0026gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57....\r## $ gender \u0026lt;chr\u0026gt; \u0026quot;male\u0026quot;, NA, NA, \u0026quot;male\u0026quot;, \u0026quot;female\u0026quot;, \u0026quot;male\u0026quot;, \u0026quot;female\u0026quot;, NA, ...\r## $ homeworld \u0026lt;chr\u0026gt; \u0026quot;Tatooine\u0026quot;, \u0026quot;Tatooine\u0026quot;, \u0026quot;Naboo\u0026quot;, \u0026quot;Tatooine\u0026quot;, \u0026quot;Alderaan\u0026quot;,...\r## $ species \u0026lt;chr\u0026gt; \u0026quot;Human\u0026quot;, \u0026quot;Droid\u0026quot;, \u0026quot;Droid\u0026quot;, \u0026quot;Human\u0026quot;, \u0026quot;Human\u0026quot;, \u0026quot;Human\u0026quot;, \u0026quot;H...\r## $ films \u0026lt;list\u0026gt; [\u0026lt;\u0026quot;Revenge of the Sith\u0026quot;, \u0026quot;Return of the Jedi\u0026quot;, \u0026quot;The Emp...\r## $ vehicles \u0026lt;list\u0026gt; [\u0026lt;\u0026quot;Snowspeeder\u0026quot;, \u0026quot;Imperial Speeder Bike\u0026quot;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, ...\r## $ starships \u0026lt;list\u0026gt; [\u0026lt;\u0026quot;X-wing\u0026quot;, \u0026quot;Imperial shuttle\u0026quot;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026quot;TIE Advanced ...\r#in base R, you would extract columns using either [, \u0026quot;name\u0026quot;] or [[\u0026quot;name\u0026quot;]], or $name\rdf[, \u0026quot;name\u0026quot;] #[] returns another data frame\r## # A tibble: 87 x 1\r## name ## \u0026lt;chr\u0026gt; ## 1 Luke Skywalker ## 2 C-3PO ## 3 R2-D2 ## 4 Darth Vader ## 5 Leia Organa ## 6 Owen Lars ## 7 Beru Whitesun lars\r## 8 R5-D4 ## 9 Biggs Darklighter ## 10 Obi-Wan Kenobi ## # ... with 77 more rows\rhead(df[[\u0026quot;name\u0026quot;]]) #[[]] returns a 1 dimensional vector. head() just shows the 1st 6 values\r## [1] \u0026quot;Luke Skywalker\u0026quot; \u0026quot;C-3PO\u0026quot; \u0026quot;R2-D2\u0026quot; \u0026quot;Darth Vader\u0026quot; ## [5] \u0026quot;Leia Organa\u0026quot; \u0026quot;Owen Lars\u0026quot;\rdf[, c(\u0026quot;name\u0026quot;, \u0026quot;mass\u0026quot;, \u0026quot;hair_color\u0026quot;)] #[] can accept a vector of names\r## # A tibble: 87 x 3\r## name mass hair_color ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke Skywalker 77 blond ## 2 C-3PO 75 \u0026lt;NA\u0026gt; ## 3 R2-D2 32 \u0026lt;NA\u0026gt; ## 4 Darth Vader 136 none ## 5 Leia Organa 49 brown ## 6 Owen Lars 120 brown, grey ## 7 Beru Whitesun lars 75 brown ## 8 R5-D4 32 \u0026lt;NA\u0026gt; ## 9 Biggs Darklighter 84 black ## 10 Obi-Wan Kenobi 77 auburn, white\r## # ... with 77 more rows\r#df[[c(\u0026quot;name\u0026quot;, \u0026quot;mass\u0026quot;, \u0026quot;hair_color\u0026quot;)]] #[[]] can\u0026#39;t and returns an error\r#or a logical or integer vector the same length as the number of columns\rdf[, 1:ncol(df) %% 2 == 0] #columns with even indices\r## # A tibble: 87 x 6\r## height hair_color eye_color gender species vehicles ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; ## 1 172 blond blue male Human \u0026lt;chr [2]\u0026gt;\r## 2 167 \u0026lt;NA\u0026gt; yellow \u0026lt;NA\u0026gt; Droid \u0026lt;chr [0]\u0026gt;\r## 3 96 \u0026lt;NA\u0026gt; red \u0026lt;NA\u0026gt; Droid \u0026lt;chr [0]\u0026gt;\r## 4 202 none yellow male Human \u0026lt;chr [0]\u0026gt;\r## 5 150 brown brown female Human \u0026lt;chr [1]\u0026gt;\r## 6 178 brown, grey blue male Human \u0026lt;chr [0]\u0026gt;\r## 7 165 brown blue female Human \u0026lt;chr [0]\u0026gt;\r## 8 97 \u0026lt;NA\u0026gt; red \u0026lt;NA\u0026gt; Droid \u0026lt;chr [0]\u0026gt;\r## 9 183 black brown male Human \u0026lt;chr [0]\u0026gt;\r## 10 182 auburn, white blue-gray male Human \u0026lt;chr [1]\u0026gt;\r## # ... with 77 more rows\rdf[, 1:5] #1st 5 columns\r## # A tibble: 87 x 5\r## name height mass hair_color skin_color ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke Skywalker 172 77 blond fair ## 2 C-3PO 167 75 \u0026lt;NA\u0026gt; gold ## 3 R2-D2 96 32 \u0026lt;NA\u0026gt; white, blue\r## 4 Darth Vader 202 136 none white ## 5 Leia Organa 150 49 brown light ## 6 Owen Lars 178 120 brown, grey light ## 7 Beru Whitesun lars 165 75 brown light ## 8 R5-D4 97 32 \u0026lt;NA\u0026gt; white, red ## 9 Biggs Darklighter 183 84 black light ## 10 Obi-Wan Kenobi 182 77 auburn, white fair ## # ... with 77 more rows\r#dplyr::select() makes extracting multiple columns even easier\rselect(df, name, mass, hair_color) #note that names don\u0026#39;t have to be quoted\r## # A tibble: 87 x 3\r## name mass hair_color ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke Skywalker 77 blond ## 2 C-3PO 75 \u0026lt;NA\u0026gt; ## 3 R2-D2 32 \u0026lt;NA\u0026gt; ## 4 Darth Vader 136 none ## 5 Leia Organa 49 brown ## 6 Owen Lars 120 brown, grey ## 7 Beru Whitesun lars 75 brown ## 8 R5-D4 32 \u0026lt;NA\u0026gt; ## 9 Biggs Darklighter 84 black ## 10 Obi-Wan Kenobi 77 auburn, white\r## # ... with 77 more rows\rselect(df, name:hair_color) #and you can get all columns in a range using unquoted names\r## # A tibble: 87 x 4\r## name height mass hair_color ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke Skywalker 172 77 blond ## 2 C-3PO 167 75 \u0026lt;NA\u0026gt; ## 3 R2-D2 96 32 \u0026lt;NA\u0026gt; ## 4 Darth Vader 202 136 none ## 5 Leia Organa 150 49 brown ## 6 Owen Lars 178 120 brown, grey ## 7 Beru Whitesun lars 165 75 brown ## 8 R5-D4 97 32 \u0026lt;NA\u0026gt; ## 9 Biggs Darklighter 183 84 black ## 10 Obi-Wan Kenobi 182 77 auburn, white\r## # ... with 77 more rows\rselect(df, 1:5, 9, 10) #select also accepts column indices and doesn\u0026#39;t require c()\r## # A tibble: 87 x 7\r## name height mass hair_color skin_color homeworld species\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke Skywalker 172 77 blond fair Tatooine Human ## 2 C-3PO 167 75 \u0026lt;NA\u0026gt; gold Tatooine Droid ## 3 R2-D2 96 32 \u0026lt;NA\u0026gt; white, blue Naboo Droid ## 4 Darth Vader 202 136 none white Tatooine Human ## 5 Leia Organa 150 49 brown light Alderaan Human ## 6 Owen Lars 178 120 brown, grey light Tatooine Human ## 7 Beru Whitesun lars 165 75 brown light Tatooine Human ## 8 R5-D4 97 32 \u0026lt;NA\u0026gt; white, red Tatooine Droid ## 9 Biggs Darklighter 183 84 black light Tatooine Human ## 10 Obi-Wan Kenobi 182 77 auburn, white fair Stewjon Human ## # ... with 77 more rows\r#select everything other than a specified column using the subtraction operator \u0026quot;-\u0026quot;\rselect(df, -hair_color, -mass) \r## # A tibble: 87 x 11\r## name height skin_color eye_color birth_year gender homeworld species films\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lis\u0026gt;\r## 1 Luke~ 172 fair blue 19 male Tatooine Human \u0026lt;chr~\r## 2 C-3PO 167 gold yellow 112 \u0026lt;NA\u0026gt; Tatooine Droid \u0026lt;chr~\r## 3 R2-D2 96 white, bl~ red 33 \u0026lt;NA\u0026gt; Naboo Droid \u0026lt;chr~\r## 4 Dart~ 202 white yellow 41.9 male Tatooine Human \u0026lt;chr~\r## 5 Leia~ 150 light brown 19 female Alderaan Human \u0026lt;chr~\r## 6 Owen~ 178 light blue 52 male Tatooine Human \u0026lt;chr~\r## 7 Beru~ 165 light blue 47 female Tatooine Human \u0026lt;chr~\r## 8 R5-D4 97 white, red red NA \u0026lt;NA\u0026gt; Tatooine Droid \u0026lt;chr~\r## 9 Bigg~ 183 light brown 24 male Tatooine Human \u0026lt;chr~\r## 10 Obi-~ 182 fair blue-gray 57 male Stewjon Human \u0026lt;chr~\r## # ... with 77 more rows, and 2 more variables: vehicles \u0026lt;list\u0026gt;,\r## # starships \u0026lt;list\u0026gt;\rselect(df, -c(name:hair_color)) #everything other than a range of named columns\r## # A tibble: 87 x 9\r## skin_color eye_color birth_year gender homeworld species films vehicles\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lis\u0026gt; \u0026lt;list\u0026gt; ## 1 fair blue 19 male Tatooine Human \u0026lt;chr~ \u0026lt;chr [2~\r## 2 gold yellow 112 \u0026lt;NA\u0026gt; Tatooine Droid \u0026lt;chr~ \u0026lt;chr [0~\r## 3 white, bl~ red 33 \u0026lt;NA\u0026gt; Naboo Droid \u0026lt;chr~ \u0026lt;chr [0~\r## 4 white yellow 41.9 male Tatooine Human \u0026lt;chr~ \u0026lt;chr [0~\r## 5 light brown 19 female Alderaan Human \u0026lt;chr~ \u0026lt;chr [1~\r## 6 light blue 52 male Tatooine Human \u0026lt;chr~ \u0026lt;chr [0~\r## 7 light blue 47 female Tatooine Human \u0026lt;chr~ \u0026lt;chr [0~\r## 8 white, red red NA \u0026lt;NA\u0026gt; Tatooine Droid \u0026lt;chr~ \u0026lt;chr [0~\r## 9 light brown 24 male Tatooine Human \u0026lt;chr~ \u0026lt;chr [0~\r## 10 fair blue-gray 57 male Stewjon Human \u0026lt;chr~ \u0026lt;chr [1~\r## # ... with 77 more rows, and 1 more variable: starships \u0026lt;list\u0026gt;\r# dplyr also provides a number of \u0026quot;select helper\u0026quot; functions that allow you to\r# select variables using string patterns, e.g.\rselect(df, contains(\u0026quot;m\u0026quot;)) #all columns with names that contain the letter m\r## # A tibble: 87 x 4\r## name mass homeworld films ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; ## 1 Luke Skywalker 77 Tatooine \u0026lt;chr [5]\u0026gt;\r## 2 C-3PO 75 Tatooine \u0026lt;chr [6]\u0026gt;\r## 3 R2-D2 32 Naboo \u0026lt;chr [7]\u0026gt;\r## 4 Darth Vader 136 Tatooine \u0026lt;chr [4]\u0026gt;\r## 5 Leia Organa 49 Alderaan \u0026lt;chr [5]\u0026gt;\r## 6 Owen Lars 120 Tatooine \u0026lt;chr [3]\u0026gt;\r## 7 Beru Whitesun lars 75 Tatooine \u0026lt;chr [3]\u0026gt;\r## 8 R5-D4 32 Tatooine \u0026lt;chr [1]\u0026gt;\r## 9 Biggs Darklighter 84 Tatooine \u0026lt;chr [1]\u0026gt;\r## 10 Obi-Wan Kenobi 77 Stewjon \u0026lt;chr [6]\u0026gt;\r## # ... with 77 more rows\rselect(df, starts_with(\u0026quot;m\u0026quot;)) #all columns with names that start with the letter m \r## # A tibble: 87 x 1\r## mass\r## \u0026lt;dbl\u0026gt;\r## 1 77\r## 2 75\r## 3 32\r## 4 136\r## 5 49\r## 6 120\r## 7 75\r## 8 32\r## 9 84\r## 10 77\r## # ... with 77 more rows\rselect(df, ends_with(\u0026quot;s\u0026quot;)) #all columns with names that end with the letter s \r## # A tibble: 87 x 5\r## mass species films vehicles starships\r## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 77 Human \u0026lt;chr [5]\u0026gt; \u0026lt;chr [2]\u0026gt; \u0026lt;chr [2]\u0026gt;\r## 2 75 Droid \u0026lt;chr [6]\u0026gt; \u0026lt;chr [0]\u0026gt; \u0026lt;chr [0]\u0026gt;\r## 3 32 Droid \u0026lt;chr [7]\u0026gt; \u0026lt;chr [0]\u0026gt; \u0026lt;chr [0]\u0026gt;\r## 4 136 Human \u0026lt;chr [4]\u0026gt; \u0026lt;chr [0]\u0026gt; \u0026lt;chr [1]\u0026gt;\r## 5 49 Human \u0026lt;chr [5]\u0026gt; \u0026lt;chr [1]\u0026gt; \u0026lt;chr [0]\u0026gt;\r## 6 120 Human \u0026lt;chr [3]\u0026gt; \u0026lt;chr [0]\u0026gt; \u0026lt;chr [0]\u0026gt;\r## 7 75 Human \u0026lt;chr [3]\u0026gt; \u0026lt;chr [0]\u0026gt; \u0026lt;chr [0]\u0026gt;\r## 8 32 Droid \u0026lt;chr [1]\u0026gt; \u0026lt;chr [0]\u0026gt; \u0026lt;chr [0]\u0026gt;\r## 9 84 Human \u0026lt;chr [1]\u0026gt; \u0026lt;chr [0]\u0026gt; \u0026lt;chr [1]\u0026gt;\r## 10 77 Human \u0026lt;chr [6]\u0026gt; \u0026lt;chr [1]\u0026gt; \u0026lt;chr [5]\u0026gt;\r## # ... with 77 more rows\r# there is also a select helper called \u0026quot;matches\u0026quot; that enables you to select columns\r# with more complex string patterns using regular expressions # a detailed introduction to the \u0026quot;matches\u0026quot; select helper, string manipulation,\r# and regular expressions will be covered in a later post.\r# see the documentation for select_helpers (from the tidyselect package, which\r# is loaded with dplyr) for more info\r# ?tidyselect::select_helpers\r# you can rearrange columns and rename them during the selection process\rselect(df, 5:last_col(), 1:4) #select columns 5 to the end, then columns 1 to 4 \r## # A tibble: 87 x 13\r## skin_color eye_color birth_year gender homeworld species films vehicles\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lis\u0026gt; \u0026lt;list\u0026gt; ## 1 fair blue 19 male Tatooine Human \u0026lt;chr~ \u0026lt;chr [2~\r## 2 gold yellow 112 \u0026lt;NA\u0026gt; Tatooine Droid \u0026lt;chr~ \u0026lt;chr [0~\r## 3 white, bl~ red 33 \u0026lt;NA\u0026gt; Naboo Droid \u0026lt;chr~ \u0026lt;chr [0~\r## 4 white yellow 41.9 male Tatooine Human \u0026lt;chr~ \u0026lt;chr [0~\r## 5 light brown 19 female Alderaan Human \u0026lt;chr~ \u0026lt;chr [1~\r## 6 light blue 52 male Tatooine Human \u0026lt;chr~ \u0026lt;chr [0~\r## 7 light blue 47 female Tatooine Human \u0026lt;chr~ \u0026lt;chr [0~\r## 8 white, red red NA \u0026lt;NA\u0026gt; Tatooine Droid \u0026lt;chr~ \u0026lt;chr [0~\r## 9 light brown 24 male Tatooine Human \u0026lt;chr~ \u0026lt;chr [0~\r## 10 fair blue-gray 57 male Stewjon Human \u0026lt;chr~ \u0026lt;chr [1~\r## # ... with 77 more rows, and 5 more variables: starships \u0026lt;list\u0026gt;, name \u0026lt;chr\u0026gt;,\r## # height \u0026lt;int\u0026gt;, mass \u0026lt;dbl\u0026gt;, hair_color \u0026lt;chr\u0026gt;\r# to select() a single column and turn it into a vector (instead of leaving it as a data frame),\rall.equal(\rpull(select(df, mass), mass), # wrap it in the pull function (also from dplyr)\rdf$mass, # or use the $ symbol\rdf[[\u0026quot;mass\u0026quot;]] # or the equivalent `[[` operator\r)\r## [1] TRUE\r3.1 Renaming Columns with select() or rename()\rColumns can also be renamed when subsetting with the select() function or without subsetting using the rename() function (also in the dplyr package).\n#rename and subset using select\rselect(df, nm = name, hgt = height, byear = birth_year) #select columns and rename them\r## # A tibble: 87 x 3\r## nm hgt byear\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Luke Skywalker 172 19 ## 2 C-3PO 167 112 ## 3 R2-D2 96 33 ## 4 Darth Vader 202 41.9\r## 5 Leia Organa 150 19 ## 6 Owen Lars 178 52 ## 7 Beru Whitesun lars 165 47 ## 8 R5-D4 97 NA ## 9 Biggs Darklighter 183 24 ## 10 Obi-Wan Kenobi 182 57 ## # ... with 77 more rows\r#to just rename columns without subsetting, use dplyr::rename() instead of select()\rrename(df, nm = name, hgt = height, b_year = birth_year) #returns all columns\r## # A tibble: 87 x 13\r## nm hgt mass hair_color skin_color eye_color b_year gender homeworld\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke~ 172 77 blond fair blue 19 male Tatooine ## 2 C-3PO 167 75 \u0026lt;NA\u0026gt; gold yellow 112 \u0026lt;NA\u0026gt; Tatooine ## 3 R2-D2 96 32 \u0026lt;NA\u0026gt; white, bl~ red 33 \u0026lt;NA\u0026gt; Naboo ## 4 Dart~ 202 136 none white yellow 41.9 male Tatooine ## 5 Leia~ 150 49 brown light brown 19 female Alderaan ## 6 Owen~ 178 120 brown, gr~ light blue 52 male Tatooine ## 7 Beru~ 165 75 brown light blue 47 female Tatooine ## 8 R5-D4 97 32 \u0026lt;NA\u0026gt; white, red red NA \u0026lt;NA\u0026gt; Tatooine ## 9 Bigg~ 183 84 black light brown 24 male Tatooine ## 10 Obi-~ 182 77 auburn, w~ fair blue-gray 57 male Stewjon ## # ... with 77 more rows, and 4 more variables: species \u0026lt;chr\u0026gt;, films \u0026lt;list\u0026gt;,\r## # vehicles \u0026lt;list\u0026gt;, starships \u0026lt;list\u0026gt;\r#to save the updated names just assign the output back to the same data frame object\rrenamed_df \u0026lt;- rename(df, nm = name, hgt = height, b_year = birth_year) #In base R you can just assign names using a string vector. #The main disadvantage of this method is that all variable names have to be specified,\r#instead of just the ones that you want to change (which is all you need to do with the rename function)\r#Unlike select() or rename(), the base R method also requires that names be assigned in the correct order,\r#according to the column indices.\rdf_names \u0026lt;- names(df) #store the original names\rnames(df) \u0026lt;- c(\u0026quot;nm\u0026quot;, \u0026quot;hgt\u0026quot;, \u0026quot;mass\u0026quot;, \u0026quot;hair_color\u0026quot;, \u0026quot;skin_color\u0026quot;, \u0026quot;eye_color\u0026quot;,\r\u0026quot;b_year\u0026quot;) #returns a warning and then all non-specified names become NA\r## Warning: The `names` must have length 13, not 7.\r## This warning is displayed once per session.\rnames(df) \u0026lt;- df_names #restore the original names from the vector saved above\r\r\r4 filter()\rfilter() is the other core dplyr verb you can use for subsetting your data. filter() lets you extract a subset of rows using logical operators.\nTo filter a data frame, specify the name of the data frame as the 1st argument, then any number of logical tests for values of variables in the data frame that you want to use as filtering criteria.\n# we\u0026#39;ll just work with a few columns to simplify the output and highlight the filtering\rdf2 \u0026lt;- select(df, name, height, species, homeworld) unique(df2$homeworld) #check the unique values of the homeworld variable, which we will use for filtering\r## [1] \u0026quot;Tatooine\u0026quot; \u0026quot;Naboo\u0026quot; \u0026quot;Alderaan\u0026quot; \u0026quot;Stewjon\u0026quot; ## [5] \u0026quot;Eriadu\u0026quot; \u0026quot;Kashyyyk\u0026quot; \u0026quot;Corellia\u0026quot; \u0026quot;Rodia\u0026quot; ## [9] \u0026quot;Nal Hutta\u0026quot; \u0026quot;Bestine IV\u0026quot; NA \u0026quot;Kamino\u0026quot; ## [13] \u0026quot;Trandosha\u0026quot; \u0026quot;Socorro\u0026quot; \u0026quot;Bespin\u0026quot; \u0026quot;Mon Cala\u0026quot; ## [17] \u0026quot;Chandrila\u0026quot; \u0026quot;Endor\u0026quot; \u0026quot;Sullust\u0026quot; \u0026quot;Cato Neimoidia\u0026quot;\r## [21] \u0026quot;Coruscant\u0026quot; \u0026quot;Toydaria\u0026quot; \u0026quot;Malastare\u0026quot; \u0026quot;Dathomir\u0026quot; ## [25] \u0026quot;Ryloth\u0026quot; \u0026quot;Vulpter\u0026quot; \u0026quot;Troiken\u0026quot; \u0026quot;Tund\u0026quot; ## [29] \u0026quot;Haruun Kal\u0026quot; \u0026quot;Cerea\u0026quot; \u0026quot;Glee Anselm\u0026quot; \u0026quot;Iridonia\u0026quot; ## [33] \u0026quot;Iktotch\u0026quot; \u0026quot;Quermia\u0026quot; \u0026quot;Dorin\u0026quot; \u0026quot;Champala\u0026quot; ## [37] \u0026quot;Geonosis\u0026quot; \u0026quot;Mirial\u0026quot; \u0026quot;Serenno\u0026quot; \u0026quot;Concord Dawn\u0026quot; ## [41] \u0026quot;Zolan\u0026quot; \u0026quot;Ojom\u0026quot; \u0026quot;Aleen Minor\u0026quot; \u0026quot;Skako\u0026quot; ## [45] \u0026quot;Muunilinst\u0026quot; \u0026quot;Shili\u0026quot; \u0026quot;Kalee\u0026quot; \u0026quot;Umbara\u0026quot; ## [49] \u0026quot;Utapau\u0026quot;\r# use filter to get a subset of all star wars characters from either Tatooine or Naboo\rfilter(df2, homeworld == \u0026quot;Tatooine\u0026quot; | homeworld == \u0026quot;Naboo\u0026quot;) #recall that the vertical bar `|` is the logical operator \u0026#39;or\u0026#39;\r## # A tibble: 21 x 4\r## name height species homeworld\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke Skywalker 172 Human Tatooine ## 2 C-3PO 167 Droid Tatooine ## 3 R2-D2 96 Droid Naboo ## 4 Darth Vader 202 Human Tatooine ## 5 Owen Lars 178 Human Tatooine ## 6 Beru Whitesun lars 165 Human Tatooine ## 7 R5-D4 97 Droid Tatooine ## 8 Biggs Darklighter 183 Human Tatooine ## 9 Anakin Skywalker 188 Human Tatooine ## 10 Palpatine 170 Human Naboo ## # ... with 11 more rows\r# this is read: filter the data frame \u0026quot;df2\u0026quot; to extract rows where the value of\r# the variable homeworld is equal to \u0026quot;Tatooine\u0026quot; or homeworld is equal to \u0026quot;Naboo\u0026quot;\r#in base R, one equivalent option would be:\rdf2[(df2$homeworld == \u0026quot;Tatooine\u0026quot; | df2$homeworld == \u0026quot;Naboo\u0026quot;), ]\r## # A tibble: 31 x 4\r## name height species homeworld\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke Skywalker 172 Human Tatooine ## 2 C-3PO 167 Droid Tatooine ## 3 R2-D2 96 Droid Naboo ## 4 Darth Vader 202 Human Tatooine ## 5 Owen Lars 178 Human Tatooine ## 6 Beru Whitesun lars 165 Human Tatooine ## 7 R5-D4 97 Droid Tatooine ## 8 Biggs Darklighter 183 Human Tatooine ## 9 Anakin Skywalker 188 Human Tatooine ## 10 \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## # ... with 21 more rows\r# notice that the $ is not needed to specify which data set the variable\r# \u0026quot;homeworld\u0026quot; is in for filter but it is if you are passing a logical vector to\r# subset a data frame using `[]` in base R\r# this is because R reads (df2$homeworld == \u0026quot;Tatooine\u0026quot; | df2$homeworld == \u0026quot;Naboo\u0026quot;)\r# as a logical test and returns a logical vector, which is then used to specify\r# which rows to extract (those with values == TRUE) and which to discard (those\r# with values == FALSE). This becomes really clear if you just pull out the\r# subsetting predicates and print the result\r(df2$homeworld == \u0026quot;Tatooine\u0026quot; | df2$homeworld == \u0026quot;Naboo\u0026quot;)\r## [1] TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE FALSE TRUE FALSE\r## [13] FALSE FALSE FALSE FALSE FALSE FALSE NA TRUE FALSE NA FALSE FALSE\r## [25] FALSE FALSE FALSE NA FALSE FALSE NA FALSE FALSE TRUE TRUE TRUE\r## [37] TRUE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE\r## [61] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## [73] NA FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE NA NA NA\r## [85] NA NA TRUE\r# to get indices instead of logical values, you can wrap the above in which(), e.g.\rind \u0026lt;- which(df2$homeworld == \u0026quot;Tatooine\u0026quot; | df2$homeworld == \u0026quot;Naboo\u0026quot;) #returns the indices where the criteria are TRUE\rind #calling an object itself without applying any functions to it just prints it to the console\r## [1] 1 2 3 4 6 7 8 9 11 20 34 35 36 37 40 41 57 58 59 63 87\r#note that you could also pass a vector of integers to subset a df2 using `[]`\rdf2[ind, ]\r## # A tibble: 21 x 4\r## name height species homeworld\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke Skywalker 172 Human Tatooine ## 2 C-3PO 167 Droid Tatooine ## 3 R2-D2 96 Droid Naboo ## 4 Darth Vader 202 Human Tatooine ## 5 Owen Lars 178 Human Tatooine ## 6 Beru Whitesun lars 165 Human Tatooine ## 7 R5-D4 97 Droid Tatooine ## 8 Biggs Darklighter 183 Human Tatooine ## 9 Anakin Skywalker 188 Human Tatooine ## 10 Palpatine 170 Human Naboo ## # ... with 11 more rows\r#to negate a logical test, you can use `!` which means \u0026quot;not\u0026quot;\r# in the context of subsetting, this negates a logical vector !(df2$homeworld == \u0026quot;Tatooine\u0026quot; | df2$homeworld == \u0026quot;Naboo\u0026quot;)\r## [1] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE TRUE\r## [13] TRUE TRUE TRUE TRUE TRUE TRUE NA FALSE TRUE NA TRUE TRUE\r## [25] TRUE TRUE TRUE NA TRUE TRUE NA TRUE TRUE FALSE FALSE FALSE\r## [37] FALSE TRUE TRUE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\r## [49] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE TRUE\r## [61] TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\r## [73] NA TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE NA NA NA\r## [85] NA NA FALSE\r#this is why the logical test for not equal to is \u0026quot;!=\u0026quot;\ridentical(!(df2$homeworld == \u0026quot;Tatooine\u0026quot;), df2$homeworld != \u0026quot;Tatooine\u0026quot;) #use identical() or all.equal() to test for equality between entire R objects\r## [1] TRUE\r# extract rows for characters who are at least 100 cm tall and are from Naboo\rfilter(df2, height \u0026gt;= 100 \u0026amp; homeworld == \u0026quot;Naboo\u0026quot;)\r## # A tibble: 10 x 4\r## name height species homeworld\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Palpatine 170 Human Naboo ## 2 Jar Jar Binks 196 Gungan Naboo ## 3 Roos Tarpals 224 Gungan Naboo ## 4 Rugor Nass 206 Gungan Naboo ## 5 Ric Olié 183 \u0026lt;NA\u0026gt; Naboo ## 6 Quarsh Panaka 183 \u0026lt;NA\u0026gt; Naboo ## 7 Gregar Typho 185 Human Naboo ## 8 Cordé 157 Human Naboo ## 9 Dormé 165 Human Naboo ## 10 Padmé Amidala 165 Human Naboo\r# when using filter(), you can substitute a comma for `\u0026amp;` when specifying multiple criteria\ridentical(filter(df2, height \u0026gt;= 100 \u0026amp; homeworld == \u0026quot;Naboo\u0026quot;),\rfilter(df2, height \u0026gt;= 100, homeworld == \u0026quot;Naboo\u0026quot;))\r## [1] TRUE\r# multiple or (`|`) clauses can instead be specified using the %in% infix operator\roption1 \u0026lt;- filter(df2, homeworld == \u0026quot;Tatooine\u0026quot; | homeworld == \u0026quot;Naboo\u0026quot; | homeworld == \u0026quot;Alderaan\u0026quot;)\roption2 \u0026lt;- filter(filter(df2, homeworld %in% c(\u0026quot;Tatooine\u0026quot;, \u0026quot;Naboo\u0026quot;, \u0026quot;Alderaan\u0026quot;)))\ridentical(option1, option2)\r## [1] TRUE\r# since %in% also returns a logical vector, it can also be negated using `!`, # although they give different results if the data contain missing values (NA)\ra \u0026lt;- !(df2$homeworld == \u0026quot;Tatooine\u0026quot; | df2$homeworld == \u0026quot;Naboo\u0026quot; | df2$homeworld == \u0026quot;Alderaan\u0026quot;)\rb \u0026lt;- !(df2$homeworld %in% c(\u0026quot;Tatooine\u0026quot;, \u0026quot;Naboo\u0026quot;, \u0026quot;Alderaan\u0026quot;))\rall.equal(a, b)\r## [1] \u0026quot;\u0026#39;is.NA\u0026#39; value mismatch: 0 in current 10 in target\u0026quot;\rb[is.na(a)] #these missing values were converted to TRUE by %in%\r## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\r# to avoid this and other problems many functions have with processing NA\r# values, it is often best to either remove them or impute them, or use the\r# na.rm argument for the function\rx \u0026lt;- na.omit(df2$homeworld)\rc \u0026lt;- !(x == \u0026quot;Tatooine\u0026quot; | x == \u0026quot;Naboo\u0026quot; | x == \u0026quot;Alderaan\u0026quot;)\r# negating a set of `or` clauses for equality is the same as multiple `and`\r# clauses for inequality\rd \u0026lt;- x != \u0026quot;Tatooine\u0026quot; \u0026amp; x != \u0026quot;Naboo\u0026quot; \u0026amp; x != \u0026quot;Alderaan\u0026quot;\re \u0026lt;- !(x %in% c(\u0026quot;Tatooine\u0026quot;, \u0026quot;Naboo\u0026quot;, \u0026quot;Alderaan\u0026quot;))\rall.equal(c, d, e)\r## [1] TRUE\r# key point: you can use logical comparisons and vectors for subsetting in R\r4.1 Subset Rows using Indices with slice()\rUnlike select(), filter() doesn’t allow the use of indices for subsetting. Instead, dplyr has another funciton called slice() for this purpose. slice() doesn’t require you to enclose multiple index values in c() either and it can also operate on indices within groups (see the section 7 below for an example).\nslice(df2, 1:5, 9:14, 25:50)\r## # A tibble: 37 x 4\r## name height species homeworld\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke Skywalker 172 Human Tatooine ## 2 C-3PO 167 Droid Tatooine ## 3 R2-D2 96 Droid Naboo ## 4 Darth Vader 202 Human Tatooine ## 5 Leia Organa 150 Human Alderaan ## 6 Biggs Darklighter 183 Human Tatooine ## 7 Obi-Wan Kenobi 182 Human Stewjon ## 8 Anakin Skywalker 188 Human Tatooine ## 9 Wilhuff Tarkin 180 Human Eriadu ## 10 Chewbacca 228 Wookiee Kashyyyk ## # ... with 27 more rows\r\r\r5 mutate()\rmutate() lets you modify and/or create new columns in a data frame.\n#Say for example that you wanted to calculate the BMI of each character in the starwars data frame\rdf2 \u0026lt;- select(df, name, height, mass) #extract columns of interest\rdf3 \u0026lt;- mutate(df2,\rheight_in_m = height/100, #convert height from cm to m, store in new column \u0026quot;height_in_m\u0026quot;\rbmi = mass/(height_in_m^2), #add a column called bmi for the calculated body mass index of each character\rbmi = signif(bmi, 1)) #round it to one significant digit/decimal place \u0026amp; assign back to the same name.\rdf3\r## # A tibble: 87 x 5\r## name height mass height_in_m bmi\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Luke Skywalker 172 77 1.72 30\r## 2 C-3PO 167 75 1.67 30\r## 3 R2-D2 96 32 0.96 30\r## 4 Darth Vader 202 136 2.02 30\r## 5 Leia Organa 150 49 1.5 20\r## 6 Owen Lars 178 120 1.78 40\r## 7 Beru Whitesun lars 165 75 1.65 30\r## 8 R5-D4 97 32 0.97 30\r## 9 Biggs Darklighter 183 84 1.83 30\r## 10 Obi-Wan Kenobi 182 77 1.82 20\r## # ... with 77 more rows\r# assigning a modified variable to the same name overwrites/modifies the column\r# note in the above mutate statement I created a new variable then used it in the same function call. # this is possible because mutate() evaluates items from left to right\r# the equivalent operations in base R would typically involve several steps\r# requiring separate assignment of intermediate objects\rdf2$height_in_m \u0026lt;- df2$height/100\rdf2$bmi \u0026lt;- df2$mass/(df2$height_in_m^2)\rdf2$bmi \u0026lt;- signif(df2$bmi, 1)\rall.equal(df2, df3)\r## [1] TRUE\r# As shown above, mutate lets you combine these steps into a single function call and doesn\u0026#39;t\r# require you to specify the dataframe$ component multiple times, you only have\r# to supply the name of the data frame as the 1st argument.\r# the transmute function (also in the dplyr package) is an alternative to mutate that\r# only retains variables you specify (i.e. it combines select and mutate in a single step)\r# You probably won\u0026#39;t use it as often as mutate, but it can be helpful occassionally\rtransmute(df2,\rheight_in_m = height/100, #convert height from cm to m, store in new column \u0026quot;height_in_m\u0026quot;\rbmi = mass/(height_in_m^2), #add a column called bmi for the calculated body mass index of each character\rbmi = signif(bmi, 1))\r## # A tibble: 87 x 2\r## height_in_m bmi\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1.72 30\r## 2 1.67 30\r## 3 0.96 30\r## 4 2.02 30\r## 5 1.5 20\r## 6 1.78 40\r## 7 1.65 30\r## 8 0.97 30\r## 9 1.83 30\r## 10 1.82 20\r## # ... with 77 more rows\r5.1 Recoding or Creating Indicator Variables using if_else(), case_when(), or recode()\rA common use of mutate (or transmute) is to encode a new variable (or overwrite an existing one) based on values of an existing variable in a data frame using the recode(), if_else() or case_when() functions in the dplyr package.\nif_else() is a condensed version of a simple if/else statement with only 2 outcomes: one if the condition specified as the 1st argument evaluates to TRUE and one if it evaluates to FALSE (i.e. no else if components). case_when() lets you use multiple if_else() calls together without having to nest them explicitly. General control flow operations using conditional statments (if, else, else if, etc.) may be covered in a later blog post… For now I recommend this section of the advanced R book for those who want to learn more about it before moving on.\n# to create an indicator/dummy/2-level variable (e.g. 1/0, T/F) you can use\r# if_else() (or ifelse()):\rif_else(df$height \u0026gt; 180, #part 1. logical test\r\u0026quot;tall\u0026quot;, #part 2. value if TRUE\r\u0026quot;short\u0026quot;) #part 3. value if FALSE (should be same class as the value if TRUE)\r## [1] \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;tall\u0026quot; ## [10] \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;short\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot;\r## [19] \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot;\r## [28] NA \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;short\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; ## [37] \u0026quot;tall\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot;\r## [46] \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;short\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; ## [55] \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;short\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot;\r## [64] \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;short\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;short\u0026quot; \u0026quot;short\u0026quot;\r## [73] \u0026quot;short\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;short\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;tall\u0026quot; \u0026quot;short\u0026quot; \u0026quot;tall\u0026quot; ## [82] NA NA NA NA NA \u0026quot;short\u0026quot;\r# use mutate to add it to an existing data frame\rmutate(select(df, name, height), #apply mutate to a subset of the data using a nested select call\rtall_or_short = if_else(height \u0026gt; 180, \u0026quot;tall\u0026quot;, \u0026quot;short\u0026quot;)) \r## # A tibble: 87 x 3\r## name height tall_or_short\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke Skywalker 172 short ## 2 C-3PO 167 short ## 3 R2-D2 96 short ## 4 Darth Vader 202 tall ## 5 Leia Organa 150 short ## 6 Owen Lars 178 short ## 7 Beru Whitesun lars 165 short ## 8 R5-D4 97 short ## 9 Biggs Darklighter 183 tall ## 10 Obi-Wan Kenobi 182 tall ## # ... with 77 more rows\r# to specify more than 2 conditions/outputs you can use case_when():\rmutate(select(df, name, height), size_class = case_when(height \u0026gt; 200 ~ \u0026quot;tall\u0026quot;, #logical_test ~ value_if_TRUE,\rheight \u0026lt; 200 \u0026amp; height \u0026gt; 100 ~ \u0026quot;medium\u0026quot;, height \u0026lt; 100 ~ \u0026quot;short\u0026quot;)\r) \r## # A tibble: 87 x 3\r## name height size_class\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke Skywalker 172 medium ## 2 C-3PO 167 medium ## 3 R2-D2 96 short ## 4 Darth Vader 202 tall ## 5 Leia Organa 150 medium ## 6 Owen Lars 178 medium ## 7 Beru Whitesun lars 165 medium ## 8 R5-D4 97 short ## 9 Biggs Darklighter 183 medium ## 10 Obi-Wan Kenobi 182 medium ## # ... with 77 more rows\r# you can recode a variable using recode():\rrecoded_using_recode \u0026lt;- mutate(select(df, name, hair_color),\rhair_color = recode(hair_color, #the variable you want to recode\r\u0026quot;blond\u0026quot; = \u0026quot;BLOND\u0026quot;,#old_value = new_value\r\u0026quot;brown\u0026quot; = \u0026quot;BROWN\u0026quot;) #you can recode any number of values this way\r#unspecified values are unaltered\r) recoded_using_recode\r## # A tibble: 87 x 2\r## name hair_color ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke Skywalker BLOND ## 2 C-3PO \u0026lt;NA\u0026gt; ## 3 R2-D2 \u0026lt;NA\u0026gt; ## 4 Darth Vader none ## 5 Leia Organa BROWN ## 6 Owen Lars brown, grey ## 7 Beru Whitesun lars BROWN ## 8 R5-D4 \u0026lt;NA\u0026gt; ## 9 Biggs Darklighter black ## 10 Obi-Wan Kenobi auburn, white\r## # ... with 77 more rows\r# unforunately, unlike other dplyr functions, recode()-ed values are specified as\r# \u0026quot;old_value\u0026quot; = \u0026quot;new_value\u0026quot; instead of the more common \u0026quot;new\u0026quot; = \u0026quot;old\u0026quot; scheme used\r# by select() and rename()\r# an alternative would be to use case_when(), e.g.:\rrecoded_using_case_when \u0026lt;- mutate(select(df, name, hair_color),\rhair_color = case_when(hair_color == \u0026quot;blond\u0026quot; ~ \u0026quot;BLOND\u0026quot;,\rhair_color == \u0026quot;brown\u0026quot; ~ \u0026quot;BROWN\u0026quot;, TRUE ~ as.character(hair_color)))\ridentical(recoded_using_recode, recoded_using_case_when) \r## [1] TRUE\r# This time we add in the \u0026quot;TRUE ~ as.character(hair_color)\u0026quot; part to signify that\r# all other non-NA cases should be the existing value of hair color as a\r# character string; otherwise non-matching values are coded as missing values (`NA`)\r# note that recode, like most R functions, returns a copy of the data object, # that you have to assign to a name if you want to save it in the environment\r#of course you can also recode variables in base R using the assingnment operator:\r#`\u0026lt;-` (keyboard shortcut = )\rdf$hair_color[df$hair_color == \u0026quot;blond\u0026quot;] \u0026lt;- \u0026quot;BLOND\u0026quot;\rdf$hair_color[df$hair_color == \u0026quot;brown\u0026quot;] \u0026lt;- \u0026quot;BROWN\u0026quot;\r#but this modifies the data frame in place, which isn\u0026#39;t always what you want\r#e.g. if you are just experimenting with a coding scheme and then decide you\r#want to change it back you\u0026#39;ll have to rerun earlier portions of your R script\r#to recover the previous version\r\r\r6 summarise()\rsummarise() makes it easy to turn a data frame into a smaller data frame of summary statistics.\nsummary1 \u0026lt;- summarise(df, #as for the other dplyr functions, the data source is specified as the 1st argument\rn = n(), #n() is a special function for use in summarize that returns the number of values\rmean_height = mean(height, na.rm = TRUE), #summary stat name in the output = function(column) in the input\rmedian_mass = median(mass, na.rm = TRUE))\rsummary1\r## # A tibble: 1 x 3\r## n mean_height median_mass\r## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 87 174. 79\r#summarise() and summarize() do the same thing and just accommodate different user spelling preferences\rsummary2 \u0026lt;- summarize(df, #as for the other dplyr functions, the data source is specified as the 1st argument\rn = n(), #n() is a special function for use in summarize that returns the number of values\rmean_height = mean(height, na.rm = TRUE), #summary stat name in the output = function(column) in the input\rmedian_mass = median(mass, na.rm = TRUE))\ridentical(summary1, summary2)\r## [1] TRUE\r\r7 group_by()\rgroup_by() adds an explicit grouping structure to a data frame that can subsequently be used by other dplyr functions to perform operations within each level of the grouping variable or level combination of the grouping variables if you specify multiple grouping variables. This is particularly useful in conjunction with either summarize() or mutate(), enabling you to calculate summary statistics or transform variables separately for each of the groups defined by group_by().\n#structure of the data before grouping\rglimpse(df)\r## Observations: 87\r## Variables: 13\r## $ name \u0026lt;chr\u0026gt; \u0026quot;Luke Skywalker\u0026quot;, \u0026quot;C-3PO\u0026quot;, \u0026quot;R2-D2\u0026quot;, \u0026quot;Darth Vader\u0026quot;, \u0026quot;Leia...\r## $ height \u0026lt;int\u0026gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180...\r## $ mass \u0026lt;dbl\u0026gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, ...\r## $ hair_color \u0026lt;chr\u0026gt; \u0026quot;BLOND\u0026quot;, NA, NA, \u0026quot;none\u0026quot;, \u0026quot;BROWN\u0026quot;, \u0026quot;brown, grey\u0026quot;, \u0026quot;BROWN\u0026quot;...\r## $ skin_color \u0026lt;chr\u0026gt; \u0026quot;fair\u0026quot;, \u0026quot;gold\u0026quot;, \u0026quot;white, blue\u0026quot;, \u0026quot;white\u0026quot;, \u0026quot;light\u0026quot;, \u0026quot;light\u0026quot;...\r## $ eye_color \u0026lt;chr\u0026gt; \u0026quot;blue\u0026quot;, \u0026quot;yellow\u0026quot;, \u0026quot;red\u0026quot;, \u0026quot;yellow\u0026quot;, \u0026quot;brown\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;blu...\r## $ birth_year \u0026lt;dbl\u0026gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57....\r## $ gender \u0026lt;chr\u0026gt; \u0026quot;male\u0026quot;, NA, NA, \u0026quot;male\u0026quot;, \u0026quot;female\u0026quot;, \u0026quot;male\u0026quot;, \u0026quot;female\u0026quot;, NA, ...\r## $ homeworld \u0026lt;chr\u0026gt; \u0026quot;Tatooine\u0026quot;, \u0026quot;Tatooine\u0026quot;, \u0026quot;Naboo\u0026quot;, \u0026quot;Tatooine\u0026quot;, \u0026quot;Alderaan\u0026quot;,...\r## $ species \u0026lt;chr\u0026gt; \u0026quot;Human\u0026quot;, \u0026quot;Droid\u0026quot;, \u0026quot;Droid\u0026quot;, \u0026quot;Human\u0026quot;, \u0026quot;Human\u0026quot;, \u0026quot;Human\u0026quot;, \u0026quot;H...\r## $ films \u0026lt;list\u0026gt; [\u0026lt;\u0026quot;Revenge of the Sith\u0026quot;, \u0026quot;Return of the Jedi\u0026quot;, \u0026quot;The Emp...\r## $ vehicles \u0026lt;list\u0026gt; [\u0026lt;\u0026quot;Snowspeeder\u0026quot;, \u0026quot;Imperial Speeder Bike\u0026quot;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, ...\r## $ starships \u0026lt;list\u0026gt; [\u0026lt;\u0026quot;X-wing\u0026quot;, \u0026quot;Imperial shuttle\u0026quot;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026quot;TIE Advanced ...\rdf3 \u0026lt;- select(df, name, species, height, mass)#1st. extract just the columns of interest\rgrouped_data \u0026lt;- group_by(df3, species) #group the data frame \u0026quot;df3\u0026quot; by the variable \u0026quot;species\u0026quot;\rclass(grouped_data) #new additional class = \u0026quot;grouped_df\u0026quot;\r## [1] \u0026quot;grouped_df\u0026quot; \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;\rglimpse(grouped_data) #note that the structure now has a groups attribute\r## Observations: 87\r## Variables: 4\r## Groups: species [38]\r## $ name \u0026lt;chr\u0026gt; \u0026quot;Luke Skywalker\u0026quot;, \u0026quot;C-3PO\u0026quot;, \u0026quot;R2-D2\u0026quot;, \u0026quot;Darth Vader\u0026quot;, \u0026quot;Leia Or...\r## $ species \u0026lt;chr\u0026gt; \u0026quot;Human\u0026quot;, \u0026quot;Droid\u0026quot;, \u0026quot;Droid\u0026quot;, \u0026quot;Human\u0026quot;, \u0026quot;Human\u0026quot;, \u0026quot;Human\u0026quot;, \u0026quot;Huma...\r## $ height \u0026lt;int\u0026gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2...\r## $ mass \u0026lt;dbl\u0026gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77....\rslice(grouped_data, 1) #look at the 1st row of each group\r## # A tibble: 38 x 4\r## # Groups: species [38]\r## name species height mass\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Ratts Tyerell Aleena 79 15\r## 2 Dexter Jettster Besalisk 198 102\r## 3 Ki-Adi-Mundi Cerean 198 82\r## 4 Mas Amedda Chagrian 196 NA\r## 5 Zam Wesell Clawdite 168 55\r## 6 C-3PO Droid 167 75\r## 7 Sebulba Dug 112 40\r## 8 Wicket Systri Warrick Ewok 88 20\r## 9 Poggle the Lesser Geonosian 183 80\r## 10 Jar Jar Binks Gungan 196 66\r## # ... with 28 more rows\r#next summarise calculates the specified summary statistics within species\rsummarise(grouped_data,\rn = n(), mean_height = mean(height, na.rm = TRUE), median_mass = median(mass, na.rm = TRUE))\r## # A tibble: 38 x 4\r## species n mean_height median_mass\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Aleena 1 79 15 ## 2 Besalisk 1 198 102 ## 3 Cerean 1 198 82 ## 4 Chagrian 1 196 NA ## 5 Clawdite 1 168 55 ## 6 Droid 5 140 53.5\r## 7 Dug 1 112 40 ## 8 Ewok 1 88 20 ## 9 Geonosian 1 183 80 ## 10 Gungan 3 209. 74 ## # ... with 28 more rows\r#summarise and summarize do the same thing and just accommodate different user spelling preferences\ridentical(summary1, summary2)\r## [1] TRUE\r#to apply mutate() within each level of a grouping variable, simply use mutate\r#on a grouped data frame:\rmutated_grouped_data \u0026lt;- mutate(grouped_data, scaled_mass = scale(mass)) #convert raw masses into standardized masses (i.e. z-scores)\rselect(mutated_grouped_data, name, mass, scaled_mass) #just print the relevant parts\r## Adding missing grouping variables: `species`\r## # A tibble: 87 x 4\r## # Groups: species [38]\r## species name mass scaled_mass\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Human Luke Skywalker 77 -0.298 ## 2 Droid C-3PO 75 0.103 ## 3 Droid R2-D2 32 -0.740 ## 4 Human Darth Vader 136 2.75 ## 5 Human Leia Organa 49 -1.74 ## 6 Human Owen Lars 120 1.92 ## 7 Human Beru Whitesun lars 75 -0.401 ## 8 Droid R5-D4 32 -0.740 ## 9 Human Biggs Darklighter 84 0.0628\r## 10 Human Obi-Wan Kenobi 77 -0.298 ## # ... with 77 more rows\r#note that this is done within species, and the results differ from the same\r#operation applied to the ungrouped data\ridentical(\rmutated_grouped_data$scaled_mass,\rmutate(df3, scaled_mass = scale(mass))$scaled_mass,\r)\r## [1] FALSE\r#it is recommended that you ungroup the data after you\u0026#39;re finished mutating it\r#if you want subsequent operations to be applied to the entire data frame\r#instead of within groups\rungrouped_data \u0026lt;- ungroup(mutated_grouped_data)\rglimpse(ungrouped_data) #notice that the groups attribute is no longer there\r## Observations: 87\r## Variables: 5\r## $ name \u0026lt;chr\u0026gt; \u0026quot;Luke Skywalker\u0026quot;, \u0026quot;C-3PO\u0026quot;, \u0026quot;R2-D2\u0026quot;, \u0026quot;Darth Vader\u0026quot;, \u0026quot;Lei...\r## $ species \u0026lt;chr\u0026gt; \u0026quot;Human\u0026quot;, \u0026quot;Droid\u0026quot;, \u0026quot;Droid\u0026quot;, \u0026quot;Human\u0026quot;, \u0026quot;Human\u0026quot;, \u0026quot;Human\u0026quot;, \u0026quot;...\r## $ height \u0026lt;int\u0026gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 18...\r## $ mass \u0026lt;dbl\u0026gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0,...\r## $ scaled_mass \u0026lt;dbl\u0026gt; -0.29828800, 0.10287692, -0.73973407, 2.74556278, -1.74...\r#and grouped_df3 is no longer one of the data frame\u0026#39;s classes\rclass(ungrouped_data)\r## [1] \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;\r\r8 Chaining Functions with the pipe operator (%\u0026gt;%)\rThanks to the magrittr package, you can use the pipe operator, “%\u0026gt;%”, which allows you to apply a series of functions to an object using easily readable code without requiring you to save intermediate objects.\nNote that you don’t have to load the magrittr package in a separate call to the library() function since the pipe operator is imported from magrittr automatically when you load the dplyr package.\nThe pipe operator takes the object on the left and passes it to the first argument of the function to the right. Functions in dplyr and other tidyverse packages take data as the 1st argument. Those function which output the same type of object that they accept as inputs (i.e. the data argument) can be chained using the “%\u0026gt;%” quite easily.\nFor example,\n# %\u0026gt;% lets you turn this nested code (evaluated from the inside out)... arrange(summarize(group_by(select(starwars, species, mass), species), mean_mass = mean(mass, na.rm = T)), desc(mean_mass))\r## # A tibble: 38 x 2\r## species mean_mass\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Hutt 1358 ## 2 Kaleesh 159 ## 3 Wookiee 124 ## 4 Trandoshan 113 ## 5 Besalisk 102 ## 6 Neimodian 90 ## 7 Kaminoan 88 ## 8 Nautolan 87 ## 9 Mon Calamari 83 ## 10 Human 82.8\r## # ... with 28 more rows\r# ...which could also be written more clearly like this:\rarrange(#evaluated last\rsummarize(#evaluated 3rd\rgroup_by(#evaluated 2nd\rselect(#evaluated 1st\rstarwars, species, mass), #1. from the starwars data frame, select() the columns species and mass\rspecies), #2. then use group_by() to group the data by species\rmean_mass = mean(mass, na.rm = T)), #3. then use summarize() to calculate the mean() mass for each species\rdesc(mean_mass)) #4. then arrange() the summary data frame by mean_mass in descending order\r## # A tibble: 38 x 2\r## species mean_mass\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Hutt 1358 ## 2 Kaleesh 159 ## 3 Wookiee 124 ## 4 Trandoshan 113 ## 5 Besalisk 102 ## 6 Neimodian 90 ## 7 Kaminoan 88 ## 8 Nautolan 87 ## 9 Mon Calamari 83 ## 10 Human 82.8\r## # ... with 28 more rows\r# ... into this tidy chain, which reads much more naturally\rsorted_mass_by_sw_species_chained \u0026lt;- starwars %\u0026gt;% #take the starwars df select(species, mass) %\u0026gt;% #pass it to the 1st agrument of select() (i.e. data), then extract species and mass\rgroup_by(species) %\u0026gt;% #output of select() becomes the input to the 1st argument of group_by(); group it by species\rsummarise(mean_mass = mean(mass, na.rm = T)) %\u0026gt;% #summarise() the grouped data frame to get the mean mass per group\rarrange(desc(mean_mass)) # arrange the grouped summary by the mean mass column in descending order\rsorted_mass_by_sw_species_chained\r## # A tibble: 38 x 2\r## species mean_mass\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Hutt 1358 ## 2 Kaleesh 159 ## 3 Wookiee 124 ## 4 Trandoshan 113 ## 5 Besalisk 102 ## 6 Neimodian 90 ## 7 Kaminoan 88 ## 8 Nautolan 87 ## 9 Mon Calamari 83 ## 10 Human 82.8\r## # ... with 28 more rows\r# the order of operations using %\u0026gt;% is so much more obvious and produces code\r# that can be read at a glance and doesn\u0026#39;t require intermediate objects which\r# each occupy memory, e.g.\rstarwars_sub \u0026lt;- select(starwars, species, mass) grouped_starwars_sub \u0026lt;- group_by(starwars_sub, species) mass_by_sw_species \u0026lt;- summarise(grouped_starwars_sub, mean_mass = mean(mass, na.rm = T))\rsorted_mass_by_sw_species_unchained \u0026lt;- arrange(mass_by_sw_species, desc(mean_mass)) # and then have to be deleted afterwards to reclaim the memory rm(starwars_sub)\rrm(grouped_starwars_sub)\rrm(mass_by_sw_species)\ridentical(sorted_mass_by_sw_species_chained, sorted_mass_by_sw_species_unchained) #the output is the same\r## [1] TRUE\r# note: the keyboard shortcut for %\u0026gt;% is ctrl + shift + M (at least on Windows machines)\rWe’ve seen that the default behaviour of %\u0026gt;% is to pass the output of the left hand side (LHS) to the 1st argument of the right hand side (RHS), i.e.\nx %\u0026gt;% f(y) is equivalent to f(x, y)\nThis is great if you’re working with tidyverse functions that accept a data frame as the 1st argument, but what if you want to pass the LHS output to another argument or position in the function on the RHS (e.g. functions with data arguments in other positions)?\n%\u0026gt;% enables you to do this by automatically assigning the LHS output to the special value “.”. To change the argument you want to assign the LHS output to, you just have to set that argument to “.”, e.g.\nx %\u0026gt;% f(y) is equivalent to x %\u0026gt;% f(., y) and\ny %\u0026gt;% f(x, .) is equivalent to f(x, y) and\nz %\u0026gt;% f(x, y, .) is equivalent to f(x, y, z)\n\r9 Navigation\rClick here to go back to the previous post or here to go to the next one on reshaping data with the tidyr package.\n\r10 Notes\r\rTo explore dplyr and in more depth checkout chapter 5 of R for data science.\n\rTo learn more about subsetting using base R I highly recommend chapter 4 of the Advanced R book.\n\rFor more details on other data manipulation techniques in base R checkout the official R introductory manual.\n\r\rThank you for visiting my blog. I welcome any suggestions for future posts, comments or other feedback you might have. Feedback from beginners and science students/trainees (or with them in mind) is especially helpful in the interest of making this guide even better for them.\n\r","date":1577664000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577664000,"objectID":"8974def855255314ff7d5bb239739947","permalink":"/post/2019-12-30-asgr-2-1-data-transformation-part-1/","publishdate":"2019-12-30T00:00:00Z","relpermalink":"/post/2019-12-30-asgr-2-1-data-transformation-part-1/","section":"post","summary":"1 TL;DR\r2 Introduction\r3 select()\r3.1 Renaming Columns with select() or rename()\r\r4 filter()\r4.1 Subset Rows using Indices with slice()\r\r5 mutate()\r5.1 Recoding or Creating Indicator Variables using if_else(), case_when(), or recode()\r\r6 summarise()\r7 group_by()\r8 Chaining Functions with the pipe operator (%\u0026gt;%)\r9 Navigation\r10 Notes\r\r\r1 TL;DR\rThe 4th post in the Scientist’s Guide to R series introduces data transformation techniques useful for wrangling/tidying/cleaning data.","tags":["R","R basics"],"title":"A Scientist's Guide to R: Step 2.1. Data Transformation - Part 1","type":"post"},{"authors":null,"categories":["R","Reproducible Research","Data Cleaning and Transformation"],"content":"\r\r1 TL;DR\r2 Introduction\r3 Basic Calculations\r4 Logical Operators\r5 Object Assignment\r6 Basic Summary Statistics\r7 Data Structures and Object Assignment\r7.1 Numeric and Character Vectors\r7.2 Logical Vectors\r7.3 Factors\r7.4 Matrices\r7.5 Dataframes\r7.6 Tibbles\r\r8 Random Numbers and Sampling\r9 Functions for Describing the Structural Information of Data Objects\r10 The Global Environment\r11 The Working Directory\r12 Projects\r13 Useful Keyboard Shortcuts (for R studio users)\r13.1 Navigation\r13.2 Notes\r\r\r\r1 TL;DR\rAs the third post in the Scientist’s Guide to R series (click here for the 1st post), we advance to the brink of the next major stage of the data analysis with R process: cleaning and transforming data. However, before we can clean or transform anything we will need to know how to do a few basic things and familiarize ourselves with some common data structures in R, which are the topics of this post.\n\r2 Introduction\rYou’ve managed to import your data and are wondering what to do next? The way in which you should proceed will critically depend upon the structure of your data, but before we get there you need to know which sorts of things you can do in R. Accordingly, this post will introduce you to some basic operations and data structures. There is a lot of content here so feel free to skip sections you are already familiar with. If this is all new you should take the time to read it since we will continue building upon this foundation as we progress to more advanced topics. You need to understand how data are represented by R before you start manipulating those representations.\n\r3 Basic Calculations\rLike any decent data analysis software, R can perform common mathematical and logical operations, as simply as you probably expect them to be.\n#Basic calculations#####\r5+6 #addition\r89-28 #subtraction\r7000*10 #multiplication\r25/5 #division\r2^20 #^ means to the power of\rexp(8) #exponential\r37 %% 2 #modulus. Returns the remainder after division.\r\r4 Logical Operators\rAlso very straightforward. These are mostly useful when selecting subsets of data or programming.\n#use double equality symbols to check for equality since \u0026quot;=\u0026quot; is reserved for\r#assignment or value specification\r1 == 1 #LHS is equal to RHS\r1 != 1 #LHS is not equal to RHS\r10 \u0026gt; 8 #LHS greater than RHS\r10 \u0026gt;= 8 #LHS greater than or equal to RHS\r10 \u0026lt; 8 #LHS less than RHS\r5 \u0026lt;= 5 #LHS less than or equal to RHS\r(1 \u0026gt; 3) \u0026amp; (10 \u0026gt; 3) #are both 1 and 10 greater than 3?\r(1 \u0026gt; 3) | (10 \u0026gt; 3) #is 1 or 10 greater than 3? #see https://www.statmethods.net/management/operators.html for more examples. \r\r5 Object Assignment\rTo save some values (or virtually anything else) to use again later you assign them to a variable. This can be done in R using either “\u0026lt;-” or “=”, but “\u0026lt;-” is typically recommended. See here for the subtleties, e.g. “=” sometimes fails when you wouldn’t expect it to, but “\u0026lt;-” typically does not. R studio also provides a handy keyboard shortcut for inserting “\u0026lt;-”: [Alt] \u0026amp; [-]\nx \u0026lt;- 27 #this stores whatever is on the RHS in the arbitrarily named variable on the LHS\rx #to print it to the console, just call the variable.\r## [1] 27\r#you can assign something to a variable and print it at the same time by wrapping it in parentheses\r(y \u0026lt;- 900)\r## [1] 900\r#although doing so is uncommon, the use of arrows for assignment also enables\r#you to assign things in the opposite direction, from left to right. Arrows also\r#make it obvious which side is being assigned to which side of the operator\r90 -\u0026gt; z\rz\r## [1] 90\r#to remove a variable from your environment use the rm() function\rrm(x)\r#assigning something to a variable that is already in use will overwrite the variable\ry \u0026lt;- 8\ry\r## [1] 8\r#the only naming conventions are that a name should not begin with a\r#number, and you should avoid using names that already exist as functions in R\r#or special characters, e.g.:\r1.m \u0026lt;- 7 #starting a variable name with a number produces an error\rTRUE \u0026lt;- 9 #TRUE is a reserved keyword for the logical statement true, also results in an error\rmean \u0026lt;- 99 #here I assign the number 99 to the same name as the mean function\r#this is probably not what you want since it makes calling mean ambiguous.\rmean\rrm(mean)\r#to check if a name is already reserved for something else, just try to look up\r#the help page for it using ?name\r?mean\r#since pretty much anything can be assigned to a variable, you can also chain assignment operations\r(y \u0026lt;- 900)\r(z \u0026lt;- y \u0026lt;- 900) #store the RHS in 2 separate variables, called y and z\r#usually you would store something slightly different in each of them, e.g. a\r#modification of the 1st variable is stored in a 2nd variable\rz \u0026lt;- y + 10 z\r\r6 Basic Summary Statistics\rR makes it incredibly easy to get simple summary statistics for numeric variables. So while we won’t dive too deeply into exploratory data analysis until later in the blog series we’ll get our toes wet here.\n#R also provides a set of basic functions for conducting common summary\r#statistic calculations\rx \u0026lt;- c(0:10)\rsum(x) #obtain the sum of the sequence of numbers from 1 to 100\r## [1] 55\rmean(x) #mean\r## [1] 5\rsd(x) #standard deviation\r## [1] 3.316625\rmedian(x) #a shortcut for the 50th percentile AKA the median\r## [1] 5\rmin(x) #the minimum\r## [1] 0\rmax(x) #the maximum\r## [1] 10\r\r7 Data Structures and Object Assignment\rMost of the things you will do in R operate upon a variety of objects.\nThe common R data structures are:\n\rVectors = a one-dimensional array of data elements/values\n\rFactors = R’s way of representing vectors as categorical variables for statistical testing and data visualization purposes.\n\rMatrices = a two-dimensional array of vectors arranged in columns and rows. All data elements are typically of the same class. Columns are rows are numbered by position from left to right (columns) and top to bottom (rows).\n\rData Frames = Also a 2D array but columns can be different classes. These are the most common data structure in R.\n\rTibbles = an enhanced version of the data frame with improved display characteristics. Tibbles have gained popularity as the tidyverse has become more prominent over the past few years.\n\rLists: a list of arbitrary objects, more flexible than data frames but less intuitive to work with. Lists elements can consist of multiple different data types, including the output of statistical tests and even entire data frames. Lists are a more advanced topic so they won’t be covered further for the time being. If you want to learn more about them now see this video.\n\rArrays: If you want to work with matrices that have more than 2 dimensions (remember that a matrix is a 2D array), checkout the array function. Arrays with more than 2 dimensions are not commonly used for the analysis of most forms of experimental data (e.g. I’ve never needed them), so I won’t be covering them in this blog series. Those who are interested in learning more about them can find a brief intro here. A more detailed treatment is provided in section 5 of the official R introductory manual\n\r\rN.B. One of the most important functions that base R provides is str(), which enables you to inspect the structure of any object.\n7.1 Numeric and Character Vectors\r#A vector is a one-dimensional array of numerical values or character strings.\r#You can create one using the c() function in R, e.g.:\rc(1, 3, 4, 5, 6)\r## [1] 1 3 4 5 6\r#to save this vector for future use, you assign it to a variable using either \u0026quot;=\u0026quot; or \u0026quot;\u0026lt;-\u0026quot;\r#it is generally recommended to use \u0026quot;\u0026lt;-\u0026quot; for assignment instead of \u0026quot;=\u0026quot;, although either will work #By wrapping the vector in str(), you gain structural information\rstr(c(1, 3, 4, 5, 6)) #a numeric vector of length 5, with values shown\r## num [1:5] 1 3 4 5 6\rc(1:100) #all numbers between a range can be specified using number:number\r## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\r## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36\r## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54\r## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\r## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90\r## [91] 91 92 93 94 95 96 97 98 99 100\rstr(c(1:100)) #a numeric vector of length 100, with the 1st 10 values shown\r## int [1:100] 1 2 3 4 5 6 7 8 9 10 ...\rc(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;) #character values/strings need to be quoted to be parsed correctly\r## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot;\r#anything entered in quotation marks is read text/character, and if at least 1\r#element is quoted, all elements of a vector are coerced to string format\rc(\u0026quot;1\u0026quot;, 2, \u0026quot;3\u0026quot;) #prints as a character vector\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot;\r#if a string vector contains only numbers, you can reclassify it as numeric\r#using as.numeric()\ras.numeric(c(\u0026quot;1\u0026quot;, 2, \u0026quot;3\u0026quot;)) #now it prints as a numeric vector\r## [1] 1 2 3\rstr(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;)) #str tells us that this is a character vector of length 3\r## chr [1:3] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot;\rc(\u0026quot;apples\u0026quot;, \u0026quot;bananas\u0026quot;, \u0026quot;cherries\u0026quot;) #each entry is in quotations and separated by a comma\r## [1] \u0026quot;apples\u0026quot; \u0026quot;bananas\u0026quot; \u0026quot;cherries\u0026quot;\rstr(c(\u0026quot;apples\u0026quot;, \u0026quot;bananas\u0026quot;, \u0026quot;cherries\u0026quot;)) #also a character vector of length 3\r## chr [1:3] \u0026quot;apples\u0026quot; \u0026quot;bananas\u0026quot; \u0026quot;cherries\u0026quot;\rc_vect \u0026lt;- letters[seq(from = 1, to = 26)] #letters[] lets you quickly create a character vector of letters\r#There are 2 subclasses of numeric vector: #\u0026quot;integer\u0026quot;, which contains only whole numbers, and \u0026quot;double\u0026quot;(also simply called\r#numeric; both labels refer to the same thing), which has both integer and\r#decimal components\r#the class function returns the class of an object. see ?class for more info.\rx \u0026lt;- c(1, 3, 4, 5, 6)\rclass(x) #by default, both integers and doubles are classified as \u0026quot;numeric\u0026quot;\r## [1] \u0026quot;numeric\u0026quot;\rx \u0026lt;- as.integer(x) #you can coerce a vector into a different class, if it is reasonable to do so.\rclass(x) #now R reads the number list as an integer vector\r## [1] \u0026quot;integer\u0026quot;\r#to conduct a logical test to see if an object is a particular class use\r#is.[class] (insert the class name), e.g.\ris.integer(x)\r## [1] TRUE\r\r7.2 Logical Vectors\rl \u0026lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE, TRUE) #logical vector\rl \u0026lt;- c(T, F, T, T, F, T) #equivalent short hand version\rclass(l)\r## [1] \u0026quot;logical\u0026quot;\r#\u0026quot;TRUE\u0026quot; and \u0026quot;FALSE\u0026quot;, which may be abbreviated as \u0026quot;T\u0026quot; and \u0026quot;F\u0026quot;, are also read by R as 1 and 0 respectively.\r#this conveniently enables you to obtain the total number of TRUE elements using the sum() function\rsum(l)\r## [1] 4\r#you can also get the proportion of TRUE values using the mean function\rmean(l)\r## [1] 0.6666667\r#both of these are very useful when conducting logical tests of a vector to see\r#how many elements match the specified criteria, e.g.:\rx \u0026lt;- c(1:10, 100:130)\rx \u0026lt; 50 #returns a logical vector with True corresponding to elements matching the logical test, \r## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE\r## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## [37] FALSE FALSE FALSE FALSE FALSE\r#in this case that the value is less than 50\rsum(x \u0026lt; 50) #how many values in x are less than 50\r## [1] 10\rmean(x \u0026lt; 50) #what proportion of the values in x are less than 50\r## [1] 0.2439024\r#using modulus 2 (divide by 2 and return the remainder) can tell you which\r#elements of a numeric vector are odd or even, which is most useful for programming.\r(x %% 2) == 0 #even values in x = TRUE\r## [1] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE TRUE FALSE\r## [13] TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE\r## [25] TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE\r## [37] TRUE FALSE TRUE FALSE TRUE\r(x %% 2) == 1 #odd values in x = TRUE\r## [1] TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE FALSE TRUE\r## [13] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE\r## [25] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE\r## [37] FALSE TRUE FALSE TRUE FALSE\r#you can do similar tests with character vectors\rc_vect \u0026lt;- c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;)\rmean(c_vect == \u0026quot;a\u0026quot;) #how many elements of c_vect are \u0026quot;a\u0026quot;\r## [1] 0.4\rsum(c_vect == \u0026quot;a\u0026quot;) #what proportion of the elements of c_vect are \u0026quot;a\u0026quot;\r## [1] 2\r#logical tests are also used to subset or filter data, as demonstrated later on...\r\r7.3 Factors\r#Factors are a modified form of either a characeter or numeric vector which\r#facilitates their use as categorical variables. You can create a factor in R\r#using either the factor() function, or by reclassifying another vector type\r#using as.factor()\rx \u0026lt;- c(1:3, 1:3, 1:3)\ras.factor(x) \r## [1] 1 2 3 1 2 3 1 2 3\r## Levels: 1 2 3\r#Note the addition of levels to the output. What as.factor does is assign each\r#unique value of the vector to a level of the factor. Under the hood this also\r#involves constructing a set of 0/1 dummy variables for each level of the\r#factor, which are needed to fit some models, like linear regession models (but\r#don\u0026#39;t worry about this, because R will do it for you automatically).\r#using the factor() function instead allows you to assign labels to the levels #you can also specify whether or not the factor is ordered\rfactor(x, levels = c(1, 2, 3), labels = c(\u0026quot;one\u0026quot;, \u0026quot;two\u0026quot;, \u0026quot;three\u0026quot;), ordered = TRUE)\r## [1] one two three one two three one two three\r## Levels: one \u0026lt; two \u0026lt; three\r\r7.4 Matrices\r#the matrix() function allows you to create a matrix\r#create an empty matrix with row and column dimensions specified using nrow and\r#ncol arguments. This can be useful if you plan to fill it in later (e.g. when\r#writing loops).\rmat1 \u0026lt;- matrix(nrow = 100, ncol = 10) str(mat1) #view the structure of the new matrix\r## logi [1:100, 1:10] NA NA NA NA NA NA ...\r#take a vector and distribute the values into 2 columns with 20 rows each\rmatrix(data = c(1:40), nrow = 20, ncol = 2) \r## [,1] [,2]\r## [1,] 1 21\r## [2,] 2 22\r## [3,] 3 23\r## [4,] 4 24\r## [5,] 5 25\r## [6,] 6 26\r## [7,] 7 27\r## [8,] 8 28\r## [9,] 9 29\r## [10,] 10 30\r## [11,] 11 31\r## [12,] 12 32\r## [13,] 13 33\r## [14,] 14 34\r## [15,] 15 35\r## [16,] 16 36\r## [17,] 17 37\r## [18,] 18 38\r## [19,] 19 39\r## [20,] 20 40\r#you can combine vectors into a matrix using cbind(vector1, vector2) or\r#rbind(vector1, vector2) depending on whether you want to combine elements as\r#row vectors or column vectors. each must be the same lenght, otherwise some\r#elements will be coded as NA for the shorter vector\rx1 \u0026lt;- c(1:50)\rx2 \u0026lt;- c(51:100)\rx3 \u0026lt;- c(1:10)\r(mat1 \u0026lt;- cbind(x1, x2)) #combined by column\r## x1 x2\r## [1,] 1 51\r## [2,] 2 52\r## [3,] 3 53\r## [4,] 4 54\r## [5,] 5 55\r## [6,] 6 56\r## [7,] 7 57\r## [8,] 8 58\r## [9,] 9 59\r## [10,] 10 60\r## [11,] 11 61\r## [12,] 12 62\r## [13,] 13 63\r## [14,] 14 64\r## [15,] 15 65\r## [16,] 16 66\r## [17,] 17 67\r## [18,] 18 68\r## [19,] 19 69\r## [20,] 20 70\r## [21,] 21 71\r## [22,] 22 72\r## [23,] 23 73\r## [24,] 24 74\r## [25,] 25 75\r## [26,] 26 76\r## [27,] 27 77\r## [28,] 28 78\r## [29,] 29 79\r## [30,] 30 80\r## [31,] 31 81\r## [32,] 32 82\r## [33,] 33 83\r## [34,] 34 84\r## [35,] 35 85\r## [36,] 36 86\r## [37,] 37 87\r## [38,] 38 88\r## [39,] 39 89\r## [40,] 40 90\r## [41,] 41 91\r## [42,] 42 92\r## [43,] 43 93\r## [44,] 44 94\r## [45,] 45 95\r## [46,] 46 96\r## [47,] 47 97\r## [48,] 48 98\r## [49,] 49 99\r## [50,] 50 100\rclass(mat1)\r## [1] \u0026quot;matrix\u0026quot;\r(mat2 \u0026lt;- rbind(x1, x2)) #combined by row\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\r## x1 1 2 3 4 5 6 7 8 9 10 11 12 13 14\r## x2 51 52 53 54 55 56 57 58 59 60 61 62 63 64\r## [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\r## x1 15 16 17 18 19 20 21 22 23 24 25 26\r## x2 65 66 67 68 69 70 71 72 73 74 75 76\r## [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\r## x1 27 28 29 30 31 32 33 34 35 36 37 38\r## x2 77 78 79 80 81 82 83 84 85 86 87 88\r## [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\r## x1 39 40 41 42 43 44 45 46 47 48 49 50\r## x2 89 90 91 92 93 94 95 96 97 98 99 100\rclass(mat2)\r## [1] \u0026quot;matrix\u0026quot;\r#combining vecotrs of unequal length produces NAs for the extra indices of the\r#longer one\r#common matrix operations work as expected with matrix objects in R\rt(mat1) #transposition\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\r## x1 1 2 3 4 5 6 7 8 9 10 11 12 13 14\r## x2 51 52 53 54 55 56 57 58 59 60 61 62 63 64\r## [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\r## x1 15 16 17 18 19 20 21 22 23 24 25 26\r## x2 65 66 67 68 69 70 71 72 73 74 75 76\r## [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\r## x1 27 28 29 30 31 32 33 34 35 36 37 38\r## x2 77 78 79 80 81 82 83 84 85 86 87 88\r## [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\r## x1 39 40 41 42 43 44 45 46 47 48 49 50\r## x2 89 90 91 92 93 94 95 96 97 98 99 100\rt(mat1) + mat2 #addition (both matrices need to have the same dimensions)\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\r## x1 2 4 6 8 10 12 14 16 18 20 22 24 26 28\r## x2 102 104 106 108 110 112 114 116 118 120 122 124 126 128\r## [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\r## x1 30 32 34 36 38 40 42 44 46 48 50 52\r## x2 130 132 134 136 138 140 142 144 146 148 150 152\r## [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\r## x1 54 56 58 60 62 64 66 68 70 72 74 76\r## x2 154 156 158 160 162 164 166 168 170 172 174 176\r## [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\r## x1 78 80 82 84 86 88 90 92 94 96 98 100\r## x2 178 180 182 184 186 188 190 192 194 196 198 200\rt(mat1) - mat2 #subtraction\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\r## x1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r## x2 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r## [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\r## x1 0 0 0 0 0 0 0 0 0 0 0 0\r## x2 0 0 0 0 0 0 0 0 0 0 0 0\r## [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\r## x1 0 0 0 0 0 0 0 0 0 0 0 0\r## x2 0 0 0 0 0 0 0 0 0 0 0 0\r## [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\r## x1 0 0 0 0 0 0 0 0 0 0 0 0\r## x2 0 0 0 0 0 0 0 0 0 0 0 0\rt(mat1) * mat2 #multiplication\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\r## x1 1 4 9 16 25 36 49 64 81 100 121 144 169 196\r## x2 2601 2704 2809 2916 3025 3136 3249 3364 3481 3600 3721 3844 3969 4096\r## [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\r## x1 225 256 289 324 361 400 441 484 529 576 625 676\r## x2 4225 4356 4489 4624 4761 4900 5041 5184 5329 5476 5625 5776\r## [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\r## x1 729 784 841 900 961 1024 1089 1156 1225 1296 1369 1444\r## x2 5929 6084 6241 6400 6561 6724 6889 7056 7225 7396 7569 7744\r## [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\r## x1 1521 1600 1681 1764 1849 1936 2025 2116 2209 2304 2401 2500\r## x2 7921 8100 8281 8464 8649 8836 9025 9216 9409 9604 9801 10000\rt(mat1) / mat2 #division\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\r## x1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r## x2 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r## [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\r## x1 1 1 1 1 1 1 1 1 1 1 1 1\r## x2 1 1 1 1 1 1 1 1 1 1 1 1\r## [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\r## x1 1 1 1 1 1 1 1 1 1 1 1 1\r## x2 1 1 1 1 1 1 1 1 1 1 1 1\r## [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\r## x1 1 1 1 1 1 1 1 1 1 1 1 1\r## x2 1 1 1 1 1 1 1 1 1 1 1 1\r\r7.5 Dataframes\r#data frames can be constructed using the dataframe function,\r#or by converting a combination of vectors/matrix using as.data.frame()\rx \u0026lt;- cbind(sample(1:6), rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), 2)) #create a 2-column\rclass(x) #a matrix\r## [1] \u0026quot;matrix\u0026quot;\r#if names are not associated with the columns the variables are labelled using\r#V[index]\rdf \u0026lt;- as.data.frame(x) df\r## V1 V2\r## 1 3 a\r## 2 5 b\r## 3 2 c\r## 4 1 a\r## 5 4 b\r## 6 6 c\rclass(df) #now a data frame\r## [1] \u0026quot;data.frame\u0026quot;\rdf \u0026lt;- as.data.frame(x) names(df) \u0026lt;- c(\u0026quot;var_1\u0026quot;, \u0026quot;var_2\u0026quot;) #change column names using the names() function\rstr(df)\r## \u0026#39;data.frame\u0026#39;: 6 obs. of 2 variables:\r## $ var_1: Factor w/ 6 levels \u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;,..: 3 5 2 1 4 6\r## $ var_2: Factor w/ 3 levels \u0026quot;a\u0026quot;,\u0026quot;b\u0026quot;,\u0026quot;c\u0026quot;: 1 2 3 1 2 3\r#create the data frame directly and specify names\rdf \u0026lt;- data.frame(\u0026quot;var_1\u0026quot; = c(sample(1:6, 6)), \u0026quot;var_2\u0026quot; = rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), 2)) str(df)\r## \u0026#39;data.frame\u0026#39;: 6 obs. of 2 variables:\r## $ var_1: int 5 2 3 4 6 1\r## $ var_2: Factor w/ 3 levels \u0026quot;a\u0026quot;,\u0026quot;b\u0026quot;,\u0026quot;c\u0026quot;: 1 2 3 1 2 3\r#if you don\u0026#39;t want strings to be converted to factors automatically, set\r#stringsAsFactors = FALSE\rdf \u0026lt;- data.frame(\u0026quot;var_1\u0026quot; = c(sample(1:6, 6)), \u0026quot;var_2\u0026quot; = rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), 2),\rstringsAsFactors = FALSE) \r\r7.6 Tibbles\r#load the tidyverse packages, which contain the tibble and as_tibble functions\rlibrary(tidyverse)\r## -- Attaching packages ------------------------------------------------------------------------------------------------------------ tidyverse 1.2.1 --\r## v ggplot2 3.2.1 v purrr 0.3.3\r## v tibble 2.1.3 v dplyr 0.8.3\r## v tidyr 1.0.0 v stringr 1.4.0\r## v readr 1.3.1 v forcats 0.4.0\r## -- Conflicts --------------------------------------------------------------------------------------------------------------- tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\r#convert a df or matrix to a tibble using as_tibble()\rdf \u0026lt;- data.frame(\u0026quot;var_1\u0026quot; = c(sample(1:6, 6)), \u0026quot;var_2\u0026quot; = rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), 2)) tbl1 \u0026lt;- as_tibble(df)\rtbl1 #printout also tells you the dimensions and class of each column\r## # A tibble: 6 x 2\r## var_1 var_2\r## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt;\r## 1 4 a ## 2 6 b ## 3 5 c ## 4 1 a ## 5 3 b ## 6 2 c\r#When creating a tibble, strings are not automatically converted to factors. #This is better from a data manipulation standpoint, which will be covered in a\r#future post on working with strings\rtbl1 \u0026lt;- tibble(\u0026quot;var_1\u0026quot; = c(sample(1:6, 6)), \u0026quot;var_2\u0026quot; = rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), 2)) tbl1\r## # A tibble: 6 x 2\r## var_1 var_2\r## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt;\r## 1 6 a ## 2 4 b ## 3 3 c ## 4 1 a ## 5 5 b ## 6 2 c\r\r\r8 Random Numbers and Sampling\r#sampling\r#obtain a random sample 6 numbers with values ranging between 1 and 40 without replacement\rsample(1:40, 6, replace=F) \r## [1] 25 21 39 2 8 30\rx \u0026lt;- c(1:30)\rx\r## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\r## [26] 26 27 28 29 30\rv \u0026lt;- sample(1:200, 100, replace=T) #sample with replacement, save it in a vector called \u0026quot;v\u0026quot;\rv\r## [1] 27 121 67 34 68 114 68 6 60 12 77 198 143 42 40 61 138 88\r## [19] 107 164 186 11 195 32 74 136 149 182 51 101 32 96 194 139 104 166\r## [37] 137 17 70 196 121 50 95 152 92 51 128 199 19 97 177 33 107 2\r## [55] 180 154 115 49 173 135 98 160 133 149 157 113 79 105 128 15 171 153\r## [73] 8 99 30 89 95 84 197 98 193 106 198 76 17 45 14 12 1 88\r## [91] 9 149 37 125 130 83 31 133 95 59\rset.seed(seed = 934) #sets criteria for random sampling for variable creation if you want it to be repeatable\rrandom.sample \u0026lt;- rnorm(1000, mean = 100, sd = 1) #create a dataset of random, normally distributed data\r#generating sequences\rseq(from = 1, to = 7, by = 1) #generate a sequence of numbers from 1 to 7, in 1 unit increments.\r## [1] 1 2 3 4 5 6 7\rseq(1, 7, 1) #specify arguments by position instead\r## [1] 1 2 3 4 5 6 7\rrep(1:10, each = 2)\r## [1] 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 10\rx \u0026lt;- c(1:12) rep(x, each = 2) #create a numeric vector containing each element of x repeated twice\r## [1] 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 10 11 11 12 12\rrep(seq(1, 7, 1), each = 3) #repeat a sequence\r## [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 6 6 6 7 7 7\rx \u0026lt;- seq(1, 7, 1)\rrep(x, each = 3) #equivalent to the nested version\r## [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 6 6 6 7 7 7\r#creating a data frame from scratch\ry \u0026lt;- c(rnorm(n = 60, mean = 100, sd = 20), rnorm(n = 10, mean = 110, sd = 20)) #creates variable y composed of 60 random scores from a normal distribution with a mean of 100 and sd of 20, along with 10 random scores from a normal distribution with a mean of 110 and sd of 20\rg \u0026lt;- factor(rep(seq(1, 7, 1), each = 10), labels = \u0026quot;g\u0026quot;, ordered = FALSE) #groups the scores from \u0026#39;y\u0026#39; into 7 sets (g1,g2,etc) containing 10 scores each\rz \u0026lt;- letters[1:5]\rdf \u0026lt;- as.data.frame(cbind(y, g, z))\rclass(df)\r## [1] \u0026quot;data.frame\u0026quot;\rclass(df$y)\r## [1] \u0026quot;factor\u0026quot;\rdf$y \u0026lt;- as.numeric(df$y) #convert to numeric\rclass(df$z) #check the class of variable \u0026quot;Z\u0026quot;\r## [1] \u0026quot;factor\u0026quot;\rclass(df$g) #check the class of variable \u0026quot;g\u0026quot;\r## [1] \u0026quot;factor\u0026quot;\r\r9 Functions for Describing the Structural Information of Data Objects\rlength(c(1:100)) #number of elements in a vector = the length of the vector\r## [1] 100\rdata \u0026lt;- mtcars #we\u0026#39;ll use the built-in mtcars dataframe as an example again\rlength(data) #when used on a matrix or data frame, returns the number of columns\r## [1] 11\rnrow(data) #number of rows\r## [1] 32\rncol(data) #number of columns\r## [1] 11\rdim(data) #returns the number of rows and columns of the object\r## [1] 32 11\runique(data$cyl) #display the unique values of the specified variable\r## [1] 6 4 8\r#useful applications of the unique() function include making it easier to\r#construct factors (since you need to know what the unique values are) and\r#making it easier to identify data entry errors (to be demonstrated in a future\r#post)\rlevels(as.factor(data$cyl)) #this is the same as unique but for factors. \r## [1] \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\r#It provides the added benefit of revealing the order of factor levels.\r#show the unique values of a vector (top line) #as well as the count of each (bottom line)\rtable(data$cyl) \r## ## 4 6 8 ## 11 7 14\r#of course the str() function, which we\u0026#39;ve already covered is incredibly\r#versatile and informative\rstr(data$cyl) #the structure of a variable within a dataframe\r## num [1:32] 6 6 4 6 8 6 8 4 4 6 ...\rstr(data) #the structure of a whole dataframe\r## \u0026#39;data.frame\u0026#39;: 32 obs. of 11 variables:\r## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\r## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ...\r## $ disp: num 160 160 108 258 360 ...\r## $ hp : num 110 110 93 110 175 105 245 62 95 123 ...\r## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\r## $ wt : num 2.62 2.88 2.32 3.21 3.44 ...\r## $ qsec: num 16.5 17 18.6 19.4 17 ...\r## $ vs : num 0 0 1 1 0 1 0 1 1 1 ...\r## $ am : num 1 1 1 0 0 0 0 0 0 0 ...\r## $ gear: num 4 4 4 3 3 3 3 4 4 4 ...\r## $ carb: num 4 4 1 1 2 1 4 2 2 4 ...\rstr(table) #the structure of a function\r## function (..., exclude = if (useNA == \u0026quot;no\u0026quot;) c(NA, NaN), useNA = c(\u0026quot;no\u0026quot;, ## \u0026quot;ifany\u0026quot;, \u0026quot;always\u0026quot;), dnn = list.names(...), deparse.level = 1)\r#the tidyverse alternative to str is the glimpse function(), which is\r#specialized for displaying the structural info of dataframes and tibbles,\r#providing a slightly nicer printout, and enabling you to peak at the structure\r#of the data in the middle of a series of \u0026quot;piped\u0026quot; or \u0026quot;chained\u0026quot; operations\r#without interrupting the sequence (more on this in the next post).\rlibrary(tidyverse)\rdf \u0026lt;- data.frame(\u0026quot;var_1\u0026quot; = c(sample(1:6, 6)), \u0026quot;var_2\u0026quot; = rep(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;), 2)) glimpse(df)\r## Observations: 6\r## Variables: 2\r## $ var_1 \u0026lt;int\u0026gt; 3, 5, 4, 1, 2, 6\r## $ var_2 \u0026lt;fct\u0026gt; a, b, c, a, b, c\r\r10 The Global Environment\rThe functions you use in R operate within what is called the global environment, which consists of the functions and other objects that you have loaded in the current R session (this is what happens you load a package with the library function), as well as any variables, data objects, or functions you have created during the session.\n#to get a list of the objects in your global environment, use the ls() function\rls()\r## [1] \u0026quot;c_vect\u0026quot; \u0026quot;data\u0026quot; \u0026quot;df\u0026quot; \u0026quot;g\u0026quot; ## [5] \u0026quot;l\u0026quot; \u0026quot;mat1\u0026quot; \u0026quot;mat2\u0026quot; \u0026quot;random.sample\u0026quot;\r## [9] \u0026quot;tbl1\u0026quot; \u0026quot;v\u0026quot; \u0026quot;x\u0026quot; \u0026quot;x1\u0026quot; ## [13] \u0026quot;x2\u0026quot; \u0026quot;x3\u0026quot; \u0026quot;y\u0026quot; \u0026quot;z\u0026quot;\r\r11 The Working Directory\rR is always connected to a specific folder on your computer called the working directory, which is the default path for loading/importing files or saving/exporting them\n#to view your current working directory, use the getwd() function, or you can\r#click on the \u0026quot;files\u0026quot; tab in the bottom right pane of R studio.\rgetwd()\r#you can change your working directory using the setwd() function, #or you can use the \u0026quot;set working directory\u0026#39; menu under the \u0026quot;Session\u0026quot; drop down\r#menu along the top of the R studio window, #or you can just use the keyboard shortcut Ctrl + Shift + H\rsetwd(\u0026quot;C:/Users/CPH/Documents/\u0026quot;)\r\r12 Projects\rThe projects feature of R studio makes it much easier to keep your work organized, and using it is strongly recommended if you are working on anything that will take longer than one or two sessions to complete.\nYou can create/start a project using the projects menu by clicking on this button:\nYou can find it in the top right corner of R studio, directly below the minimize/maximize/exit buttons. Creating (or loading) a package also sets the working directory to the project folder automatically.\n\r13 Useful Keyboard Shortcuts (for R studio users)\rOne of the great benefits of using R studio are the keyboard shortcuts that speed up the coding process. Here are some I’ve found to be useful:\n\rassignment operator \u0026lt;-: [Alt] + [-]\n\rextract variable: [Ctrl] + [Alt] + [v]. Highlight some code, use this shortcut, and enter the variable name\n\rcomment lines in/out: [Ctrl] + [Shift] + [c]\n\rreflow comments: highlight/select comments and use [Ctrl] + [Shift] + [/] to reflow/wrap them for easier reading.\n\rreindent lines: use [Ctrl] + [i] to realign the indentation of your R code so it is easier to read.\n\rinsert code section title: [Ctrl] + [Shift] + [r]. This can also be done by starting a line with # to comment it out, and ending it with #### or —-\n\ropen/close the R script outline, which contains a list of the code sections you’ve defined using the code section titles that can be clicked on to quickly navigate through your script: [Ctrl] + [Shift] + [o]\n\rview the definition of a function: press [F2] when the cursor is on a function name\n\r\rN.B. To see all available keyboard shortcuts use [Alt] + [Shift] + [K] or click “keyboard shortcuts help” under the R studio help menu. Many of the more useful ones are also accessible under the R studio code menu.\n13.1 Navigation\rClick here to go back to the previous post or here to go to the next one.\n\r13.2 Notes\r\rFor more details on the data structures and operations introduced in this post, you may find the official R introductory manual or Michael Crawley’s “The R Book” helpful.\r\rThank you for visiting my blog. I welcome any suggestions for future posts, comments or other feedback you might have. Feedback from beginners and science students/trainees (or with them in mind) is especially helpful in the interest of making this guide even better for them.\n\r\r","date":1565049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565049600,"objectID":"20378f5da6053db32dee0638dd159aa7","permalink":"/post/2019-08-06-asgr-2-0-basic-operations-and-data-structures/","publishdate":"2019-08-06T00:00:00Z","relpermalink":"/post/2019-08-06-asgr-2-0-basic-operations-and-data-structures/","section":"post","summary":"1 TL;DR\r2 Introduction\r3 Basic Calculations\r4 Logical Operators\r5 Object Assignment\r6 Basic Summary Statistics\r7 Data Structures and Object Assignment\r7.1 Numeric and Character Vectors\r7.2 Logical Vectors\r7.3 Factors\r7.4 Matrices\r7.5 Dataframes\r7.6 Tibbles\r\r8 Random Numbers and Sampling\r9 Functions for Describing the Structural Information of Data Objects\r10 The Global Environment\r11 The Working Directory\r12 Projects\r13 Useful Keyboard Shortcuts (for R studio users)\r13.","tags":["R","R basics"],"title":"A Scientist's Guide to R: Step 2.0. Basic Operations \u0026 Data Structures","type":"post"},{"authors":["Bettio, L.","Thacker, J.","Hutton, C.P.","Christie, B.R."],"categories":null,"content":"","date":1565049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565049600,"objectID":"e3ecc74756a08d2fd2225d11b799e379","permalink":"/publication/bettio-et-al-2019/","publishdate":"2019-08-06T00:00:00Z","relpermalink":"/publication/bettio-et-al-2019/","section":"publication","summary":"Synaptic plasticity is an experience-dependent process that results in long-lasting changes in synaptic communication. This phenomenon stimulates structural, molecular, and genetic changes in the brain and is the leading biological model for learning and memory processes. Synapses are able to show persistent increases in synaptic strength, or long-term potentiation (LTP), as well as persistent decreases in synaptic strength, known as long-term depression (LTD). Understanding the complex interactions that regulate these activity-dependent processes can provide insight for the development of strategies to improve cognitive function. Twenty years ago, we provided the first evidence indicating that aerobic exercise can reliably enhance LTP, and went on to show that it can also regulate some of the mechanisms involved in LTD induction. Since then, several laboratories have confirmed and expanded these findings, helping to identify different molecular mechanisms involved in exercise-mediated changes in synaptic efficacy. This chapter reviews this material and shows how these experimental findings may prove valuable for alleviating the burden of neurodegenerative diseases in an aging population.","tags":["exercise","plasticity","neuroscience"],"title":"Modulation of synaptic plasticity by exercise","type":"publication"},{"authors":["Christie, B.R.","Trivino-Paredes, J.","Pinar, C.","Neale, K.J.","Meconi, A.","Reid, H.","Hutton, C.P."],"categories":null,"content":"","date":1563494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563494400,"objectID":"fe855db811d2c9747eb0c5d7b46fdd27","permalink":"/publication/christie-et-al-2019/","publishdate":"2019-07-19T00:00:00Z","relpermalink":"/publication/christie-et-al-2019/","section":"publication","summary":"Preclinical models for mild traumatic brain injury (mTBI) need to recapitulate several essential clinical features associated with mTBI, including a lack of significant neuropathology and the onset of neurocognitive symptoms normally associated with mTBI. Here we show how to establish a protocol for reliably and repeatedly inducing a mild awake closed head injury (ACHI) in rats, with no mortality or clinical indications of persistent pain. Moreover, we implement a new rapid neurological assessment protocol (NAP) that can be completely conducted within 1 min of each impact. This ACHI model will help to rectify the paucity of data on how repeated mTBI (r‐mTBI) impacts the juvenile brain, an area of significant concern in clinical populations where there is evidence that behavioral sequelae following injury can be more persistent in juveniles. In addition, the ACHI model can help determine if r‐mTBI early in life can predispose the brain to exhibiting greater neuropathology (i.e., chronic traumatic encephalopathy) later in life and can facilitate the identification of critical periods of vulnerability to r‐mTBI across the lifespan. This article describes the protocol for administering an awake closed head mTBI (i.e., ACHI) to rats, as well as how to perform a rapid NAP following each ACHI. Methods for administering the ACHI to individual subjects repeatedly are described, as are the methods and scoring system for the NAP. The goal of this article is to provide a standardized set of procedures allowing the ACHI and NAP protocols to be used reliably by different laboratories.","tags":["behaviour","concussion","mild traumatic brain injury","head trauma"],"title":"A rapid neurological assessment protocol for repeated mild traumatic brain injury in awake rats","type":"publication"},{"authors":null,"categories":["R","Reproducible Research"],"content":"\r\r1 TL;DR\r2 Introduction\r3 Installing and Loading R Packages\r4 How to Get Your Data Into R\r4.1 Comma Delimited (.csv) Files\r4.2 Tab Delimited (.txt) Files\r4.3 Files With Other Delimiting Characters (also .txt)\r4.4 Microsoft Excel Files (.xlsx, .xls)\r4.5 Files from SPSS, SAS, or Stata\r4.6 Fixed Width Files (.txt, .gz, .bz2, .xz, .zip, etc.)\r4.7 html/xml files\r4.8 JavaScript Object Notation (JSON) Files\r4.9 Navigation\r4.10 Notes\r\r\r\r1 TL;DR\rAs the second post in the Scientist’s Guide to R series (click here for the 1st post), this post will goes into the first step of the data analysis with R process: importing your data from a variety of common “flat file” sources (.csv, .txt, .xlsx, etc).\n\r2 Introduction\rSo you’ve spent weeks/months/years conducting a carefully planned research project and collected a bunch of data, and want to know how the study turned out using the advanced functionality of R? The first things you will need to do are:\n\ri) Install R \u0026amp; R studio. Install R using this link. The R studio integrated development environment (IDE) is available here (R studio is not necessary to use R but it is strongly recommended). For instructions on using R studio, see these videos.\n\rii) install and load R add-on packages.\n\riii) import your data.\n\r\rAfter you’ve installed R studio, read on for a brief explanation of how to install and load R packages. These additional packages greatly expand the functionality of R (e.g. allow you to build webpages like this blog using the blogdown package), making it much easier to learn and use. While there are many different ways to do things using R, this guide will cover the ways I have found in practice to have the right balance of effectiveness, efficiency, and accessibility/transparency. There are many excellent books and online resources for those wanting to learn more about alternative methods. However, to avoid confusing those attempting to learn data analysis with R for the 1st time, the scope of these tutorials will be limited to recommended methods only.\nAfter explaining how to install and load R packages, this post will cover how to easily import data from a variety of common sources ranging from comma-separated-variable (.csv) files, to excel spreadsheets, and fixed width files.\n\r3 Installing and Loading R Packages\rInstalling \u0026amp; loading packages into R is a very straightforward process. For packages hosted on CRAN (which is where the “official” versions of most R packages reside), simply run the following:\n#To include comments in R (like this), begin the comment with \u0026quot;#\u0026quot;, which will tell R\r#to treat the following info as a comment and not try to run it as live R code.\r#installation command\rinstall.packages(\u0026quot;package_name\u0026quot;) #replace \u0026quot;package_name\u0026quot; with the name of the package # you want to install\r#load the package when you want to use it using the base R library() function\rlibrary(package_name) #replace \u0026quot;package_name\u0026quot; with the name of the package #you\u0026#39;ve installed and want to use\r# for example, some of the functions I recommend below for importing data use\r# the readr package, which is part of the tidyverse suite of pacakges:\rinstall.packages(\u0026quot;readr\u0026quot;) #replace \u0026quot;package_name\u0026quot; with the name of the package library(readr)\r#or if you want all of the tidyverse packages (which we will be using throughout this guide)\rinstall.packages(\u0026quot;tidyverse\u0026quot;)\rlibrary(tidyverse) #loads readr along with other helpful packages, like ggplot2 for graphing.\rFor packages hosted on GitHub, which are typically in development, the process is slightly more complicated but still requires only couple of lines of code:\n#1. first install and load the devtools package\r#install devtools from CRAN\rinstall.packages(\u0026quot;devtools\u0026quot;)\r#load the devtools package\rlibrary(devtools)\r#2. install the package of interest from its github repository using # the install_github function()\rinstall_github(\u0026quot;repository_name/package_name\u0026quot;)\r# For example, to get the development version of the tidyr package, # which contains excellent new functions for converting between the long and wide # forms of data (there will be a later post on this), you would run:\rinstall_github(\u0026quot;tidyverse/tidyr\u0026quot;) #contains more flexible # pivoting functions than the CRAN version of tidyr.\r# Packages installed from GitHub can then also be loaded using the library() function # and the package name only:\rlibrary(tidyr)\r#N.B. If you are using R studio, you can also install packages from CRAN # by clicking on the packages tab of the lower right panel and clicking the \u0026quot;Install\u0026quot; button\r\r4 How to Get Your Data Into R\rGetting your data into R is also easy. Commands and packages specific to different data sources are listed below:\n4.1 Comma Delimited (.csv) Files\rCSV files are the most common type of data file you’ll encounter as a data analyst. These can be imported using the read_csv() function from the readr package.\n#using the readr package:\rlibrary(readr)\rread_csv(\u0026quot;directory_path/filename.csv\u0026quot;) #e.g. C:/Users/your_name/Desktop/documents/data.csv\r# If you want to use the data after importing it, assign it to be stored in an object using # the assignment operator: \u0026quot;\u0026lt;-\u0026quot;\rdata \u0026lt;- read_csv(\u0026quot;directory_path/filename.csv\u0026quot;)\r# N.B. If you are using R studio, you can use the keyboard shortcut # [\u0026quot;alt\u0026quot; + \u0026quot;-\u0026quot;] to insert \u0026quot;\u0026lt;-\u0026quot; # you can then examine the data by simply running the assigned name\rdata\r# or clicking on it in the \u0026quot;environment\u0026quot; window in R studio (by default it is # in the top right panel)\r\r4.2 Tab Delimited (.txt) Files\rlibrary(readr)\rdata \u0026lt;- read_tsv(\u0026quot;directory_path/filename.txt\u0026quot;)\r#yes, it is that easy ;)\r\r4.3 Files With Other Delimiting Characters (also .txt)\rThe read_csv() and read_tsv() functions are actually convienience wrappers for the workhorse function of the readr package, read_delim(), which can be used to read in data files with any delimiting character (e.g. “-”, “$”, etc.). This is only slightly more complicated, in that you have to specify what the delimiting character is using the delim argument.\nlibrary(readr)\rdata \u0026lt;- read_delim(\u0026quot;directory_path/filename.txt\u0026quot;, delim = \u0026quot;|\u0026quot;) # specify the file path/name.txt and the delimiting character, # whatever it happens to be for your data file\r\r4.4 Microsoft Excel Files (.xlsx, .xls)\rExcel files are fairly easy to import using the readxl package. Since these files may contain several speadsheets, you also have to specify the sheet that you want to use… unless it is the first one, which is assumed by default.\nlibrary(readxl)\r# If you want the first sheet then you can just specify the path\rdata \u0026lt;- read_excel(path = \u0026quot;path/file.xlsx\u0026quot;)\r# If you want a different sheet then you have to specify which one using the sheet argument\rdata \u0026lt;- read_excel(path = \u0026quot;path/file.xlsx\u0026quot;,\rsheet = \u0026quot;sheet_name\u0026quot;)\r# Using the excel_sheets helper function makes things easier by showing you what the names # of each sheet are in the excel file.\rexcel_sheets(\u0026quot;path/file.xlsx\u0026quot;)\r# You can also specify the sheet using an integer\rdata \u0026lt;- read_excel(path = \u0026quot;path/file.xlsx\u0026quot;,\rsheet = 2) #to get the 2nd sheet\r\r4.5 Files from SPSS, SAS, or Stata\rIf you’ve recently switched from working in SPSS, SAS, or Stata (or are collaborating with someone who uses these programs), you can also easily import your existing SPSS or SAS data files directly into R using the haven package.\nlibrary(haven)\r#SAS files\rdata \u0026lt;- read_sas(\u0026quot;file_name.sas7bdat\u0026quot;) #if the file is in your working directory\r#otherwise specify the file path as well, e.g. \u0026quot;path/file_name.sas7bdat\u0026quot;\r#SPSS files\rdata \u0026lt;- read_sav(\u0026quot;file_name.sav\u0026quot;)\r#Stata files\rdata \u0026lt;- read_dta(\u0026quot;file_name.dta\u0026quot;)\r\r4.6 Fixed Width Files (.txt, .gz, .bz2, .xz, .zip, etc.)\rFixed width files are a less common type of data source where the values are separated not by tabs or specific characters but a set amount of white/empty space other than a tab. These can be loaded using the read_fwf() function in readr.\nlibrary(readr)\r# For these fixed width files, you can specify column positions (i.e. breaks between values)\r# in multiple ways. See https://readr.tidyverse.org/reference/read_fwf.html, # which I used to obtain templates of the commands below, for further details.\r# The main arguments you need to include are file = the file path/name.extension,\r# col_positions, and col_names().\r# the col_positions argument is different from the others we\u0026#39;ve see so far (e.g. delim) # in that it is primarily intended to be used by specifying a call to another function, # such as fwf_empty (see below), which tells read_fwf() how you want to specify the columns.\r# some options are:\r# 1. Guess based on position of empty columns using the fwf_empty helper function # as an argument(easiest):\rfile_path \u0026lt;- c(\u0026quot;directory_path/filename.txt\u0026quot;) #store the file path/name as \u0026quot;file_path\u0026quot;\rdata \u0026lt;- read_fwf(file = file_path,\rcol_positions = fwf_empty(file_path, col_names = c(\u0026quot;col_1\u0026quot;, \u0026quot;col_2\u0026quot;, \u0026quot;col_3\u0026quot;)) #replace the above labels with column names you want to use.\r#note that col_names is an argument supplied to the fwf_empty helper function\r#which is then passed along to the read_fwf function.\r# 2. Using a vector of field widths (a bit more tedious) with # the fwf_widths() helper function:\rdata \u0026lt;- read_fwf(file = file_path,\rcol_positions = fwf_widths(c(10, 7, 19, 290),\r#replace the above with values appropriate to your data col_names = c(\u0026quot;col_1\u0026quot;, \u0026quot;col_2\u0026quot;, \u0026quot;col_3\u0026quot;))\r# 3. Specifying starting and ending positions of the columns with paired vectors\r# using the fwf_positions() helper function\rread_fwf(file = file_path, fwf_positions(start = c(1, 50), #which positions do the columns start at\rend = c(25, 75), #which positions do the columns end at\rcol_names = c(\u0026quot;col_1\u0026quot;, \u0026quot;col_2\u0026quot;))) #list of column names\r# 4. Using named arguments for starting and ending positions with # the fwf_cols() helper function:\rread_fwf(file = file_path, col_positions = fwf_cols(name = c(1, 10), birthdate = c(20, 50),\rage = c(60, 63)))\r# 5. Via named arguments and column widths, also using the fwf_cols helper function:\rread_fwf(file = file_path,\rfwf_cols(subject_id = 15, #e.g. the subject_id column contains data in the 1st 15 characters of each row\rtest_score = 20))\r\r4.7 html/xml files\rThose who need to scrape data from the web for their research can use the rvest and XML packages. Since this is not what most experimental scientists (at least among the many I’ve had the pleasure of meeting) will need to do, I have only provided the necessary functions below without a detailed explanation. See this blog post for further details.\n#HTML pages\rlibrary(rvest)\r#store the url\rurl \u0026lt;- c(\u0026quot;http://www.webpagename.com\u0026quot;)\r#read in the html data\rweb_page \u0026lt;- read_html(url)\r#extract the html info for the target table\rweb_page_table \u0026lt;- html_nodes(web_page, \u0026quot;table\u0026quot;)\r#parse the html table info into a data set that you can use in R data \u0026lt;- html_table(web_page_table, fill = TRUE)\r#XML\rlibrary(XML)\r#read in the html data\rdata \u0026lt;- readHTMLTable(url)\r\r4.8 JavaScript Object Notation (JSON) Files\rJSON files are way of storing JavaScript objects as text that can easily be imported in to R using the fromJSON() function from the jsonlite package. Science trainees are unlikely to encounter this type of data unless they are working with web sources, e.g. twitter. See this link for an accessible overview.\nlibrary(jsonlite)\rr_bloggers_data \u0026lt;- fromJSON(\u0026quot;~/Desktop/r-bloggers.json\u0026quot;)\r\r4.9 Navigation\rClick here to go back to the previous post or here for the next post (on basic operations and data structures) in R.\n\r4.10 Notes\r\rExporting data is also easy and will be covered in a later post. E.g. for readr functions this is usually done via the appropriately named write_ functions, like write_csv().\n\rBy default, the above readr functions will assume that the first row of your data file contains the variable names. For these functions, you can disable this by setting the col_names argument to FALSE, or you can provide a string/character vector of names to use instead by setting col_names to the name of the string vector (see the next post for more on data vectors).\n\rArguments to a function can either be specified by name (which I have done here for the sake of transparency) or by position to save some typing. E.g. read_fwf(file_path, fwf_cols(subject_id = 15, test_score = 20)) is equivalent to the read_fwf syntax I provided above with argument names specified. There will be more on this in the next section of the blog.\n\rAn R package only needs to be loaded once in an R session for the functions it contains to be available for use throughout the same session.\n\rThere are also ways to import data from relational databases using SQL syntax. However, these methods go beyond the scope of this blog series since they are not relevant to most academic researchers and may be covered in the distant future if/when I write a second volume on R for machine learning and data science. An introduction to importing data from these sources can be found here. Those who are interested in starting to learn SQL (e.g. for work) would benefit from the free Introduction to SQL for Data Science course provided by datacamp.com, which is where I started learning how to use SQL.\n\rUsers working with large flat files (\u0026gt; 100 MB) may want to consider the vroom function in the vroom package or fread function in the data.table package instead of readr to import their data. You could use these for flat files of any size, but the speed difference probably won’t really be noticable for smaller files.\n\r\rThank you for reading and I welcome any suggestions for future posts, comments or other feedback you might have. Feedback from beginners and science students/trainees (or with them in mind) is especially helpful in the interest of making this guide even better for them.\n\r\r","date":1561161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561161600,"objectID":"514b4c5f6244a81280540a1bbe95b075","permalink":"/post/2019-06-22-asgr-1-getting-data-into-r/","publishdate":"2019-06-22T00:00:00Z","relpermalink":"/post/2019-06-22-asgr-1-getting-data-into-r/","section":"post","summary":"1 TL;DR\r2 Introduction\r3 Installing and Loading R Packages\r4 How to Get Your Data Into R\r4.1 Comma Delimited (.csv) Files\r4.2 Tab Delimited (.txt) Files\r4.3 Files With Other Delimiting Characters (also .txt)\r4.4 Microsoft Excel Files (.xlsx, .xls)\r4.5 Files from SPSS, SAS, or Stata\r4.6 Fixed Width Files (.txt, .gz, .bz2, .xz, .zip, etc.)\r4.7 html/xml files\r4.8 JavaScript Object Notation (JSON) Files\r4.","tags":["R","R Basics","Data Analysis","Importing Data"],"title":"A Scientist's Guide to R: Step 1. Getting Data into R","type":"post"},{"authors":null,"categories":["R","Data Analysis","Statistics","Reproducible Research"],"content":"Introduction This tutorial will be the first of many blog posts for new researchers and science program students/trainees on how to use R as an analytical and productivity tool in the process of conducting scientific research. Specifically, in this post I will explain why you might want to use R and provide a brief overview of the basic R workflow used in the analysis of experimentally obtained data.\nAs someone who has spent many years analyzing data using a variety of technologies (e.g. SPSS, Excel, R), I know how intimidating and frustrating it can be to start learning R for the first time instead of using the simple point-and-click graphical user interfaces (GUIs) provided by commercial (i.e. paid) programs like SPSS. These programs work fine for implementing many simple and common analytical methods (e.g. t-tests, one-way ANOVAs to compare 2+ groups to one another on a single dependent measure) but as your research questions and data structures become more complicated (e.g. predictive modelling, working with non-normally distributed data), you will quickly find the functionality of these GUI-based programs to be rather limited. For me, this realization occured as a new graduate student at McMaster when I wanted to test a complex experimental hypothesis to see if there was a difference between one experimental group and several other groups (simultaneously, thereby avoiding the need to correct p-values for multiple comparisons) using a linear contrast within a factorial ANOVA (the subject of a future blog post). You also have to pay to use commercial statistical packages, which can be a significant barrier for low-income students or even reseach labs struggling to maintain operating funds (which is most labs in Canada given the ~15% current funding rate).\nIn contrast, R is constantly undergoing development by a large community of statisticians, scientific researchers, and other data science professionals, offering a way to use pretty much any analytical tool out there. Moreover, as long as researchers and statisticians continue preferring to develop novel analytical methods in R, it will continue to be the most advanced statistical software available. However, the fact that so many people are developing R also means that for any given analytical method of interest there will be multiple different ways of using it in R, each potentially requiring very different syntax. Thus, knowing which of the many options to use can be a huge challenge for new useRs. As someone with experience training researchers to use R, and who has spent countless hours exploring the many options available to do different things in R, a major purpose of this blog will be to demonstrate how to use R for data analysis and boost your research productivity using methods that I have found to work best (from a scientific research perspective) in practice.\nOnce you know what you are doing (e.g. from reading these posts), R can arguably be even easier to use than commercial programs. This is clearly demonstrated by the fact that one can accomplish a great deal with only a few lines of code, e.g. a simple linear regression requires only a single line of code (to be covered in more detail in a future post):\nsummary(lm(Y ~ X, data = your_dataset)) With this line you ask R for a (regression) summary of a linear model (lm) where Y is a continuous outcome variable, X is a predictor, X and Y are contained in the dataframe \u0026ldquo;your_dataset\u0026rdquo;, and \u0026ldquo;~\u0026rdquo; means \u0026ldquo;as a function of\u0026rdquo;. You would have to click through seveal menus to obtain the same results in a GUI-based program.\nPerhaps the greatest benefit of using R is that once you have written a script to conduct an analysis, you can reproduce that analysis with new data simply by re-running the script (updating variable names as needed). When combined with R Markdown, you can generate reproducible reports which make collaboration and communication of your results much easier. Think of just re-running a script a collaborator has written in R vs. following a set of step-by-step instructions to reproduce an analysis done via SPSS or Excel. The former usually takes less than a minute, while the latter could take hours and depends on how good your colleague's instructions are. Since R is a fully functional programming language, things get even better when you start automating repetitive processess using loops.\nAnother major advantage of using R is of course that it is free.\nWorkflow Outline Now that we know why you should use R, what are the main steps of analyzing data? My typical analytical workflow follows these 7 steps:\n 0.Install and load necessary packages (e.g. tidyverse) 1. Import the data. 2. Clean/Transform the data. This part usually takes longer than the analysis, but may not be necessary if you've entered the data yourself. 3. Explore the data using descriptive summary statistics and visualizations. 4. Resolve structural issues. If any issues are detected in step 3, go back to step 2 and resolve them, restructuring data as needed. 5. Model: Fit an appropriate statistical model \u0026amp; check model assumptions (e.g. normality). Use a different model/test if assumptions are violated (e.g. a permutation test). Conduct follow-up or post-hoc tests if desired. 6. Analyze and Interpret the results using appropriate graphics (e.g. tables/figures) and text summaries. These can easily be used as a foundation for building presentation slides or the results section of a manuscript. 7. Communicate the results to the target audience (e.g. other scientists) with a description of the methods used, relevant background/introduction and insightful discussion in a coherent and accessible report.  This represents the basic steps involved in the majority of data analysis tasks that one typically encounters in the social and behavioural sciences. As you might expect, for machine learning projects there are a few extra steps involved.\nEach of these sections will be covered in detail in future posts. Thanks for reading and I welcome any suggestions for future posts, comments or other feedback you might have.\nNavigation Click here to continue to the next post on installing R, R packages, and importing data.\nNotes  This post has been peer-reviewed and approved by the University of Victoria Data Science Studio   The workflow I recommend above is very similar to the well-known data science workflow advocated by Grolemund and Wickham in their R for data science book. However, an important difference is that I recommend that most/all of the exploration and wrangling of the data be completed prior to advancing to the modelling stage. The idea is to sort out the structure of your final analytical dataset first and fit fewer models to save time.  ","date":1558051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558051200,"objectID":"2cd0a4d9015b48e04820425cf8875786","permalink":"/post/2019-05-17-asgr-basic-workflow/","publishdate":"2019-05-17T00:00:00Z","relpermalink":"/post/2019-05-17-asgr-basic-workflow/","section":"post","summary":"Introduction This tutorial will be the first of many blog posts for new researchers and science program students/trainees on how to use R as an analytical and productivity tool in the process of conducting scientific research. Specifically, in this post I will explain why you might want to use R and provide a brief overview of the basic R workflow used in the analysis of experimentally obtained data.\nAs someone who has spent many years analyzing data using a variety of technologies (e.","tags":["R","R Basics","Data Analysis","Statistics"],"title":"A Scientist's Guide to R: Introduction and Basic Workflow","type":"post"},{"authors":[],"categories":[],"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34;\rif porridge == \u0026#34;blueberry\u0026#34;:\rprint(\u0026#34;Eating...\u0026#34;)\r Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\rPress Space to play!\nOne Two Three  A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\rPress the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view \r  Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}}\r{{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}\r Custom CSS Example Let's make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Hutton, C.P.","Lemon, J.A.","Sakic, B.","Rollo, C.D.","Boreham, D.R.","Fahnestock, M.","Wojtowicz, J.M.","Becker, S."],"categories":null,"content":"","date":1525824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525824000,"objectID":"f44290383f29de3d76d364a347b2d667","permalink":"/publication/hutton-et-al-2018/","publishdate":"2018-05-09T00:00:00Z","relpermalink":"/publication/hutton-et-al-2018/","section":"publication","summary":"The increasing global burden of Alzheimer's disease (AD) and failure of conventional treatments to stop neurodegeneration necessitates an alternative approach. Evidence of inflammation, mitochondrial dysfunction, and oxidative stress prior to the accumulation of amyloid-β in the prodromal stage of AD (mild cognitive impairment; MCI) suggests that early interventions which counteract these features, such as dietary supplements, may ameliorate the onset of MCI-like behavioral symptoms. We administered a polyphenol-containing multiple ingredient dietary supplement (MDS), or vehicle, to both sexes of triple transgenic (3xTg-AD) mice and wildtype mice for 2 months from 2-4 months of age. We hypothesized that the MDS would preserve spatial learning, which is known to be impaired in untreated 3xTg-AD mice by 4 months of age. Behavioral phenotyping of animals was done at 1-2 and 3-4 months of age using a comprehensive battery of tests. As previously reported in males, both sexes of 3xTg-AD mice exhibited increased anxiety-like behavior at 1-2 months of age, prior to deficits in learning and memory, which did not appear until 3-4 months of age. The MDS did not reduce this anxiety or prevent impairments in novel object recognition (both sexes) or on the water maze probe trial (females only). Strikingly, the MDS specifically prevented 3xTg-AD mice (both sexes) from developing impairments (exhibited by untreated 3xTg-AD controls) in working memory and spatial learning. The MDS also increased sucrose preference, an indicator of hedonic tone. These data show that the MDS can prevent some, but not all, psychopathology in an AD model.","tags":["behaviour","Alzheimer's disease","mild cognitive impairment","anhedonia","anxiety","dietary supplements","learning","memory"],"title":"Early intervention with a multi-ingredient dietary supplement improves mood and spatial memory in a triple transgenic mouse model of Alzheimer's disease","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Hutton, C.P.","Déry, N.","Rosa, R.","Lemon, J.A.","Rollo, C.D.","Boreham, D.R.","Fahnestock, M.","deCatanzaro, D.","Wojtowicz, J.M.","Becker, S."],"categories":null,"content":"","date":1447286400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1447286400,"objectID":"92e5074edcfa032a3cfd9352980ccb3f","permalink":"/publication/hutton-et-al-2015/","publishdate":"2015-11-12T00:00:00Z","relpermalink":"/publication/hutton-et-al-2015/","section":"publication","summary":"Severe chronic stress can have a profoundly negative impact on the brain, affecting plasticity, neurogenesis, memory and mood. On the other hand, there are factors that upregulate neurogenesis, which include dietary antioxidants and physical activity. These factors are associated with biochemical processes that are also altered in age-related cognitive decline and dementia, such as neurotrophin expression, oxidative stress and inflammation. We exposed mice to an unpredictable series of stressors or left them undisturbed (controls). Subsets of stressed and control mice were concurrently given (1) no additional treatment, (2) a complex dietary supplement (CDS) designed to ameliorate inflammation, oxidative stress, mitochondrial dysfunction, insulin resistance and membrane integrity, (3) a running wheel in each of their home cages that permitted them to exercise, or (4) both the CDS and the running wheel for exercise. Four weeks of unpredictable stress reduced the animals' preference for saccharin, increased their adrenal weights and abolished the exercise-induced upregulation of neurogenesis that was observed in non-stressed animals. Unexpectedly, stress did not reduce hippocampal size, brain-derived neurotrophic factor (BDNF), or neurogenesis. The combination of dietary supplementation and exercise had multiple beneficial effects, as reflected in the number of doublecortin (DCX)-positive immature neurons in the dentate gyrus (DG), the sectional area of the DG and hippocampal CA1, as well as increased hippocampal BDNF messenger ribonucleic acid (mRNA) and serum vascular endothelial growth factor (VEGF) levels. In contrast, these benefits were not observed in chronically stressed animals exposed to either dietary supplementation or exercise alone. These findings could have important clinical implications for those suffering from chronic stress-related disorders such as major depression.","tags":["behaviour","stress","neurogenesis","hippocampus","exercise","dietary supplements","depression"],"title":"Synergistic effects of diet and exercise on hippocampal function in chronically stressed mice or chronic stress","type":"publication"},{"authors":["Aujla, H.","Hutton, C.P.","Rogala, B."],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"57b65b3baedda85acd37dcafdd0d7ea2","permalink":"/publication/aujla-et-al-2013/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/publication/aujla-et-al-2013/","section":"publication","summary":"Dysregulation of stress- and reward-related neurocircuitry that results from experience with alcohol or chronic stress may underlie vulnerability to initiate or relapse to alcohol use. Related changes in behaviors associated with such dysregulation may provide insight into factors that increase or decrease vulnerability to alcohol dependence in humans. In the present study, anxiety-like behavior and the acquisition of conditioned place preference were assessed in male Wistar rats with differing histories of exposure to stress or alcohol two weeks following the termination of treatment. Thus, time spent on the open arms of the elevated plus maze as well as the number of days required to acquire alcohol-based place conditioning were evaluated in control animals vs. those with a history of exposure to chronic unpredictable stress, experimenter-administered intragastric alcohol, or selfadministered alcohol liquid diet. Exposure to chronic unpredictable stress decreased open arm exploration on the elevated plus maze while no difference in open arm exploration was observed between the control group and alcohol-administration groups. However, animals with a history of intragastric ethanol administration or chronic unpredictable stress acquired alcohol-based placed conditioning after 3 alcohol-context pairings while control animals and animals with a history of alcohol self-administration required 4 alcohol-context pairings to acquire a significant preference for the alcohol-paired compartment. Results reveal that 1) a history of alcohol contributes to the acquisition of conditioned place preference but is dependent on the administration regimen and 2) chronic unpredictable stress enhances the acquisition of alcohol-based conditioned place preference and produces anxiogenic performance. These findings suggest overlap in the neuroadaptive changes that result from chronic exposure to alcohol or stress which may contribute to vulnerability to alcohol use.","tags":["behaviour","alcohol","stress"],"title":"Assessing anxiety and reward-related behaviors following alcohol administration or chronic stress","type":"publication"}]